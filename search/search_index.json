{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ethome Tools for machine learning of animal behavior. This library interprets pose-tracking files (at present, from DLC or NWB formats) and behavior annotations (at present, from BORIS and NWB formats) to help train a behavior classifier, perform unsupervised learning, and other common analysis tasks. Features Read in pose data and corresponding behavior annotations to make supervised learning easy Interpolate pose data to improve low-confidence predictions Create generic features for analysis and downstream ML tasks Create features specifically for mouse resident-intruder setup Perform unsupervised learning on pose data to extract discrete behavioral motifs (similar to MotionMapper) Quickly generate a movie with behavior predictions Installation pip install ethome-ml Quickstart It's easiest to start with an NWB file, which has metadata already connected to the pose data. Import from ethome import create_dataset from ethome.io import get_sample_nwb_paths Gather a sample NWB file fn_in = get_sample_nwb_paths() Create the dataframe: dataset = create_dataset(fn_in) dataset is an extended pandas DataFrame, so can be treated exactly as you would treat any other dataframe. ethome adds a bunch of metadata about the dataset, for instance you can list the body parts with: dataset.pose.body_parts The key functionality of ethome is the ability to easily create features for machine learning. You can use pre-built featuresets, or make your own. Here are two designed specifically to work with a mouse resident-intruder setup: dataset.features.add('cnn1d_prob') dataset.features.add('mars') You must have labeled your body parts in a certain way to use these two feature sets (see the How To). But other, more generic, feature creation functions are provided that work for any animal configuration. (The 'mars' feature-set is designed for studying social behavior in mice, based heavily on the MARS framework Segalin et al. [1]) Now you can access a features table, labels, and groups for learning with dataset.ml.features, dataset.ml.labels, dataset.ml.group . From here it's easy to use some ML libraries to train a behavior classifier. For example: from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, dataset.ml.features, dataset.ml.labels, groups = dataset.ml.group) Since the dataset object is just an extended Pandas dataframe we can manipulate it as such. E.g. we can add our model predictions to the dataframe: from sklearn.model_selection import cross_val_predict predictions = cross_val_predict(model, dataset.ml.features, dataset.ml.labels, groups = dataset.ml.group) dataset['prediction'] = predictions If the raw video file paths are provided in the metadata, under the video key, we can make a movie overlaying these predictions over the original video dataset.io.save_movie(['label', 'prediction'], '.') A more detailed run through of features is provided in the how-to guide. References [1] \"The Mouse Action Recognition System (MARS): a software pipeline for automated analysis of social behaviors in mice\" Segalin et al, eLife 2021","title":"Home"},{"location":"#ethome","text":"Tools for machine learning of animal behavior. This library interprets pose-tracking files (at present, from DLC or NWB formats) and behavior annotations (at present, from BORIS and NWB formats) to help train a behavior classifier, perform unsupervised learning, and other common analysis tasks.","title":"Ethome"},{"location":"#features","text":"Read in pose data and corresponding behavior annotations to make supervised learning easy Interpolate pose data to improve low-confidence predictions Create generic features for analysis and downstream ML tasks Create features specifically for mouse resident-intruder setup Perform unsupervised learning on pose data to extract discrete behavioral motifs (similar to MotionMapper) Quickly generate a movie with behavior predictions","title":"Features"},{"location":"#installation","text":"pip install ethome-ml","title":"Installation"},{"location":"#quickstart","text":"It's easiest to start with an NWB file, which has metadata already connected to the pose data. Import from ethome import create_dataset from ethome.io import get_sample_nwb_paths Gather a sample NWB file fn_in = get_sample_nwb_paths() Create the dataframe: dataset = create_dataset(fn_in) dataset is an extended pandas DataFrame, so can be treated exactly as you would treat any other dataframe. ethome adds a bunch of metadata about the dataset, for instance you can list the body parts with: dataset.pose.body_parts The key functionality of ethome is the ability to easily create features for machine learning. You can use pre-built featuresets, or make your own. Here are two designed specifically to work with a mouse resident-intruder setup: dataset.features.add('cnn1d_prob') dataset.features.add('mars') You must have labeled your body parts in a certain way to use these two feature sets (see the How To). But other, more generic, feature creation functions are provided that work for any animal configuration. (The 'mars' feature-set is designed for studying social behavior in mice, based heavily on the MARS framework Segalin et al. [1]) Now you can access a features table, labels, and groups for learning with dataset.ml.features, dataset.ml.labels, dataset.ml.group . From here it's easy to use some ML libraries to train a behavior classifier. For example: from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, dataset.ml.features, dataset.ml.labels, groups = dataset.ml.group) Since the dataset object is just an extended Pandas dataframe we can manipulate it as such. E.g. we can add our model predictions to the dataframe: from sklearn.model_selection import cross_val_predict predictions = cross_val_predict(model, dataset.ml.features, dataset.ml.labels, groups = dataset.ml.group) dataset['prediction'] = predictions If the raw video file paths are provided in the metadata, under the video key, we can make a movie overlaying these predictions over the original video dataset.io.save_movie(['label', 'prediction'], '.') A more detailed run through of features is provided in the how-to guide.","title":"Quickstart"},{"location":"#references","text":"[1] \"The Mouse Action Recognition System (MARS): a software pipeline for automated analysis of social behaviors in mice\" Segalin et al, eLife 2021","title":"References"},{"location":"how-to/","text":"How to This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. A very basic outline is also in the quick start section of the readme. This guide covers basic usage -- it doesn't comprehensively describe how to use every function or feature in ethome ; you can consult the API docs for more information on usage. After installation, cut and past the code samples below to follow along. 1 Getting started ethome makes it easy to perform common machine learning analyses on pose-tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. At present, data from DeepLabCut or pose data stored in NWB files is supported (via the ndx-pose extension). 1a Loading NWB files The first task is to load the data into a form useful for machine learning. The basic functionality of the package can be seen as using an extended pandas DataFrame , with associated support functions that are suited for behavior analysis. The DataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. The NeurodataWithoutBorders format can store both pose tracking data and behavioral annotations, along with associated metadata. If all of your data is stored in this format, then it's easy to import it into ethome : from ethome import create_dataset from ethome.io import get_sample_nwb_paths fn_in = get_sample_nwb_paths() recordings = create_dataset(fn_in) You can provide multiple recordings, just provide a list of paths instead. Each separate file is assumed to represent a different session/experiment/time period. I.e., they're not meant to represent the same session from different cameras, or the same session for different animals. 1b Loading your own metadata/loading DLC files If your data is stored in DeepLabCut csvs or h5 files, perhaps with accompanying behavioral annotations from BORIS, then you'll have to associate these with each other, and provide relevant metadata yourself. Sections 1b -> 1f outline how to do this. Data stored in NWB files have already addressed each of these steps. To import the data, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s -- this is how each video is identified. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' metadata = {tracking_csv : {'fps': 30, 'resolution': (1200, 1600)}} Beyond providing the fps for each video, all other fields are optional. 1c Helper function for making metadata dictionary Often you'll have many videos that have the same metadata, in that case you can easily create an appropriate dictionary with the helper function create_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: from ethome import create_metadata tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata dictionary now has two entries, one for each video, each listing the same FPS and resolution. Any keyword that is an iterable of the same length as the tracking files is zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to create_metadata : tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, labels = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30 ) to all videos, the entry labeled_data[i] would then be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately. 1d Special fields When making this metadata dictionary, keep in mind: * The labels field is special. If it is provided, then it is treated as housing the paths to behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . * The video field is also special. You should use it to provide a path to the corresponding video that was tracked. If available, this will be used by some of the visualization functions. * For each video, the fps field must be provided, so that frame numbers can be converted into times. 1e Scaling pose data There is some support for scaling the data to get it into desired units, consistent across all recordings. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . By default, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. You can specify the units by providing the units key yourself. Supported units include: 'mm', 'cm', 'm', 'in', 'ft'. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate. 1f Making the data frame Once you have the metadata dictionary prepared, you can easily create a DataFrame as: recordings = create_dataset(metadata) This creates a pandas dataframe, recordings , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata . 1g Renaming things If your tracking project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = create_dataset(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts -- you can provide a part_renamer dictionary. 1h Metadata When recordings is created, additional metadata is computed and accessible via: * recordings.metadata houses the following attributes: * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC 2 Interpolate low-confidence pose tracking Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and better tracks may be obtained by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with from ethome import interpolate_lowconf_points interpolate_lowconf_points(recordings) 3 Generate features To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in a few different ways. You can either use one of the feature-making functions provided or create a custom feature-making function, or custom class. To use the inbuilt functions you can reference them by identifying string, or provide the function itself to the features.add function. For instance, to compute the distances between all body parts (within and between animals), you could do: recordings.features.add('distances') This will compute and add the distances between all body parts of all animals. 3a In-built support for resident-intruder setup First, if your setup is a social mouse study, involving two mice, similar enough to the standard resident-intruder setup, then you can use some pre-designed feature sets. The body parts that are tracked must be those from the MARS dataset (See figure). You will have to have labeled and tracked your mice in DLC in the same way. (with the same animal and body part names -- ethome 's create_dataset function can rename them appropriately) The cnn1d_prob , mars , mars_reduced and social functions can be used to make features for this setup. cnn1d_prob runs a 1D CNN and outputs prediction probabilities of three behaviors (attack, mount, and investigation). Even if you're not interested in these exact behaviors, they may still be useful for predicting the occurance of other behaviors, as part of an ensemble model. mars computes a long list of features as used in the MARS paper. You can refer to that paper for more details. mars_reduced is a reduced version of the MARS features social is a set of features that only involve measures of one animal in relation to the other. 3b Generic features You can generate more generic features using the following functions: * centroid the centroid of each animal's body parts * centroid_velocity the velocity of the centroids * centroid_interanimal the distances between the centroids of all the animals * centroid_interanimal_speed the rate of change of centroid_interanimal * intrabodypartspeeds the speeds of all body parts * intrabodypartdistances the distances between all animals body parts (inter- and intra-animal) These classes work for any animal setup, not just resident-intruder with specific body parts, as assumed for the mars features. 3c Add your own features There are two ways to add your own feature sets to your DataFrame. The first is to create a function that takes a pandas DataFrame, and returns a new DataFrame with the features you want to add. For example: def diff_cols(df, required_columns = []): return df[required_columns].diff() recordings.features.add(diff_cols, required_columns = ['resident_neck_x', 'resident_neck_y']) The second is to create a class that has, at the least, the method transform . class BodyPartDiff: def __init__(self, required_columns): self.required_columns = required_columns def transform(self, df): return df[self.required_columns].diff() head_diff = BodyPartDiff(['resident_neck_x', 'resident_neck_y']) recordings.features.add(head_diff) This is more verbose than the above, but has the advantage that the it can be re-used. E.g. you could fit the instance to training data and apply it to test data, similar to sklearn's approach. 3d Features manipulation By default, when new features are added to the dataframe, they are considered 'active'. Active features can be accessed through recordings.ml.features You can pass this to any ML method for further processing. This is just a convenience for managing the long list of features you will have created in the steps above. You can always just treat recordings like a pandas DataFrame and do ML how you would normally. To activate features you can use recordings.features.activate , and to deactivate features you can use recordings.features.deactivate . Deactivating keeps them in the DataTable, but just no longer includes those features in the recordings.ml.features view. 4 Fit a model for behavior classification Ok! The hard work is done, so now you can easily train a behavior classifier based on the features you've computed and the labels provided. E.g. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, recording.ml.features, recording.ml.labels, recordings.ml.group) 5 Perform unsupervised learning STILL UNDER CONSTRUCTION 6 Make output movies Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos: dataset.io.save_movie(['label', 'prediction'], '.') The video field in the metadata , specifying the path to the underlying video, has to be present for each recording for this to work. 7 Save your data You can save your data as a pickled DataFrame with recordings.io.save('outfile.pkl') (and can be loaded again with:) recordings = pd.DataFrame.io.load('outfile.pkl') NOTE: By importing ethome you extend the functionality of the pandas dataframe, hence can access things like .io.load 8 Summary and reference list of added functionality by ethome For reference, the metadata and added functions added to the dataframe are: * recordings.metadata , which houses * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC * recordings.features , feature creation and manipulation * activate : activate columns by name * deactivate : deactivate columns by name * regex : select column names based on regex * add : create new features * recordings.ml , machine learning conveniences * features : a 'active' set of features * labels : if loaded from BORIS, behavior labels. from text to label with recordings.metadata.label_key * group : the corresponding video filename for all rows in the table -- can be used for GroupKFold CV, or similar * recordings.io , I/O functions * save : save DataFrame as pickle file * to_dlc_csv : save original tracking data back into csv -- if you interpolated or otherwise manipulated the data * load : load DataFrame from pickle file * save_movie : create a movie with some feature column you indicate overlaid See the API docs for usage details.","title":"How to"},{"location":"how-to/#how-to","text":"This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. A very basic outline is also in the quick start section of the readme. This guide covers basic usage -- it doesn't comprehensively describe how to use every function or feature in ethome ; you can consult the API docs for more information on usage. After installation, cut and past the code samples below to follow along.","title":"How to"},{"location":"how-to/#1-getting-started","text":"ethome makes it easy to perform common machine learning analyses on pose-tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. At present, data from DeepLabCut or pose data stored in NWB files is supported (via the ndx-pose extension).","title":"1 Getting started"},{"location":"how-to/#1a-loading-nwb-files","text":"The first task is to load the data into a form useful for machine learning. The basic functionality of the package can be seen as using an extended pandas DataFrame , with associated support functions that are suited for behavior analysis. The DataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. The NeurodataWithoutBorders format can store both pose tracking data and behavioral annotations, along with associated metadata. If all of your data is stored in this format, then it's easy to import it into ethome : from ethome import create_dataset from ethome.io import get_sample_nwb_paths fn_in = get_sample_nwb_paths() recordings = create_dataset(fn_in) You can provide multiple recordings, just provide a list of paths instead. Each separate file is assumed to represent a different session/experiment/time period. I.e., they're not meant to represent the same session from different cameras, or the same session for different animals.","title":"1a Loading NWB files"},{"location":"how-to/#1b-loading-your-own-metadataloading-dlc-files","text":"If your data is stored in DeepLabCut csvs or h5 files, perhaps with accompanying behavioral annotations from BORIS, then you'll have to associate these with each other, and provide relevant metadata yourself. Sections 1b -> 1f outline how to do this. Data stored in NWB files have already addressed each of these steps. To import the data, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s -- this is how each video is identified. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' metadata = {tracking_csv : {'fps': 30, 'resolution': (1200, 1600)}} Beyond providing the fps for each video, all other fields are optional.","title":"1b Loading your own metadata/loading DLC files"},{"location":"how-to/#1c-helper-function-for-making-metadata-dictionary","text":"Often you'll have many videos that have the same metadata, in that case you can easily create an appropriate dictionary with the helper function create_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: from ethome import create_metadata tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata dictionary now has two entries, one for each video, each listing the same FPS and resolution. Any keyword that is an iterable of the same length as the tracking files is zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to create_metadata : tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, labels = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30 ) to all videos, the entry labeled_data[i] would then be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately.","title":"1c Helper function for making metadata dictionary"},{"location":"how-to/#1d-special-fields","text":"When making this metadata dictionary, keep in mind: * The labels field is special. If it is provided, then it is treated as housing the paths to behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . * The video field is also special. You should use it to provide a path to the corresponding video that was tracked. If available, this will be used by some of the visualization functions. * For each video, the fps field must be provided, so that frame numbers can be converted into times.","title":"1d Special fields"},{"location":"how-to/#1e-scaling-pose-data","text":"There is some support for scaling the data to get it into desired units, consistent across all recordings. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . By default, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. You can specify the units by providing the units key yourself. Supported units include: 'mm', 'cm', 'm', 'in', 'ft'. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate.","title":"1e Scaling pose data"},{"location":"how-to/#1f-making-the-data-frame","text":"Once you have the metadata dictionary prepared, you can easily create a DataFrame as: recordings = create_dataset(metadata) This creates a pandas dataframe, recordings , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata .","title":"1f Making the data frame"},{"location":"how-to/#1g-renaming-things","text":"If your tracking project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = create_dataset(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts -- you can provide a part_renamer dictionary.","title":"1g Renaming things"},{"location":"how-to/#1h-metadata","text":"When recordings is created, additional metadata is computed and accessible via: * recordings.metadata houses the following attributes: * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC","title":"1h Metadata"},{"location":"how-to/#2-interpolate-low-confidence-pose-tracking","text":"Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and better tracks may be obtained by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with from ethome import interpolate_lowconf_points interpolate_lowconf_points(recordings)","title":"2 Interpolate low-confidence pose tracking"},{"location":"how-to/#3-generate-features","text":"To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in a few different ways. You can either use one of the feature-making functions provided or create a custom feature-making function, or custom class. To use the inbuilt functions you can reference them by identifying string, or provide the function itself to the features.add function. For instance, to compute the distances between all body parts (within and between animals), you could do: recordings.features.add('distances') This will compute and add the distances between all body parts of all animals.","title":"3 Generate features"},{"location":"how-to/#3a-in-built-support-for-resident-intruder-setup","text":"First, if your setup is a social mouse study, involving two mice, similar enough to the standard resident-intruder setup, then you can use some pre-designed feature sets. The body parts that are tracked must be those from the MARS dataset (See figure). You will have to have labeled and tracked your mice in DLC in the same way. (with the same animal and body part names -- ethome 's create_dataset function can rename them appropriately) The cnn1d_prob , mars , mars_reduced and social functions can be used to make features for this setup. cnn1d_prob runs a 1D CNN and outputs prediction probabilities of three behaviors (attack, mount, and investigation). Even if you're not interested in these exact behaviors, they may still be useful for predicting the occurance of other behaviors, as part of an ensemble model. mars computes a long list of features as used in the MARS paper. You can refer to that paper for more details. mars_reduced is a reduced version of the MARS features social is a set of features that only involve measures of one animal in relation to the other.","title":"3a In-built support for resident-intruder setup"},{"location":"how-to/#3b-generic-features","text":"You can generate more generic features using the following functions: * centroid the centroid of each animal's body parts * centroid_velocity the velocity of the centroids * centroid_interanimal the distances between the centroids of all the animals * centroid_interanimal_speed the rate of change of centroid_interanimal * intrabodypartspeeds the speeds of all body parts * intrabodypartdistances the distances between all animals body parts (inter- and intra-animal) These classes work for any animal setup, not just resident-intruder with specific body parts, as assumed for the mars features.","title":"3b Generic features"},{"location":"how-to/#3c-add-your-own-features","text":"There are two ways to add your own feature sets to your DataFrame. The first is to create a function that takes a pandas DataFrame, and returns a new DataFrame with the features you want to add. For example: def diff_cols(df, required_columns = []): return df[required_columns].diff() recordings.features.add(diff_cols, required_columns = ['resident_neck_x', 'resident_neck_y']) The second is to create a class that has, at the least, the method transform . class BodyPartDiff: def __init__(self, required_columns): self.required_columns = required_columns def transform(self, df): return df[self.required_columns].diff() head_diff = BodyPartDiff(['resident_neck_x', 'resident_neck_y']) recordings.features.add(head_diff) This is more verbose than the above, but has the advantage that the it can be re-used. E.g. you could fit the instance to training data and apply it to test data, similar to sklearn's approach.","title":"3c Add your own features"},{"location":"how-to/#3d-features-manipulation","text":"By default, when new features are added to the dataframe, they are considered 'active'. Active features can be accessed through recordings.ml.features You can pass this to any ML method for further processing. This is just a convenience for managing the long list of features you will have created in the steps above. You can always just treat recordings like a pandas DataFrame and do ML how you would normally. To activate features you can use recordings.features.activate , and to deactivate features you can use recordings.features.deactivate . Deactivating keeps them in the DataTable, but just no longer includes those features in the recordings.ml.features view.","title":"3d Features manipulation"},{"location":"how-to/#4-fit-a-model-for-behavior-classification","text":"Ok! The hard work is done, so now you can easily train a behavior classifier based on the features you've computed and the labels provided. E.g. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, recording.ml.features, recording.ml.labels, recordings.ml.group)","title":"4 Fit a model for behavior classification"},{"location":"how-to/#5-perform-unsupervised-learning","text":"STILL UNDER CONSTRUCTION","title":"5 Perform unsupervised learning"},{"location":"how-to/#6-make-output-movies","text":"Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos: dataset.io.save_movie(['label', 'prediction'], '.') The video field in the metadata , specifying the path to the underlying video, has to be present for each recording for this to work.","title":"6 Make output movies"},{"location":"how-to/#7-save-your-data","text":"You can save your data as a pickled DataFrame with recordings.io.save('outfile.pkl') (and can be loaded again with:) recordings = pd.DataFrame.io.load('outfile.pkl') NOTE: By importing ethome you extend the functionality of the pandas dataframe, hence can access things like .io.load","title":"7 Save your data"},{"location":"how-to/#8-summary-and-reference-list-of-added-functionality-by-ethome","text":"For reference, the metadata and added functions added to the dataframe are: * recordings.metadata , which houses * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC * recordings.features , feature creation and manipulation * activate : activate columns by name * deactivate : deactivate columns by name * regex : select column names based on regex * add : create new features * recordings.ml , machine learning conveniences * features : a 'active' set of features * labels : if loaded from BORIS, behavior labels. from text to label with recordings.metadata.label_key * group : the corresponding video filename for all rows in the table -- can be used for GroupKFold CV, or similar * recordings.io , I/O functions * save : save DataFrame as pickle file * to_dlc_csv : save original tracking data back into csv -- if you interpolated or otherwise manipulated the data * load : load DataFrame from pickle file * save_movie : create a movie with some feature column you indicate overlaid See the API docs for usage details.","title":"8 Summary and reference list of added functionality by ethome"},{"location":"api-docs/","text":"API Overview Modules features interpolation io : Loading and saving tracking and behavior annotation files plot unsupervised utils : Small helper utilities video : Basic video tracking and behavior class that houses data. Classes io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor Functions interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.get_sample_nwb_paths : Get path to a sample NWB file with tracking data for testing and dev purposes. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_NWB_tracks : Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package). io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api-docs/#api-overview","text":"","title":"API Overview"},{"location":"api-docs/#modules","text":"features interpolation io : Loading and saving tracking and behavior annotation files plot unsupervised utils : Small helper utilities video : Basic video tracking and behavior class that houses data.","title":"Modules"},{"location":"api-docs/#classes","text":"io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor","title":"Classes"},{"location":"api-docs/#functions","text":"interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.get_sample_nwb_paths : Get path to a sample NWB file with tracking data for testing and dev purposes. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_NWB_tracks : Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package). io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api-docs/features/","text":"module features Global Variables FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Features"},{"location":"api-docs/features/#module-features","text":"","title":"module features"},{"location":"api-docs/features/#global-variables","text":"FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/interpolation/","text":"module interpolation function interpolate_lowconf_points interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"Interpolation"},{"location":"api-docs/interpolation/#module-interpolation","text":"","title":"module interpolation"},{"location":"api-docs/interpolation/#function-interpolate_lowconf_points","text":"interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"function interpolate_lowconf_points"},{"location":"api-docs/io/","text":"module io Loading and saving tracking and behavior annotation files Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS function uniquifier uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order function save_sklearn_model save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to function load_sklearn_model load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model function read_DLC_tracks read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True, labels: DataFrame = None ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer function read_NWB_tracks read_NWB_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package). Args: fn_in : nwb file that has the tracking information part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for pose tracks (excluding likelihoods, if read in), Scorer function save_DLC_tracks_h5 save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe function load_data load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object. function get_sample_nwb_paths get_sample_nwb_paths() Get path to a sample NWB file with tracking data for testing and dev purposes. Returns: Path to a sample NWB file. function get_sample_data_paths get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files function get_sample_data get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files function read_boris_annotation read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors. function create_behavior_labels create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors. class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"api-docs/io/#module-io","text":"Loading and saving tracking and behavior annotation files","title":"module io"},{"location":"api-docs/io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS","title":"Global Variables"},{"location":"api-docs/io/#function-uniquifier","text":"uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order","title":"function uniquifier"},{"location":"api-docs/io/#function-save_sklearn_model","text":"save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to","title":"function save_sklearn_model"},{"location":"api-docs/io/#function-load_sklearn_model","text":"load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model","title":"function load_sklearn_model"},{"location":"api-docs/io/#function-read_dlc_tracks","text":"read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True, labels: DataFrame = None ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer","title":"function read_DLC_tracks"},{"location":"api-docs/io/#function-read_nwb_tracks","text":"read_NWB_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package). Args: fn_in : nwb file that has the tracking information part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for pose tracks (excluding likelihoods, if read in), Scorer","title":"function read_NWB_tracks"},{"location":"api-docs/io/#function-save_dlc_tracks_h5","text":"save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe","title":"function save_DLC_tracks_h5"},{"location":"api-docs/io/#function-load_data","text":"load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object.","title":"function load_data"},{"location":"api-docs/io/#function-get_sample_nwb_paths","text":"get_sample_nwb_paths() Get path to a sample NWB file with tracking data for testing and dev purposes. Returns: Path to a sample NWB file.","title":"function get_sample_nwb_paths"},{"location":"api-docs/io/#function-get_sample_data_paths","text":"get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files","title":"function get_sample_data_paths"},{"location":"api-docs/io/#function-get_sample_data","text":"get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files","title":"function get_sample_data"},{"location":"api-docs/io/#function-read_boris_annotation","text":"read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function read_boris_annotation"},{"location":"api-docs/io/#function-create_behavior_labels","text":"create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function create_behavior_labels"},{"location":"api-docs/io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"api-docs/io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"api-docs/io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"api-docs/io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"api-docs/io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"api-docs/plot/","text":"module plot Global Variables global_config function plot_embedding plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_unsupervised_results plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_ethogram plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects function create_ethogram_video create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None function create_sample_videos create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None function create_mosaic_video create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None class MplColorHelper method __init__ __init__(cmap_name, start_val, stop_val) method get_rgb get_rgb(val) This file was automatically generated via lazydocs .","title":"Plot"},{"location":"api-docs/plot/#module-plot","text":"","title":"module plot"},{"location":"api-docs/plot/#global-variables","text":"global_config","title":"Global Variables"},{"location":"api-docs/plot/#function-plot_embedding","text":"plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_embedding"},{"location":"api-docs/plot/#function-plot_unsupervised_results","text":"plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_unsupervised_results"},{"location":"api-docs/plot/#function-plot_ethogram","text":"plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects","title":"function plot_ethogram"},{"location":"api-docs/plot/#function-create_ethogram_video","text":"create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None","title":"function create_ethogram_video"},{"location":"api-docs/plot/#function-create_sample_videos","text":"create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None","title":"function create_sample_videos"},{"location":"api-docs/plot/#function-create_mosaic_video","text":"create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None","title":"function create_mosaic_video"},{"location":"api-docs/plot/#class-mplcolorhelper","text":"","title":"class MplColorHelper"},{"location":"api-docs/plot/#method-__init__","text":"__init__(cmap_name, start_val, stop_val)","title":"method __init__"},{"location":"api-docs/plot/#method-get_rgb","text":"get_rgb(val) This file was automatically generated via lazydocs .","title":"method get_rgb"},{"location":"api-docs/unsupervised/","text":"module unsupervised function compute_tsne_embedding compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding function compute_morlet compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time. function compute_density compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts) function compute_watershed compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate. function cluster_behaviors cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"Unsupervised"},{"location":"api-docs/unsupervised/#module-unsupervised","text":"","title":"module unsupervised"},{"location":"api-docs/unsupervised/#function-compute_tsne_embedding","text":"compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding","title":"function compute_tsne_embedding"},{"location":"api-docs/unsupervised/#function-compute_morlet","text":"compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time.","title":"function compute_morlet"},{"location":"api-docs/unsupervised/#function-compute_density","text":"compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts)","title":"function compute_density"},{"location":"api-docs/unsupervised/#function-compute_watershed","text":"compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate.","title":"function compute_watershed"},{"location":"api-docs/unsupervised/#function-cluster_behaviors","text":"cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"function cluster_behaviors"},{"location":"api-docs/utils/","text":"module utils Small helper utilities function checkFFMPEG checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api-docs/utils/#module-utils","text":"Small helper utilities","title":"module utils"},{"location":"api-docs/utils/#function-checkffmpeg","text":"checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"function checkFFMPEG"},{"location":"api-docs/video/","text":"module video Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class. A note on unit conversions For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed. Global Variables global_config FEATURE_MAKERS UNIT_DICT function create_metadata create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided function create_dataset create_dataset( metadata: dict = None, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods. function load_experiment load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file function get_sample_openfield_data get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files class EthologyMetadataAccessor method __init__ __init__(pandas_obj) property details property label_key property n_videos property reverse_label_key property videos class EthologyFeaturesAccessor method __init__ __init__(pandas_obj) property active method activate activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated). method add add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed method deactivate deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns. method deactivate_cols deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features. method regex regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names class EthologyPoseAccessor method __init__ __init__(pandas_obj) property animal_setup property animals property body_parts property raw_track_columns class EthologyMLAccessor method __init__ __init__(pandas_obj) property features property fold_cols property folds property group property label_cols property labels property splitter class EthologyIOAccessor method __init__ __init__(pandas_obj) method load load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file. method save save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path. method save_movie save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out' method to_dlc_csv to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"Video"},{"location":"api-docs/video/#module-video","text":"Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class.","title":"module video"},{"location":"api-docs/video/#a-note-on-unit-conversions","text":"For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed.","title":"A note on unit conversions"},{"location":"api-docs/video/#global-variables","text":"global_config FEATURE_MAKERS UNIT_DICT","title":"Global Variables"},{"location":"api-docs/video/#function-create_metadata","text":"create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided","title":"function create_metadata"},{"location":"api-docs/video/#function-create_dataset","text":"create_dataset( metadata: dict = None, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods.","title":"function create_dataset"},{"location":"api-docs/video/#function-load_experiment","text":"load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file","title":"function load_experiment"},{"location":"api-docs/video/#function-get_sample_openfield_data","text":"get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files","title":"function get_sample_openfield_data"},{"location":"api-docs/video/#class-ethologymetadataaccessor","text":"","title":"class EthologyMetadataAccessor"},{"location":"api-docs/video/#method-__init__","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-details","text":"","title":"property details"},{"location":"api-docs/video/#property-label_key","text":"","title":"property label_key"},{"location":"api-docs/video/#property-n_videos","text":"","title":"property n_videos"},{"location":"api-docs/video/#property-reverse_label_key","text":"","title":"property reverse_label_key"},{"location":"api-docs/video/#property-videos","text":"","title":"property videos"},{"location":"api-docs/video/#class-ethologyfeaturesaccessor","text":"","title":"class EthologyFeaturesAccessor"},{"location":"api-docs/video/#method-__init___1","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-active","text":"","title":"property active"},{"location":"api-docs/video/#method-activate","text":"activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated).","title":"method activate"},{"location":"api-docs/video/#method-add","text":"add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed","title":"method add"},{"location":"api-docs/video/#method-deactivate","text":"deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns.","title":"method deactivate"},{"location":"api-docs/video/#method-deactivate_cols","text":"deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features.","title":"method deactivate_cols"},{"location":"api-docs/video/#method-regex","text":"regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names","title":"method regex"},{"location":"api-docs/video/#class-ethologyposeaccessor","text":"","title":"class EthologyPoseAccessor"},{"location":"api-docs/video/#method-__init___2","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-animal_setup","text":"","title":"property animal_setup"},{"location":"api-docs/video/#property-animals","text":"","title":"property animals"},{"location":"api-docs/video/#property-body_parts","text":"","title":"property body_parts"},{"location":"api-docs/video/#property-raw_track_columns","text":"","title":"property raw_track_columns"},{"location":"api-docs/video/#class-ethologymlaccessor","text":"","title":"class EthologyMLAccessor"},{"location":"api-docs/video/#method-__init___3","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-features","text":"","title":"property features"},{"location":"api-docs/video/#property-fold_cols","text":"","title":"property fold_cols"},{"location":"api-docs/video/#property-folds","text":"","title":"property folds"},{"location":"api-docs/video/#property-group","text":"","title":"property group"},{"location":"api-docs/video/#property-label_cols","text":"","title":"property label_cols"},{"location":"api-docs/video/#property-labels","text":"","title":"property labels"},{"location":"api-docs/video/#property-splitter","text":"","title":"property splitter"},{"location":"api-docs/video/#class-ethologyioaccessor","text":"","title":"class EthologyIOAccessor"},{"location":"api-docs/video/#method-__init___4","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#method-load","text":"load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file.","title":"method load"},{"location":"api-docs/video/#method-save","text":"save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path.","title":"method save"},{"location":"api-docs/video/#method-save_movie","text":"save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out'","title":"method save_movie"},{"location":"api-docs/video/#method-to_dlc_csv","text":"to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"method to_dlc_csv"}]}