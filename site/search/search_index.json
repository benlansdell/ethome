{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ethome Machine learning for animal behavior. Interprets pose-tracking files (currently only from DLC), behavior annotations (currently only from BORIS) to train a behavior classifier. Installation pip install ethome Can install optional extras with: pip install numpy, cython pip install ethome[all] This includes matplotlib, keras, and Linderman lab's state-space model package, ssm . Note that installing ssm requires cython and numpy for the build, so must be already present in the environment. Quickstart Import from glob import glob from ethome import ExperimentDataFrame, clone_metadata from ethome import compute_dl_probability_features, compute_mars_features Gather the DLC and BORIS tracking and annotation files tracking_files, boris_files = get_sample_data_paths() Setup some parameters frame_width = 20 # (float) length of entire horizontal shot frame_width_units = 'in' # (str) units frame_width is given in fps = 30 # (int) frames per second resolution = (1200, 1600) # (tuple) HxW in pixels Create a parameter object and video dataset metadata = clone_metadata(tracking_files, label_files = boris_files, frame_width = frame_width, fps = fps, frame_width_units = frame_width_units, resolution = resolution) animal_renamer = {'adult': 'resident', 'juvenile':'intruder'} dataset = ExperimentDataFrame(metadata, animal_renamer=animal_renamer) Now create features on this dataset dataset.add_features(compute_dl_probability_features, featureset_name = '1dcnn', add_to_features = True) dataset.add_features(compute_mars_features, featureset_name = 'MARS', add_to_features = True) Now access a features table, labels, and groups for learning with dataset.features, dataset.labels, dataset.groups . From here it's easy to use some ML libraries to predict behavior. For example: from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.model_selection import cross_val_predict from sklearn.metrics import accuracy_score model = RandomForestClassifier() predictions = cross_val_predict(model, dataset.features, dataset.labels, dataset.groups) score = accuracy_score(dataset.labels, predictions)","title":"Home"},{"location":"#ethome","text":"Machine learning for animal behavior. Interprets pose-tracking files (currently only from DLC), behavior annotations (currently only from BORIS) to train a behavior classifier.","title":"ethome"},{"location":"#installation","text":"pip install ethome Can install optional extras with: pip install numpy, cython pip install ethome[all] This includes matplotlib, keras, and Linderman lab's state-space model package, ssm . Note that installing ssm requires cython and numpy for the build, so must be already present in the environment.","title":"Installation"},{"location":"#quickstart","text":"Import from glob import glob from ethome import ExperimentDataFrame, clone_metadata from ethome import compute_dl_probability_features, compute_mars_features Gather the DLC and BORIS tracking and annotation files tracking_files, boris_files = get_sample_data_paths() Setup some parameters frame_width = 20 # (float) length of entire horizontal shot frame_width_units = 'in' # (str) units frame_width is given in fps = 30 # (int) frames per second resolution = (1200, 1600) # (tuple) HxW in pixels Create a parameter object and video dataset metadata = clone_metadata(tracking_files, label_files = boris_files, frame_width = frame_width, fps = fps, frame_width_units = frame_width_units, resolution = resolution) animal_renamer = {'adult': 'resident', 'juvenile':'intruder'} dataset = ExperimentDataFrame(metadata, animal_renamer=animal_renamer) Now create features on this dataset dataset.add_features(compute_dl_probability_features, featureset_name = '1dcnn', add_to_features = True) dataset.add_features(compute_mars_features, featureset_name = 'MARS', add_to_features = True) Now access a features table, labels, and groups for learning with dataset.features, dataset.labels, dataset.groups . From here it's easy to use some ML libraries to predict behavior. For example: from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.model_selection import cross_val_predict from sklearn.metrics import accuracy_score model = RandomForestClassifier() predictions = cross_val_predict(model, dataset.features, dataset.labels, dataset.groups) score = accuracy_score(dataset.labels, predictions)","title":"Quickstart"},{"location":"how-to/","text":"How to This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. The basic outline is also in the quick start section above. 1 Getting started The package makes it easier to perform common downstream analyses on pose tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. Currently only DeepLabCut is supported. The first task is to load this data into a data structure and associate metadata with it. The basic functionality of the package can be seen as using an extended pandas DataFrame, with associated support functions that are suited for behavioral analysis. The VideoDataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. To create the VideoDataFrame object, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' fps = 30 resolution = (1200, 1600) metadata = {tracking_csv : {'fps': fps, 'resolution': resolution}} Beyond providing the fps for each video, all fields are optional. Helper function for making metadata dictionary If you have many videos that have the same metadata, you can easily create such a dictionary with the helper function clone_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = clone_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata now has two entries, each listing the same FPS and resolution. If one of the keywords is a list of the same length as the tracking files, then this data is instead zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to clone_metadata: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = clone_metadata(tracking_csvs, label_files = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30) to all videos, the entry labeled_data[i] would be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately. Special fields The label_files field is special. If it is provided, then it is treated as housing behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . For each video, the fps field must be provided, so that frame numbers can be converted into times. Scaling pose data There is some support for scaling the data to get it into desired units, consistent across all recordings. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed. Making the data frame Once you have the metadata dictionary prepared, you can easily create a VideoDataFrame as: recordings = ExperimentDataFrame(metadata) This houses a dataframe, recordings.data , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata . It also houses recordings.metadata that stores the dictionary provided. If your DLC project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = ExperimentDataFrame(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts; you can provide a part_renamer dictionary. Now you're ready to get started. 2 Interpolate low-confidence pose tracking Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and a better track may be provided by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with interpolate_lowconf_points(recordings) 3 Generate features To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in the following ways. 3a Resident-intruder setup First, if your setup is a social mouse study, involving two mice, close enough to the standard resident-intruder setup, then you can use a 3b Add your own feature-maker To use your own feature creator: Create a function, e.g. create_custom_features , and provide the Features class a list of columns that are needed by this function to compute the features. The function has the form: create_custom_features(<df>, <raw_col_names>, <animal_setup>, **kwargs) Where: df is the dataframe to compute the features on, raw_col_names is a list of the names of the columns in the dataframe that contain the raw data used for the feature creation. These are required for the model. animal_setup is a dictionary with keys bodypart_ids , mouse_ids , colnames . bodypart_ids is a list of the bodypart ids that are used in the dataframe mouse_ids is a list of the mouse ids that are used in the dataframe colnames is the list product(animals, XY_IDS, body_parts) **kwargs are extra arguments passed onto the feature creation function. The function returns: A dataframe, that only contains the new features. These will be added to the ExperimentDataFrame as columns. Once you have such a function defined, you can create a \"feature making object\" with custom_feature_maker = Features(create_custom_features, req_columns) This allows you to use the same This could be used on datasets as: dataset.add_features(custom_feature_maker, featureset_name = 'CUSTOM', add_to_features = True) 3c Add or remove features 4 Fit a model for behavior classification 5 Perform unsupervised learning 6 Make output movies Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos dataset.make_movie(['label', 'prediction'], '.') 7 Save your data","title":"How to"},{"location":"how-to/#how-to","text":"This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. The basic outline is also in the quick start section above.","title":"How to"},{"location":"how-to/#1-getting-started","text":"The package makes it easier to perform common downstream analyses on pose tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. Currently only DeepLabCut is supported. The first task is to load this data into a data structure and associate metadata with it. The basic functionality of the package can be seen as using an extended pandas DataFrame, with associated support functions that are suited for behavioral analysis. The VideoDataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. To create the VideoDataFrame object, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' fps = 30 resolution = (1200, 1600) metadata = {tracking_csv : {'fps': fps, 'resolution': resolution}} Beyond providing the fps for each video, all fields are optional.","title":"1 Getting started"},{"location":"how-to/#helper-function-for-making-metadata-dictionary","text":"If you have many videos that have the same metadata, you can easily create such a dictionary with the helper function clone_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = clone_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata now has two entries, each listing the same FPS and resolution. If one of the keywords is a list of the same length as the tracking files, then this data is instead zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to clone_metadata: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = clone_metadata(tracking_csvs, label_files = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30) to all videos, the entry labeled_data[i] would be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately.","title":"Helper function for making metadata dictionary"},{"location":"how-to/#special-fields","text":"The label_files field is special. If it is provided, then it is treated as housing behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . For each video, the fps field must be provided, so that frame numbers can be converted into times.","title":"Special fields"},{"location":"how-to/#scaling-pose-data","text":"There is some support for scaling the data to get it into desired units, consistent across all recordings. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed.","title":"Scaling pose data"},{"location":"how-to/#making-the-data-frame","text":"Once you have the metadata dictionary prepared, you can easily create a VideoDataFrame as: recordings = ExperimentDataFrame(metadata) This houses a dataframe, recordings.data , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata . It also houses recordings.metadata that stores the dictionary provided. If your DLC project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = ExperimentDataFrame(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts; you can provide a part_renamer dictionary. Now you're ready to get started.","title":"Making the data frame"},{"location":"how-to/#2-interpolate-low-confidence-pose-tracking","text":"Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and a better track may be provided by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with interpolate_lowconf_points(recordings)","title":"2 Interpolate low-confidence pose tracking"},{"location":"how-to/#3-generate-features","text":"To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in the following ways.","title":"3 Generate features"},{"location":"how-to/#3a-resident-intruder-setup","text":"First, if your setup is a social mouse study, involving two mice, close enough to the standard resident-intruder setup, then you can use a","title":"3a Resident-intruder setup"},{"location":"how-to/#3b-add-your-own-feature-maker","text":"To use your own feature creator: Create a function, e.g. create_custom_features , and provide the Features class a list of columns that are needed by this function to compute the features. The function has the form: create_custom_features(<df>, <raw_col_names>, <animal_setup>, **kwargs) Where: df is the dataframe to compute the features on, raw_col_names is a list of the names of the columns in the dataframe that contain the raw data used for the feature creation. These are required for the model. animal_setup is a dictionary with keys bodypart_ids , mouse_ids , colnames . bodypart_ids is a list of the bodypart ids that are used in the dataframe mouse_ids is a list of the mouse ids that are used in the dataframe colnames is the list product(animals, XY_IDS, body_parts) **kwargs are extra arguments passed onto the feature creation function. The function returns: A dataframe, that only contains the new features. These will be added to the ExperimentDataFrame as columns. Once you have such a function defined, you can create a \"feature making object\" with custom_feature_maker = Features(create_custom_features, req_columns) This allows you to use the same This could be used on datasets as: dataset.add_features(custom_feature_maker, featureset_name = 'CUSTOM', add_to_features = True)","title":"3b Add your own feature-maker"},{"location":"how-to/#3c-add-or-remove-features","text":"","title":"3c Add or remove features"},{"location":"how-to/#4-fit-a-model-for-behavior-classification","text":"","title":"4 Fit a model for behavior classification"},{"location":"how-to/#5-perform-unsupervised-learning","text":"","title":"5 Perform unsupervised learning"},{"location":"how-to/#6-make-output-movies","text":"Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos dataset.make_movie(['label', 'prediction'], '.')","title":"6 Make output movies"},{"location":"how-to/#7-save-your-data","text":"","title":"7 Save your data"},{"location":"api-docs/","text":"API Overview Modules config : Configuration options for ethome functions. dl dl.dl_features dl.dl_generators dl.dl_models dl.feature_engineering dl.grid_searches features : Functions to take pose tracks and compute a set of features from them. generic_features : Functions to take pose tracks and compute a set of features from them interpolation io : Loading and saving tracking and behavior annotation files mars_features ml : Machine learning functions models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities video : Basic video tracking and behavior class that houses data. Classes dl_features.Trainer dl_generators.MABe_Generator features.Features io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.MLDataFrame : DataFrame useful for interfacing between pandas and sklearn. Stores a data video.ExperimentDataFrame Functions dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything dl_generators.features_distances dl_generators.features_distances_normalized dl_generators.features_identity dl_generators.features_mars dl_generators.features_mars_distr dl_generators.features_via_sklearn dl_generators.make_df dl_models.build_baseline_model feature_engineering.augment_features feature_engineering.boiler_plate feature_engineering.make_features_distances feature_engineering.make_features_mars feature_engineering.make_features_mars_distr feature_engineering.make_features_mars_reduced feature_engineering.make_features_social feature_engineering.make_features_velocities generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.rename_df_cols : Rename dataframe columns io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.clone_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_videodataset : Load ExperimentDataFrame from file. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api-docs/#api-overview","text":"","title":"API Overview"},{"location":"api-docs/#modules","text":"config : Configuration options for ethome functions. dl dl.dl_features dl.dl_generators dl.dl_models dl.feature_engineering dl.grid_searches features : Functions to take pose tracks and compute a set of features from them. generic_features : Functions to take pose tracks and compute a set of features from them interpolation io : Loading and saving tracking and behavior annotation files mars_features ml : Machine learning functions models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities video : Basic video tracking and behavior class that houses data.","title":"Modules"},{"location":"api-docs/#classes","text":"dl_features.Trainer dl_generators.MABe_Generator features.Features io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.MLDataFrame : DataFrame useful for interfacing between pandas and sklearn. Stores a data video.ExperimentDataFrame","title":"Classes"},{"location":"api-docs/#functions","text":"dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything dl_generators.features_distances dl_generators.features_distances_normalized dl_generators.features_identity dl_generators.features_mars dl_generators.features_mars_distr dl_generators.features_via_sklearn dl_generators.make_df dl_models.build_baseline_model feature_engineering.augment_features feature_engineering.boiler_plate feature_engineering.make_features_distances feature_engineering.make_features_mars feature_engineering.make_features_mars_distr feature_engineering.make_features_mars_reduced feature_engineering.make_features_social feature_engineering.make_features_velocities generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.rename_df_cols : Rename dataframe columns io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.clone_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_videodataset : Load ExperimentDataFrame from file. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api-docs/config/","text":"module config Configuration options for ethome functions. Global Variables global_config This file was automatically generated via lazydocs .","title":"Config"},{"location":"api-docs/config/#module-config","text":"Configuration options for ethome functions.","title":"module config"},{"location":"api-docs/config/#global-variables","text":"global_config This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/dl.dl_features/","text":"module dl.dl_features Global Variables sweeps_baseline feature_spaces has_keras THIS_FILE_DIR function seed_everything seed_everything(seed=2012) function normalize_data normalize_data(orig_pose_dictionary) function run_task run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True ) function lrs lrs(epoch, lr, freq=10) function convert_to_mars_format convert_to_mars_format(df, colnames, animal_setup) function convert_to_pandas_df convert_to_pandas_df(data, colnames=None) function compute_dl_probability_features compute_dl_probability_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) class Trainer method __init__ __init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fc19c9c5830>, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fc2d16bba70> ) method delete_model delete_model() method get_test_prediction_probabilities get_test_prediction_probabilities() method initialize_model initialize_model(**kwargs) method train train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"Dl.dl features"},{"location":"api-docs/dl.dl_features/#module-dldl_features","text":"","title":"module dl.dl_features"},{"location":"api-docs/dl.dl_features/#global-variables","text":"sweeps_baseline feature_spaces has_keras THIS_FILE_DIR","title":"Global Variables"},{"location":"api-docs/dl.dl_features/#function-seed_everything","text":"seed_everything(seed=2012)","title":"function seed_everything"},{"location":"api-docs/dl.dl_features/#function-normalize_data","text":"normalize_data(orig_pose_dictionary)","title":"function normalize_data"},{"location":"api-docs/dl.dl_features/#function-run_task","text":"run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True )","title":"function run_task"},{"location":"api-docs/dl.dl_features/#function-lrs","text":"lrs(epoch, lr, freq=10)","title":"function lrs"},{"location":"api-docs/dl.dl_features/#function-convert_to_mars_format","text":"convert_to_mars_format(df, colnames, animal_setup)","title":"function convert_to_mars_format"},{"location":"api-docs/dl.dl_features/#function-convert_to_pandas_df","text":"convert_to_pandas_df(data, colnames=None)","title":"function convert_to_pandas_df"},{"location":"api-docs/dl.dl_features/#function-compute_dl_probability_features","text":"compute_dl_probability_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs )","title":"function compute_dl_probability_features"},{"location":"api-docs/dl.dl_features/#class-trainer","text":"","title":"class Trainer"},{"location":"api-docs/dl.dl_features/#method-__init__","text":"__init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fc19c9c5830>, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fc2d16bba70> )","title":"method __init__"},{"location":"api-docs/dl.dl_features/#method-delete_model","text":"delete_model()","title":"method delete_model"},{"location":"api-docs/dl.dl_features/#method-get_test_prediction_probabilities","text":"get_test_prediction_probabilities()","title":"method get_test_prediction_probabilities"},{"location":"api-docs/dl.dl_features/#method-initialize_model","text":"initialize_model(**kwargs)","title":"method initialize_model"},{"location":"api-docs/dl.dl_features/#method-train","text":"train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"method train"},{"location":"api-docs/dl.dl_generators/","text":"module dl.dl_generators Global Variables has_keras function make_df make_df(pts, colnames=None) function features_identity features_identity(inputs) function features_via_sklearn features_via_sklearn(inputs, featurizer) function features_mars features_mars(x) function features_mars_distr features_mars_distr(x) function features_distances features_distances(inputs) function features_distances_normalized features_distances_normalized(inputs) class MABe_Generator method __init__ __init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fc2ac27c290> ) method augment_fn augment_fn(x) method on_epoch_end on_epoch_end() This file was automatically generated via lazydocs .","title":"Dl.dl generators"},{"location":"api-docs/dl.dl_generators/#module-dldl_generators","text":"","title":"module dl.dl_generators"},{"location":"api-docs/dl.dl_generators/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs/dl.dl_generators/#function-make_df","text":"make_df(pts, colnames=None)","title":"function make_df"},{"location":"api-docs/dl.dl_generators/#function-features_identity","text":"features_identity(inputs)","title":"function features_identity"},{"location":"api-docs/dl.dl_generators/#function-features_via_sklearn","text":"features_via_sklearn(inputs, featurizer)","title":"function features_via_sklearn"},{"location":"api-docs/dl.dl_generators/#function-features_mars","text":"features_mars(x)","title":"function features_mars"},{"location":"api-docs/dl.dl_generators/#function-features_mars_distr","text":"features_mars_distr(x)","title":"function features_mars_distr"},{"location":"api-docs/dl.dl_generators/#function-features_distances","text":"features_distances(inputs)","title":"function features_distances"},{"location":"api-docs/dl.dl_generators/#function-features_distances_normalized","text":"features_distances_normalized(inputs)","title":"function features_distances_normalized"},{"location":"api-docs/dl.dl_generators/#class-mabe_generator","text":"","title":"class MABe_Generator"},{"location":"api-docs/dl.dl_generators/#method-__init__","text":"__init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fc2ac27c290> )","title":"method __init__"},{"location":"api-docs/dl.dl_generators/#method-augment_fn","text":"augment_fn(x)","title":"method augment_fn"},{"location":"api-docs/dl.dl_generators/#method-on_epoch_end","text":"on_epoch_end() This file was automatically generated via lazydocs .","title":"method on_epoch_end"},{"location":"api-docs/dl.dl_models/","text":"module dl.dl_models Global Variables has_keras function build_baseline_model build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) This file was automatically generated via lazydocs .","title":"Dl.dl models"},{"location":"api-docs/dl.dl_models/#module-dldl_models","text":"","title":"module dl.dl_models"},{"location":"api-docs/dl.dl_models/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs/dl.dl_models/#function-build_baseline_model","text":"build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) This file was automatically generated via lazydocs .","title":"function build_baseline_model"},{"location":"api-docs/dl.feature_engineering/","text":"module dl.feature_engineering Global Variables XY_IDS function augment_features augment_features(window_size=5, n_shifts=3, mode='shift') function boiler_plate boiler_plate(features_df) function make_features_distances make_features_distances(df, animal_setup) function make_features_mars make_features_mars(df, animal_setup, n_shifts=3, mode='shift') function make_features_mars_distr make_features_mars_distr(x, y) function make_features_mars_reduced make_features_mars_reduced(df, animal_setup, n_shifts=2, mode='diff') function make_features_velocities make_features_velocities(df, animal_setup, n_shifts=5) function make_features_social make_features_social(df, animal_setup, n_shifts=3, mode='shift') This file was automatically generated via lazydocs .","title":"Dl.feature engineering"},{"location":"api-docs/dl.feature_engineering/#module-dlfeature_engineering","text":"","title":"module dl.feature_engineering"},{"location":"api-docs/dl.feature_engineering/#global-variables","text":"XY_IDS","title":"Global Variables"},{"location":"api-docs/dl.feature_engineering/#function-augment_features","text":"augment_features(window_size=5, n_shifts=3, mode='shift')","title":"function augment_features"},{"location":"api-docs/dl.feature_engineering/#function-boiler_plate","text":"boiler_plate(features_df)","title":"function boiler_plate"},{"location":"api-docs/dl.feature_engineering/#function-make_features_distances","text":"make_features_distances(df, animal_setup)","title":"function make_features_distances"},{"location":"api-docs/dl.feature_engineering/#function-make_features_mars","text":"make_features_mars(df, animal_setup, n_shifts=3, mode='shift')","title":"function make_features_mars"},{"location":"api-docs/dl.feature_engineering/#function-make_features_mars_distr","text":"make_features_mars_distr(x, y)","title":"function make_features_mars_distr"},{"location":"api-docs/dl.feature_engineering/#function-make_features_mars_reduced","text":"make_features_mars_reduced(df, animal_setup, n_shifts=2, mode='diff')","title":"function make_features_mars_reduced"},{"location":"api-docs/dl.feature_engineering/#function-make_features_velocities","text":"make_features_velocities(df, animal_setup, n_shifts=5)","title":"function make_features_velocities"},{"location":"api-docs/dl.feature_engineering/#function-make_features_social","text":"make_features_social(df, animal_setup, n_shifts=3, mode='shift') This file was automatically generated via lazydocs .","title":"function make_features_social"},{"location":"api-docs/dl.grid_searches/","text":"module dl.grid_searches Global Variables has_keras feature_spaces sweeps_baseline This file was automatically generated via lazydocs .","title":"Dl.grid searches"},{"location":"api-docs/dl.grid_searches/#module-dlgrid_searches","text":"","title":"module dl.grid_searches"},{"location":"api-docs/dl.grid_searches/#global-variables","text":"has_keras feature_spaces sweeps_baseline This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/dl/","text":"module dl This file was automatically generated via lazydocs .","title":"Dl"},{"location":"api-docs/dl/#module-dl","text":"This file was automatically generated via lazydocs .","title":"module dl"},{"location":"api-docs/features/","text":"module features Functions to take pose tracks and compute a set of features from them. To make your own feature creator: Create a function, e.g. create_custom_features , and provide the Features class a list of columns that are needed by this function to compute the features. The function create_custom_features has the form: create_custom_features(<df>, <raw_col_names>, <animal_setup>, **kwargs) Where: df is the dataframe to compute the features on raw_col_names is a list of the names of the columns in the dataframe that contain the raw data used for the feature creation. These are required for the model. animal_setup is a dictionary with keys bodypart_ids , mouse_ids , colnames . bodypart_ids is a list of the bodypart ids that are used in the dataframe mouse_ids is a list of the mouse ids that are used in the dataframe colnames is the list product(animals, XY_IDS, body_parts) **kwargs are extra arguments passed onto the feature creation function. The function returns: A dataframe, that only contains the new features. These will be added to the ExperimentDataFrame as columns. Once you have such a function defined, you can create a \"feature making object\" with custom_feature_maker = Features(create_custom_features, req_columns) This could be used on datasets as: dataset.add_features(custom_feature_maker, featureset_name = 'CUSTOM', add_to_features = True) Global Variables default_tracking_columns mars_feature_maker marsreduced_feature_maker cnn_probability_feature_maker social_feature_maker com_interanimal_feature_maker com_interanimal_speed_feature_maker com_feature_maker com_velocity_feature_maker speed_feature_maker distance_feature_maker class Features method __init__ __init__(feature_maker: Callable, required_columns: list, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: feature_maker : The function that will be used to compute the features. required_columns : The columns that are required to compute the features. method make make(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. This file was automatically generated via lazydocs .","title":"Features"},{"location":"api-docs/features/#module-features","text":"Functions to take pose tracks and compute a set of features from them. To make your own feature creator: Create a function, e.g. create_custom_features , and provide the Features class a list of columns that are needed by this function to compute the features. The function create_custom_features has the form: create_custom_features(<df>, <raw_col_names>, <animal_setup>, **kwargs) Where: df is the dataframe to compute the features on raw_col_names is a list of the names of the columns in the dataframe that contain the raw data used for the feature creation. These are required for the model. animal_setup is a dictionary with keys bodypart_ids , mouse_ids , colnames . bodypart_ids is a list of the bodypart ids that are used in the dataframe mouse_ids is a list of the mouse ids that are used in the dataframe colnames is the list product(animals, XY_IDS, body_parts) **kwargs are extra arguments passed onto the feature creation function. The function returns: A dataframe, that only contains the new features. These will be added to the ExperimentDataFrame as columns. Once you have such a function defined, you can create a \"feature making object\" with custom_feature_maker = Features(create_custom_features, req_columns) This could be used on datasets as: dataset.add_features(custom_feature_maker, featureset_name = 'CUSTOM', add_to_features = True)","title":"module features"},{"location":"api-docs/features/#global-variables","text":"default_tracking_columns mars_feature_maker marsreduced_feature_maker cnn_probability_feature_maker social_feature_maker com_interanimal_feature_maker com_interanimal_speed_feature_maker com_feature_maker com_velocity_feature_maker speed_feature_maker distance_feature_maker","title":"Global Variables"},{"location":"api-docs/features/#class-features","text":"","title":"class Features"},{"location":"api-docs/features/#method-__init__","text":"__init__(feature_maker: Callable, required_columns: list, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: feature_maker : The function that will be used to compute the features. required_columns : The columns that are required to compute the features.","title":"method __init__"},{"location":"api-docs/features/#method-make","text":"make(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. This file was automatically generated via lazydocs .","title":"method make"},{"location":"api-docs/generic_features/","text":"module generic_features Functions to take pose tracks and compute a set of features from them function compute_centerofmass_interanimal_distances compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_centerofmass_interanimal_speed compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass_velocity compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass compute_centerofmass( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_speed_features compute_speed_features( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Generic features"},{"location":"api-docs/generic_features/#module-generic_features","text":"Functions to take pose tracks and compute a set of features from them","title":"module generic_features"},{"location":"api-docs/generic_features/#function-compute_centerofmass_interanimal_distances","text":"compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_distances"},{"location":"api-docs/generic_features/#function-compute_centerofmass_interanimal_speed","text":"compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_speed"},{"location":"api-docs/generic_features/#function-compute_centerofmass_velocity","text":"compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_velocity"},{"location":"api-docs/generic_features/#function-compute_centerofmass","text":"compute_centerofmass( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass"},{"location":"api-docs/generic_features/#function-compute_speed_features","text":"compute_speed_features( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_speed_features"},{"location":"api-docs/generic_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_distance_features"},{"location":"api-docs/interpolation/","text":"module interpolation function interpolate_lowconf_points interpolate_lowconf_points( edf: ExperimentDataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : ExperimentDataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"Interpolation"},{"location":"api-docs/interpolation/#module-interpolation","text":"","title":"module interpolation"},{"location":"api-docs/interpolation/#function-interpolate_lowconf_points","text":"interpolate_lowconf_points( edf: ExperimentDataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : ExperimentDataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"function interpolate_lowconf_points"},{"location":"api-docs/io/","text":"module io Loading and saving tracking and behavior annotation files Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS function uniquifier uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order function save_sklearn_model save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to function load_sklearn_model load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model function read_DLC_tracks read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer function rename_df_cols rename_df_cols(df: DataFrame, renamer: dict) \u2192 DataFrame Rename dataframe columns Args: df : Pandas dataframe whose columns to rename renamer : dictionary whose key:value pairs define the substitutions to make Returns: The dataframe with renamed columns. function save_DLC_tracks_h5 save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe function load_data load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object. function get_sample_data_paths get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files function get_sample_data get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files function read_boris_annotation read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors. function create_behavior_labels create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors. class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"api-docs/io/#module-io","text":"Loading and saving tracking and behavior annotation files","title":"module io"},{"location":"api-docs/io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS","title":"Global Variables"},{"location":"api-docs/io/#function-uniquifier","text":"uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order","title":"function uniquifier"},{"location":"api-docs/io/#function-save_sklearn_model","text":"save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to","title":"function save_sklearn_model"},{"location":"api-docs/io/#function-load_sklearn_model","text":"load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model","title":"function load_sklearn_model"},{"location":"api-docs/io/#function-read_dlc_tracks","text":"read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer","title":"function read_DLC_tracks"},{"location":"api-docs/io/#function-rename_df_cols","text":"rename_df_cols(df: DataFrame, renamer: dict) \u2192 DataFrame Rename dataframe columns Args: df : Pandas dataframe whose columns to rename renamer : dictionary whose key:value pairs define the substitutions to make Returns: The dataframe with renamed columns.","title":"function rename_df_cols"},{"location":"api-docs/io/#function-save_dlc_tracks_h5","text":"save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe","title":"function save_DLC_tracks_h5"},{"location":"api-docs/io/#function-load_data","text":"load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object.","title":"function load_data"},{"location":"api-docs/io/#function-get_sample_data_paths","text":"get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files","title":"function get_sample_data_paths"},{"location":"api-docs/io/#function-get_sample_data","text":"get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files","title":"function get_sample_data"},{"location":"api-docs/io/#function-read_boris_annotation","text":"read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function read_boris_annotation"},{"location":"api-docs/io/#function-create_behavior_labels","text":"create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function create_behavior_labels"},{"location":"api-docs/io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"api-docs/io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"api-docs/io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"api-docs/io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"api-docs/io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"api-docs/mars_features/","text":"module mars_features function compute_mars_features compute_mars_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_mars_reduced_features compute_mars_reduced_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_social_features compute_social_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_velocity_features compute_velocity_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Mars features"},{"location":"api-docs/mars_features/#module-mars_features","text":"","title":"module mars_features"},{"location":"api-docs/mars_features/#function-compute_mars_features","text":"compute_mars_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_mars_features"},{"location":"api-docs/mars_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_distance_features"},{"location":"api-docs/mars_features/#function-compute_mars_reduced_features","text":"compute_mars_reduced_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_mars_reduced_features"},{"location":"api-docs/mars_features/#function-compute_social_features","text":"compute_social_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_social_features"},{"location":"api-docs/mars_features/#function-compute_velocity_features","text":"compute_velocity_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_velocity_features"},{"location":"api-docs/ml/","text":"module ml Machine learning functions This file was automatically generated via lazydocs .","title":"Ml"},{"location":"api-docs/ml/#module-ml","text":"Machine learning functions This file was automatically generated via lazydocs .","title":"module ml"},{"location":"api-docs/models/","text":"module models Basic video tracking and behavior class that houses data class HMMSklearn method __init__ __init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize property params method fit fit(X, y) method predict predict(X) class F1Optimizer method __init__ __init__(N=1000, labels=[1]) method fit fit(X, y) method fit_transform fit_transform(X, y=None) method predict predict(X) method predict_proba predict_proba(X) method transform transform(X) class ModelTransformer method __init__ __init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit() method fit fit(X, y) method fit_transform fit_transform(X, y=None) method transform transform(X) This file was automatically generated via lazydocs .","title":"Models"},{"location":"api-docs/models/#module-models","text":"Basic video tracking and behavior class that houses data","title":"module models"},{"location":"api-docs/models/#class-hmmsklearn","text":"","title":"class HMMSklearn"},{"location":"api-docs/models/#method-__init__","text":"__init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize","title":"method __init__"},{"location":"api-docs/models/#property-params","text":"","title":"property params"},{"location":"api-docs/models/#method-fit","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-predict","text":"predict(X)","title":"method predict"},{"location":"api-docs/models/#class-f1optimizer","text":"","title":"class F1Optimizer"},{"location":"api-docs/models/#method-__init___1","text":"__init__(N=1000, labels=[1])","title":"method __init__"},{"location":"api-docs/models/#method-fit_1","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-fit_transform","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs/models/#method-predict_1","text":"predict(X)","title":"method predict"},{"location":"api-docs/models/#method-predict_proba","text":"predict_proba(X)","title":"method predict_proba"},{"location":"api-docs/models/#method-transform","text":"transform(X)","title":"method transform"},{"location":"api-docs/models/#class-modeltransformer","text":"","title":"class ModelTransformer"},{"location":"api-docs/models/#method-__init___2","text":"__init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit()","title":"method __init__"},{"location":"api-docs/models/#method-fit_2","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-fit_transform_1","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs/models/#method-transform_1","text":"transform(X) This file was automatically generated via lazydocs .","title":"method transform"},{"location":"api-docs/plot/","text":"module plot Global Variables global_config function plot_embedding plot_embedding( dataset: ExperimentDataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_unsupervised_results plot_unsupervised_results( dataset: ExperimentDataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_ethogram plot_ethogram( dataset: ExperimentDataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects function create_ethogram_video create_ethogram_video( dataset: ExperimentDataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None function create_sample_videos create_sample_videos( dataset: ExperimentDataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None function create_mosaic_video create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None class MplColorHelper method __init__ __init__(cmap_name, start_val, stop_val) method get_rgb get_rgb(val) This file was automatically generated via lazydocs .","title":"Plot"},{"location":"api-docs/plot/#module-plot","text":"","title":"module plot"},{"location":"api-docs/plot/#global-variables","text":"global_config","title":"Global Variables"},{"location":"api-docs/plot/#function-plot_embedding","text":"plot_embedding( dataset: ExperimentDataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_embedding"},{"location":"api-docs/plot/#function-plot_unsupervised_results","text":"plot_unsupervised_results( dataset: ExperimentDataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_unsupervised_results"},{"location":"api-docs/plot/#function-plot_ethogram","text":"plot_ethogram( dataset: ExperimentDataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects","title":"function plot_ethogram"},{"location":"api-docs/plot/#function-create_ethogram_video","text":"create_ethogram_video( dataset: ExperimentDataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None","title":"function create_ethogram_video"},{"location":"api-docs/plot/#function-create_sample_videos","text":"create_sample_videos( dataset: ExperimentDataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None","title":"function create_sample_videos"},{"location":"api-docs/plot/#function-create_mosaic_video","text":"create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None","title":"function create_mosaic_video"},{"location":"api-docs/plot/#class-mplcolorhelper","text":"","title":"class MplColorHelper"},{"location":"api-docs/plot/#method-__init__","text":"__init__(cmap_name, start_val, stop_val)","title":"method __init__"},{"location":"api-docs/plot/#method-get_rgb","text":"get_rgb(val) This file was automatically generated via lazydocs .","title":"method get_rgb"},{"location":"api-docs/unsupervised/","text":"module unsupervised function compute_tsne_embedding compute_tsne_embedding( dataset: ExperimentDataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding function compute_morlet compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time. function compute_density compute_density( dataset: ExperimentDataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : ExperimentDataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts) function compute_watershed compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate. function cluster_behaviors cluster_behaviors( dataset: ExperimentDataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the ExperimentDataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"Unsupervised"},{"location":"api-docs/unsupervised/#module-unsupervised","text":"","title":"module unsupervised"},{"location":"api-docs/unsupervised/#function-compute_tsne_embedding","text":"compute_tsne_embedding( dataset: ExperimentDataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding","title":"function compute_tsne_embedding"},{"location":"api-docs/unsupervised/#function-compute_morlet","text":"compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time.","title":"function compute_morlet"},{"location":"api-docs/unsupervised/#function-compute_density","text":"compute_density( dataset: ExperimentDataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : ExperimentDataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts)","title":"function compute_density"},{"location":"api-docs/unsupervised/#function-compute_watershed","text":"compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate.","title":"function compute_watershed"},{"location":"api-docs/unsupervised/#function-cluster_behaviors","text":"cluster_behaviors( dataset: ExperimentDataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the ExperimentDataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"function cluster_behaviors"},{"location":"api-docs/utils/","text":"module utils Small helper utilities function checkFFMPEG checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api-docs/utils/#module-utils","text":"Small helper utilities","title":"module utils"},{"location":"api-docs/utils/#function-checkffmpeg","text":"checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"function checkFFMPEG"},{"location":"api-docs/video/","text":"module video Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class. A note on unit conversions For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed. Global Variables global_config UNIT_DICT function clone_metadata clone_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided function load_videodataset load_videodataset(fn_in: str) \u2192 ExperimentDataFrame Load ExperimentDataFrame from file. Args: fn_in : path to file to load Returns: ExperimentDataFrame object from pickle file function get_sample_openfield_data get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files class MLDataFrame DataFrame useful for interfacing between pandas and sklearn. Stores a data table and metadata dictionary. When feature columns, label columns and fold columns are specified then creates properties features, labels, folds and splitter that sklearn accepts for ML. method __init__ __init__( data: DataFrame, metadata: dict = {}, fold_cols=None, feature_cols=None, label_cols=None ) property features property folds property labels property splitter method add_data add_data(new_data, col_names) method save save(fn) class ExperimentDataFrame method __init__ __init__( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'clone_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed property features property folds property group property labels property n_videos property splitter property videos method activate_features_by_name activate_features_by_name(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated). method add_data add_data(new_data, col_names) method add_features add_features( feature_maker: Features, featureset_name: str, add_to_features=False, **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed method get_columns_regex get_columns_regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names method load load(fn_in: str) \u2192 None Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file. method make_movie make_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out' method remove_feature_cols remove_feature_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features. method remove_features_by_name remove_features_by_name(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns. method save save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path. method to_dlc_csv to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"Video"},{"location":"api-docs/video/#module-video","text":"Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class.","title":"module video"},{"location":"api-docs/video/#a-note-on-unit-conversions","text":"For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed.","title":"A note on unit conversions"},{"location":"api-docs/video/#global-variables","text":"global_config UNIT_DICT","title":"Global Variables"},{"location":"api-docs/video/#function-clone_metadata","text":"clone_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided","title":"function clone_metadata"},{"location":"api-docs/video/#function-load_videodataset","text":"load_videodataset(fn_in: str) \u2192 ExperimentDataFrame Load ExperimentDataFrame from file. Args: fn_in : path to file to load Returns: ExperimentDataFrame object from pickle file","title":"function load_videodataset"},{"location":"api-docs/video/#function-get_sample_openfield_data","text":"get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files","title":"function get_sample_openfield_data"},{"location":"api-docs/video/#class-mldataframe","text":"DataFrame useful for interfacing between pandas and sklearn. Stores a data table and metadata dictionary. When feature columns, label columns and fold columns are specified then creates properties features, labels, folds and splitter that sklearn accepts for ML.","title":"class MLDataFrame"},{"location":"api-docs/video/#method-__init__","text":"__init__( data: DataFrame, metadata: dict = {}, fold_cols=None, feature_cols=None, label_cols=None )","title":"method __init__"},{"location":"api-docs/video/#property-features","text":"","title":"property features"},{"location":"api-docs/video/#property-folds","text":"","title":"property folds"},{"location":"api-docs/video/#property-labels","text":"","title":"property labels"},{"location":"api-docs/video/#property-splitter","text":"","title":"property splitter"},{"location":"api-docs/video/#method-add_data","text":"add_data(new_data, col_names)","title":"method add_data"},{"location":"api-docs/video/#method-save","text":"save(fn)","title":"method save"},{"location":"api-docs/video/#class-ExperimentDataFrame","text":"","title":"class ExperimentDataFrame"},{"location":"api-docs/video/#method-__init___1","text":"__init__( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'clone_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed","title":"method __init__"},{"location":"api-docs/video/#property-features_1","text":"","title":"property features"},{"location":"api-docs/video/#property-folds_1","text":"","title":"property folds"},{"location":"api-docs/video/#property-group","text":"","title":"property group"},{"location":"api-docs/video/#property-labels_1","text":"","title":"property labels"},{"location":"api-docs/video/#property-n_videos","text":"","title":"property n_videos"},{"location":"api-docs/video/#property-splitter_1","text":"","title":"property splitter"},{"location":"api-docs/video/#property-videos","text":"","title":"property videos"},{"location":"api-docs/video/#method-activate_features_by_name","text":"activate_features_by_name(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated).","title":"method activate_features_by_name"},{"location":"api-docs/video/#method-add_data_1","text":"add_data(new_data, col_names)","title":"method add_data"},{"location":"api-docs/video/#method-add_features","text":"add_features( feature_maker: Features, featureset_name: str, add_to_features=False, **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed","title":"method add_features"},{"location":"api-docs/video/#method-get_columns_regex","text":"get_columns_regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names","title":"method get_columns_regex"},{"location":"api-docs/video/#method-load","text":"load(fn_in: str) \u2192 None Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file.","title":"method load"},{"location":"api-docs/video/#method-make_movie","text":"make_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out'","title":"method make_movie"},{"location":"api-docs/video/#method-remove_feature_cols","text":"remove_feature_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features.","title":"method remove_feature_cols"},{"location":"api-docs/video/#method-remove_features_by_name","text":"remove_features_by_name(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns.","title":"method remove_features_by_name"},{"location":"api-docs/video/#method-save_1","text":"save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path.","title":"method save"},{"location":"api-docs/video/#method-to_dlc_csv","text":"to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"method to_dlc_csv"}]}