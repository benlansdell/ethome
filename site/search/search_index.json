{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ethome Machine learning for animal behavior. Interprets pose-tracking files (currently only from DLC) and behavior annotations (currently only from BORIS) to train a behavior classifier, perform unsupervised learning, and other common analysis tasks. Features Interpolate DLC data Create generic features for kinematic analysis and downstream ML tasks Create features specifically for mouse resident-intruder setup Read in DLC pose data and corresponding BORIS behavior annotations to make supervised learning easy Perform unsupervised learning on pose data to extract discrete behavioral motifs (MotionMapper) Quickly generate a movie with behavior predictions Installation pip install ethome-ml Can install optional extras with: pip install numpy, cython pip install ethome-ml[all] This includes matplotlib, keras, and Linderman lab's state-space model package, ssm . Note that installing ssm requires cython and numpy for the build, so must be already present in the environment. Quickstart Import from glob import glob from ethome import create_dataset, create_metadata from ethome.features import CNN1DProb, MARS from ethome.io import get_sample_data_paths Gather the DLC and BORIS tracking and annotation files tracking_files, boris_files = get_sample_data_paths() Setup some parameters frame_width = 20 # (float) length of entire horizontal shot frame_width_units = 'in' # (str) units frame_width is given in fps = 30 # (int) frames per second resolution = (1200, 1600) # (tuple) HxW in pixels Create a parameter object and video dataset metadata = create_metadata(tracking_files, labels = boris_files, frame_width = frame_width, fps = fps, frame_width_units = frame_width_units, resolution = resolution) animal_renamer = {'adult': 'resident', 'juvenile':'intruder'} dataset = create_dataset(metadata, animal_renamer=animal_renamer) Now create features on this dataset. Feature creation objects are class instances, similar to sk-learn: cnn_probabilities = CNN1DProb() mars = MARS() dataset.features.add(cnn_probabilities, featureset_name = '1dcnn', add_to_features = True) dataset.features.add(mars, featureset_name = 'MARS', add_to_features = True) Now access a features table, labels, and groups for learning with dataset.ml.features, dataset.ml.labels, dataset.ml.group . From here it's easy to use some ML libraries to predict behavior. For example: from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, dataset.ml.features, dataset.ml.labels, dataset.ml.group) Read more in the how-to guide in the docs.","title":"Home"},{"location":"#ethome","text":"Machine learning for animal behavior. Interprets pose-tracking files (currently only from DLC) and behavior annotations (currently only from BORIS) to train a behavior classifier, perform unsupervised learning, and other common analysis tasks.","title":"Ethome"},{"location":"#features","text":"Interpolate DLC data Create generic features for kinematic analysis and downstream ML tasks Create features specifically for mouse resident-intruder setup Read in DLC pose data and corresponding BORIS behavior annotations to make supervised learning easy Perform unsupervised learning on pose data to extract discrete behavioral motifs (MotionMapper) Quickly generate a movie with behavior predictions","title":"Features"},{"location":"#installation","text":"pip install ethome-ml Can install optional extras with: pip install numpy, cython pip install ethome-ml[all] This includes matplotlib, keras, and Linderman lab's state-space model package, ssm . Note that installing ssm requires cython and numpy for the build, so must be already present in the environment.","title":"Installation"},{"location":"#quickstart","text":"Import from glob import glob from ethome import create_dataset, create_metadata from ethome.features import CNN1DProb, MARS from ethome.io import get_sample_data_paths Gather the DLC and BORIS tracking and annotation files tracking_files, boris_files = get_sample_data_paths() Setup some parameters frame_width = 20 # (float) length of entire horizontal shot frame_width_units = 'in' # (str) units frame_width is given in fps = 30 # (int) frames per second resolution = (1200, 1600) # (tuple) HxW in pixels Create a parameter object and video dataset metadata = create_metadata(tracking_files, labels = boris_files, frame_width = frame_width, fps = fps, frame_width_units = frame_width_units, resolution = resolution) animal_renamer = {'adult': 'resident', 'juvenile':'intruder'} dataset = create_dataset(metadata, animal_renamer=animal_renamer) Now create features on this dataset. Feature creation objects are class instances, similar to sk-learn: cnn_probabilities = CNN1DProb() mars = MARS() dataset.features.add(cnn_probabilities, featureset_name = '1dcnn', add_to_features = True) dataset.features.add(mars, featureset_name = 'MARS', add_to_features = True) Now access a features table, labels, and groups for learning with dataset.ml.features, dataset.ml.labels, dataset.ml.group . From here it's easy to use some ML libraries to predict behavior. For example: from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, dataset.ml.features, dataset.ml.labels, dataset.ml.group) Read more in the how-to guide in the docs.","title":"Quickstart"},{"location":"how-to/","text":"How to This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. A very basic outline is also in the quick start section of the readme. Note also that this guide covers basic usage; it doesn't comprehensively describe how to use every function or feature in ethome , you can consult the API docs for more information on usage. 1 Getting started ethome makes it easy to perform common machine learning analyses on pose-tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. At present DeepLabCut is supported. The first task is to load this data into a data structure and associate metadata with it. The basic functionality of the package can be seen as using an extended pandas DataFrame , with associated support functions that are suited for behavior analysis. The DataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. To create the dataframe, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s -- this is how each video is identified. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' fps = 30 resolution = (1200, 1600) metadata = {tracking_csv : {'fps': fps, 'resolution': resolution}} Beyond providing the fps for each video, all other fields are optional. 1a Helper function for making metadata dictionary Often you'll have many videos that have the same metadata, in that case you can easily create an appropriate dictionary with the helper function create_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata now has two entries, one for each video, each listing the same FPS and resolution. Any keyword that is an iterable of the same length as the tracking files is zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to create_metadata : tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, labels = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30 ) to all videos, the entry labeled_data[i] would then be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately. 1b Special fields The labels field is special. If it is provided, then it is treated as housing behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . The video field is also special. You should use it to provide a path to the corresponding video that was tracked. This will be used by some of the visualization functions. For each video, the fps field must be provided, so that frame numbers can be converted into times. 1c Scaling pose data There is some support for scaling the data to get it into desired units, consistent across all recordings. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . By default, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. You can specify the units by providing the 'units' key yourself. Supported units include: 'mm', 'cm', 'm', 'in', 'ft'. 1d Making the data frame Once you have the metadata dictionary prepared, you can easily create a DataFrame as: recordings = create_dataset(metadata) This creates a pandas dataframe, recordings , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata . If your DLC project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = create_dataset(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts -- you can provide a part_renamer dictionary. 1e Metadata When recordings is created, additional metadata is computed and accessible via: * recordings.metadata houses the following attributes: * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC 2 Interpolate low-confidence pose tracking Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and better tracks may be obtained by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with interpolate_lowconf_points(recordings) 3 Generate features To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in a few different ways. You can either use one of the feature-making functions provided or create a custom feature-making function, or custom class. To use the inbuilt functions you can reference them by identifying string, or provide the function itself to the features.add function. For instance, to compute the distances between all body parts (within and between animals), you could do: The basic pattern is shown in this example: recordings.features.add('distances') This will compute and add the distances between all body parts of all animals. 3a In-built support for resident-intruder setup First, if your setup is a social mouse study, involving two mice, similar enough to the standard resident-intruder setup, then you can use some pre-designed feature sets. The body parts that are tracked must be those from the MARS dataset (See figure). You will have to have labeled and tracked your mice in DLC in the same way. (with the same animal and body part names -- those ethome 's create_dataset function can rename them appropriately) The cnn1d_prob , mars , mars_reduced and social functions can be used to make features for this setup. cnn1d_prob runs a 1D CNN and outputs prediction probabilities of three behaviors (attack, mount, and investigation). Even if you're not interested in these exact behaviors, they may still be useful for predicting the occurance of other behaviors, as part of an ensemble model. mars computes a long list of features as used in the MARS paper. You can refer to that paper for more details. mars_reduced is a reduced version of the MARS features social is the subset of MARS features that only involve measures of one animal in relation to the other. 3b Generic features You can generate more generic features using the following functions: * centroid the centroid of each animal's body parts * centroid_velocity the velocity of the centroids * centroid_interanimal the distances between the centroids of all the animals * centroid_interanimal_speed the rate of change of centroid_interanimal * speeds the speeds of all body parts * distances the distances between all animals body parts (inter- and intra-animal) These classes work for any animal setup, not just resident-intruder with specific body parts, as assumed for the mars features. 3c Add your own features There are two ways to add your own feature sets to your DataFrame. The first is to create a function that takes a pandas DataFrame, and returns a new DataFrame with the features you want to add. For example: def diff_cols(self, df, required_columns = []): return df[required_columns].diff() recordings.features.add(diff_cols, required_columns = ['resident_neck_x', 'resident_neck_y']) The second is to create a class that has, at the least, the method transform . class BodyPartDiff: def __init__(self, required_columns): self.required_columns = required_columns def transform(self, df): return df[self.required_columns].diff() head_diff = BodyPartDiff(['resident_neck_x', 'resident_neck_y']) recordings.features.add(head_diff) This is more verbose than the above, but has the advantage that the it can be re-used. E.g. you could fit the instance to training data and apply it to test data, similar to sklearn's approach. 3d Features manipulation By default, when new features are added to the dataframe, they are considered 'active'. Active features can be accessed through recordings.ml.features You can pass this to any ML method for further processing. This is just a convenience for managing the long list of features you will have created in the steps above. You can always just treat recordings like a pandas DataFrame and do ML how you would normally. To activate features you can use recordings.features.activate , and to deactivate features you can use recordings.features.deactivate . Deactivating keeps them in the DataTable, but just no longer includes those features in recordings.ml.features . 4 Fit a model for behavior classification Ok! The hard work is done, so now you can easily train a behavior classifier based on the features you've computed and the labels provided. E.g. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, recording.ml.features, recording.ml.labels, recordings.ml.group) 5 Perform unsupervised learning STILL UNDER CONSTRUCTION 6 Make output movies Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos: dataset.io.save_movie(['label', 'prediction'], '.') The video field in the metadata , specifying the path to the underlying video, has to be present for each recording for this to work. 7 Save your data You can save your data as a pickled DataFrame with recordings.io.save('outfile.pkl') (and can be loaded again with:) recordings = pd.DataFrame.io.load('outfile.pkl') 8 Summary and reference list of added functionality by ethome For reference, the metadata and added functions added to the dataframe are: * recordings.metadata , which houses * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC * recordings.features , feature creation and manipulation * activate : activate columns by name * deactivate : deactivate columns by name * regex : select column names based on regex * add : create new features * recordings.ml , machine learning conveniences * features : a 'active' set of features * labels : if loaded from BORIS, behavior labels. from text to label with recordings.metadata.label_key * group : the corresponding video filename for all rows in the table -- can be used for GroupKFold CV, or similar * recordings.io , I/O functions * save : save DataFrame as pickle file * to_dlc_csv : save original tracking data back into csv -- if you interpolated or otherwise manipulated the data * load : load DataFrame from pickle file * save_movie : create a movie with some feature column you indicate overlaid See the docs for usage details.","title":"How to"},{"location":"how-to/#how-to","text":"This guide covers the all of the tasks you can perform with this package, roughly in the order you'd want to do them. A very basic outline is also in the quick start section of the readme. Note also that this guide covers basic usage; it doesn't comprehensively describe how to use every function or feature in ethome , you can consult the API docs for more information on usage.","title":"How to"},{"location":"how-to/#1-getting-started","text":"ethome makes it easy to perform common machine learning analyses on pose-tracking data, perhaps in combination with behavioral annotations. The key thing you need to get started, then, is pose tracking data. At present DeepLabCut is supported. The first task is to load this data into a data structure and associate metadata with it. The basic functionality of the package can be seen as using an extended pandas DataFrame , with associated support functions that are suited for behavior analysis. The DataFrame object will house data from one or more video's worth of pose data, along with associated metadata for each video. To create the dataframe, you'll need to provide metadata for each video you want to analyze. For this, you create a metadata dictionary. This is a dictionary whose keys are paths to pose-tracking DLC .csv s -- this is how each video is identified. The value of each entry is a dictionary that provides details about that video. For instance, you may have: tracking_csv = './dlc_tracking_file.csv' fps = 30 resolution = (1200, 1600) metadata = {tracking_csv : {'fps': fps, 'resolution': resolution}} Beyond providing the fps for each video, all other fields are optional.","title":"1 Getting started"},{"location":"how-to/#1a-helper-function-for-making-metadata-dictionary","text":"Often you'll have many videos that have the same metadata, in that case you can easily create an appropriate dictionary with the helper function create_metadata . Say you now have two tracking files, each with the same FPS and resolution. You can make the corresponding metadata dictionary with: tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, fps = fps, resolution = resolution) The metadata now has two entries, one for each video, each listing the same FPS and resolution. Any keyword that is an iterable of the same length as the tracking files is zipped with the tracking files accordingly. That is, if you also have behavioral annotations provided by BORIS for each of the videos, then you should prepare a list labeled_data and provide that to create_metadata : tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv'] labeled_data = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv'] fps = 30 resolution = (1200, 1600) metadata = create_metadata(tracking_csvs, labels = labeled_data, fps = fps, resolution = resolution) Rather than assigning the same value (e.g. fps = 30 ) to all videos, the entry labeled_data[i] would then be associated with tracking_csvs[i] . These lists, therefore, must be sorted appropriately.","title":"1a Helper function for making metadata dictionary"},{"location":"how-to/#1b-special-fields","text":"The labels field is special. If it is provided, then it is treated as housing behavioral annotations exported from a corresponding BORIS project. The package loads these behavior annotations and adds them to the data frame with the field label . The video field is also special. You should use it to provide a path to the corresponding video that was tracked. This will be used by some of the visualization functions. For each video, the fps field must be provided, so that frame numbers can be converted into times.","title":"1b Special fields"},{"location":"how-to/#1c-scaling-pose-data","text":"There is some support for scaling the data to get it into desired units, consistent across all recordings. If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields frame_width , resolution , and frame_width_units . If you want to keep track of the units, you can add a units key to the metadata. This could be pixels , cm , etc, as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide frame_width , frame_width_units and resolution for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. resolution is a tuple (H,W) in pixels of the videos and frame_width is the width of the image, in units frame_width_units . By default, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. You can specify the units by providing the 'units' key yourself. Supported units include: 'mm', 'cm', 'm', 'in', 'ft'.","title":"1c Scaling pose data"},{"location":"how-to/#1d-making-the-data-frame","text":"Once you have the metadata dictionary prepared, you can easily create a DataFrame as: recordings = create_dataset(metadata) This creates a pandas dataframe, recordings , that contains pose data, and perhaps behavior annotations, from all the videos listed in metadata . If your DLC project named the animals some way, but you want them named another way in this dataframe, you can provide an animal_renamer dictionary as an argument to the constructor: recordings = create_dataset(metadata, animal_renamer={'adult': 'resident', 'juvenile':'intruder'}) Similarly with the body parts -- you can provide a part_renamer dictionary.","title":"1d Making the data frame"},{"location":"how-to/#1e-metadata","text":"When recordings is created, additional metadata is computed and accessible via: * recordings.metadata houses the following attributes: * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC","title":"1e Metadata"},{"location":"how-to/#2-interpolate-low-confidence-pose-tracking","text":"Some simple support for interpolating low-confidence tracks in DLC is provided. Often predicted locations below a given confidence level are noisy and unreliable, and better tracks may be obtained by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. You can achieve this with interpolate_lowconf_points(recordings)","title":"2 Interpolate low-confidence pose tracking"},{"location":"how-to/#3-generate-features","text":"To do machine learning you'll want to create features from the pose tracking data. ethome can help you do this in a few different ways. You can either use one of the feature-making functions provided or create a custom feature-making function, or custom class. To use the inbuilt functions you can reference them by identifying string, or provide the function itself to the features.add function. For instance, to compute the distances between all body parts (within and between animals), you could do: The basic pattern is shown in this example: recordings.features.add('distances') This will compute and add the distances between all body parts of all animals.","title":"3 Generate features"},{"location":"how-to/#3a-in-built-support-for-resident-intruder-setup","text":"First, if your setup is a social mouse study, involving two mice, similar enough to the standard resident-intruder setup, then you can use some pre-designed feature sets. The body parts that are tracked must be those from the MARS dataset (See figure). You will have to have labeled and tracked your mice in DLC in the same way. (with the same animal and body part names -- those ethome 's create_dataset function can rename them appropriately) The cnn1d_prob , mars , mars_reduced and social functions can be used to make features for this setup. cnn1d_prob runs a 1D CNN and outputs prediction probabilities of three behaviors (attack, mount, and investigation). Even if you're not interested in these exact behaviors, they may still be useful for predicting the occurance of other behaviors, as part of an ensemble model. mars computes a long list of features as used in the MARS paper. You can refer to that paper for more details. mars_reduced is a reduced version of the MARS features social is the subset of MARS features that only involve measures of one animal in relation to the other.","title":"3a In-built support for resident-intruder setup"},{"location":"how-to/#3b-generic-features","text":"You can generate more generic features using the following functions: * centroid the centroid of each animal's body parts * centroid_velocity the velocity of the centroids * centroid_interanimal the distances between the centroids of all the animals * centroid_interanimal_speed the rate of change of centroid_interanimal * speeds the speeds of all body parts * distances the distances between all animals body parts (inter- and intra-animal) These classes work for any animal setup, not just resident-intruder with specific body parts, as assumed for the mars features.","title":"3b Generic features"},{"location":"how-to/#3c-add-your-own-features","text":"There are two ways to add your own feature sets to your DataFrame. The first is to create a function that takes a pandas DataFrame, and returns a new DataFrame with the features you want to add. For example: def diff_cols(self, df, required_columns = []): return df[required_columns].diff() recordings.features.add(diff_cols, required_columns = ['resident_neck_x', 'resident_neck_y']) The second is to create a class that has, at the least, the method transform . class BodyPartDiff: def __init__(self, required_columns): self.required_columns = required_columns def transform(self, df): return df[self.required_columns].diff() head_diff = BodyPartDiff(['resident_neck_x', 'resident_neck_y']) recordings.features.add(head_diff) This is more verbose than the above, but has the advantage that the it can be re-used. E.g. you could fit the instance to training data and apply it to test data, similar to sklearn's approach.","title":"3c Add your own features"},{"location":"how-to/#3d-features-manipulation","text":"By default, when new features are added to the dataframe, they are considered 'active'. Active features can be accessed through recordings.ml.features You can pass this to any ML method for further processing. This is just a convenience for managing the long list of features you will have created in the steps above. You can always just treat recordings like a pandas DataFrame and do ML how you would normally. To activate features you can use recordings.features.activate , and to deactivate features you can use recordings.features.deactivate . Deactivating keeps them in the DataTable, but just no longer includes those features in recordings.ml.features .","title":"3d Features manipulation"},{"location":"how-to/#4-fit-a-model-for-behavior-classification","text":"Ok! The hard work is done, so now you can easily train a behavior classifier based on the features you've computed and the labels provided. E.g. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score model = RandomForestClassifier() cross_val_score(model, recording.ml.features, recording.ml.labels, recordings.ml.group)","title":"4 Fit a model for behavior classification"},{"location":"how-to/#5-perform-unsupervised-learning","text":"STILL UNDER CONSTRUCTION","title":"5 Perform unsupervised learning"},{"location":"how-to/#6-make-output-movies","text":"Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos: dataset.io.save_movie(['label', 'prediction'], '.') The video field in the metadata , specifying the path to the underlying video, has to be present for each recording for this to work.","title":"6 Make output movies"},{"location":"how-to/#7-save-your-data","text":"You can save your data as a pickled DataFrame with recordings.io.save('outfile.pkl') (and can be loaded again with:) recordings = pd.DataFrame.io.load('outfile.pkl')","title":"7 Save your data"},{"location":"how-to/#8-summary-and-reference-list-of-added-functionality-by-ethome","text":"For reference, the metadata and added functions added to the dataframe are: * recordings.metadata , which houses * details : the metadata dictionary given to create_dataset * videos : list of videos given in metadata * n_videos : number of videos in DataFrame * label_key : associates numbers with text labels for each behavior * reverse_label_key : associates text labels for each behavior number * recordings.pose , houses pose information: * body_parts : list of body parts loaded from the DLC file(s) * animals : list of animals loaded from the DLC files(s) * animal_setup : dictionary detailing animal parts and names * raw_track_columns : all original columns names loaded from DLC * recordings.features , feature creation and manipulation * activate : activate columns by name * deactivate : deactivate columns by name * regex : select column names based on regex * add : create new features * recordings.ml , machine learning conveniences * features : a 'active' set of features * labels : if loaded from BORIS, behavior labels. from text to label with recordings.metadata.label_key * group : the corresponding video filename for all rows in the table -- can be used for GroupKFold CV, or similar * recordings.io , I/O functions * save : save DataFrame as pickle file * to_dlc_csv : save original tracking data back into csv -- if you interpolated or otherwise manipulated the data * load : load DataFrame from pickle file * save_movie : create a movie with some feature column you indicate overlaid See the docs for usage details.","title":"8 Summary and reference list of added functionality by ethome"},{"location":"api-docs/","text":"API Overview Modules config : Configuration options for ethome functions. features features.cnn1d features.dl_features features.features : Functions to take pose tracks and compute a set of features from them. features.generic_features : Functions to take pose tracks and compute a set of features from them features.mars_features interpolation io : Loading and saving tracking and behavior annotation files models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities version video : Basic video tracking and behavior class that houses data. Classes cnn1d.MABe_Generator dl_features.Trainer features.CNN1DProb features.Centroid features.CentroidInteranimal features.CentroidInteranimalSpeed features.CentroidVelocity features.Distances features.Features features.MARSFeatures features.MARSReduced features.Social features.Speeds io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor Functions cnn1d.build_baseline_model cnn1d.features_distances cnn1d.features_distances_normalized cnn1d.features_identity cnn1d.features_mars cnn1d.features_mars_distr cnn1d.features_via_sklearn cnn1d.make_df dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything features.CNN1DProb.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CNN1DProb.fit features.CNN1DProb.fit_transform features.CNN1DProb.transform : Make the features. This is called internally by the dataset object when running add_features . features.Centroid.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Centroid.fit features.Centroid.fit_transform features.Centroid.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimal.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimal.fit features.CentroidInteranimal.fit_transform features.CentroidInteranimal.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimalSpeed.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimalSpeed.fit features.CentroidInteranimalSpeed.fit_transform features.CentroidInteranimalSpeed.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidVelocity.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidVelocity.fit features.CentroidVelocity.fit_transform features.CentroidVelocity.transform : Make the features. This is called internally by the dataset object when running add_features . features.Distances.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Distances.fit features.Distances.fit_transform features.Distances.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSFeatures.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSFeatures.fit features.MARSFeatures.fit_transform features.MARSFeatures.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSReduced.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSReduced.fit features.MARSReduced.fit_transform features.MARSReduced.transform : Make the features. This is called internally by the dataset object when running add_features . features.Social.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Social.fit features.Social.fit_transform features.Social.transform : Make the features. This is called internally by the dataset object when running add_features . features.Speeds.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Speeds.fit features.Speeds.fit_transform features.Speeds.transform : Make the features. This is called internally by the dataset object when running add_features . features.feature_class_maker generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features mars_features.augment_features mars_features.boiler_plate mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features mars_features.make_features_distances mars_features.make_features_mars mars_features.make_features_mars_distr mars_features.make_features_mars_reduced mars_features.make_features_social mars_features.make_features_velocities interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api-docs/#api-overview","text":"","title":"API Overview"},{"location":"api-docs/#modules","text":"config : Configuration options for ethome functions. features features.cnn1d features.dl_features features.features : Functions to take pose tracks and compute a set of features from them. features.generic_features : Functions to take pose tracks and compute a set of features from them features.mars_features interpolation io : Loading and saving tracking and behavior annotation files models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities version video : Basic video tracking and behavior class that houses data.","title":"Modules"},{"location":"api-docs/#classes","text":"cnn1d.MABe_Generator dl_features.Trainer features.CNN1DProb features.Centroid features.CentroidInteranimal features.CentroidInteranimalSpeed features.CentroidVelocity features.Distances features.Features features.MARSFeatures features.MARSReduced features.Social features.Speeds io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor","title":"Classes"},{"location":"api-docs/#functions","text":"cnn1d.build_baseline_model cnn1d.features_distances cnn1d.features_distances_normalized cnn1d.features_identity cnn1d.features_mars cnn1d.features_mars_distr cnn1d.features_via_sklearn cnn1d.make_df dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything features.CNN1DProb.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CNN1DProb.fit features.CNN1DProb.fit_transform features.CNN1DProb.transform : Make the features. This is called internally by the dataset object when running add_features . features.Centroid.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Centroid.fit features.Centroid.fit_transform features.Centroid.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimal.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimal.fit features.CentroidInteranimal.fit_transform features.CentroidInteranimal.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimalSpeed.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimalSpeed.fit features.CentroidInteranimalSpeed.fit_transform features.CentroidInteranimalSpeed.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidVelocity.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidVelocity.fit features.CentroidVelocity.fit_transform features.CentroidVelocity.transform : Make the features. This is called internally by the dataset object when running add_features . features.Distances.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Distances.fit features.Distances.fit_transform features.Distances.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSFeatures.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSFeatures.fit features.MARSFeatures.fit_transform features.MARSFeatures.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSReduced.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSReduced.fit features.MARSReduced.fit_transform features.MARSReduced.transform : Make the features. This is called internally by the dataset object when running add_features . features.Social.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Social.fit features.Social.fit_transform features.Social.transform : Make the features. This is called internally by the dataset object when running add_features . features.Speeds.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Speeds.fit features.Speeds.fit_transform features.Speeds.transform : Make the features. This is called internally by the dataset object when running add_features . features.feature_class_maker generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features mars_features.augment_features mars_features.boiler_plate mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features mars_features.make_features_distances mars_features.make_features_mars mars_features.make_features_mars_distr mars_features.make_features_mars_reduced mars_features.make_features_social mars_features.make_features_velocities interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api-docs/config/","text":"module config Configuration options for ethome functions. Global Variables global_config This file was automatically generated via lazydocs .","title":"Config"},{"location":"api-docs/config/#module-config","text":"Configuration options for ethome functions.","title":"module config"},{"location":"api-docs/config/#global-variables","text":"global_config This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/features.cnn1d/","text":"module features.cnn1d Global Variables has_keras function build_baseline_model build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) function make_df make_df(pts, colnames=None) function features_identity features_identity(inputs) function features_via_sklearn features_via_sklearn(inputs, featurizer) function features_mars features_mars(x) function features_mars_distr features_mars_distr(x) function features_distances features_distances(inputs) function features_distances_normalized features_distances_normalized(inputs) class MABe_Generator method __init__ __init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fbe20a39200> ) method augment_fn augment_fn(x) method on_epoch_end on_epoch_end() This file was automatically generated via lazydocs .","title":"Features.cnn1d"},{"location":"api-docs/features.cnn1d/#module-featurescnn1d","text":"","title":"module features.cnn1d"},{"location":"api-docs/features.cnn1d/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs/features.cnn1d/#function-build_baseline_model","text":"build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None )","title":"function build_baseline_model"},{"location":"api-docs/features.cnn1d/#function-make_df","text":"make_df(pts, colnames=None)","title":"function make_df"},{"location":"api-docs/features.cnn1d/#function-features_identity","text":"features_identity(inputs)","title":"function features_identity"},{"location":"api-docs/features.cnn1d/#function-features_via_sklearn","text":"features_via_sklearn(inputs, featurizer)","title":"function features_via_sklearn"},{"location":"api-docs/features.cnn1d/#function-features_mars","text":"features_mars(x)","title":"function features_mars"},{"location":"api-docs/features.cnn1d/#function-features_mars_distr","text":"features_mars_distr(x)","title":"function features_mars_distr"},{"location":"api-docs/features.cnn1d/#function-features_distances","text":"features_distances(inputs)","title":"function features_distances"},{"location":"api-docs/features.cnn1d/#function-features_distances_normalized","text":"features_distances_normalized(inputs)","title":"function features_distances_normalized"},{"location":"api-docs/features.cnn1d/#class-mabe_generator","text":"","title":"class MABe_Generator"},{"location":"api-docs/features.cnn1d/#method-__init__","text":"__init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fbe20a39200> )","title":"method __init__"},{"location":"api-docs/features.cnn1d/#method-augment_fn","text":"augment_fn(x)","title":"method augment_fn"},{"location":"api-docs/features.cnn1d/#method-on_epoch_end","text":"on_epoch_end() This file was automatically generated via lazydocs .","title":"method on_epoch_end"},{"location":"api-docs/features.dl_features/","text":"module features.dl_features Global Variables has_keras THIS_FILE_DIR default_config feature_spaces sweeps_baseline function seed_everything seed_everything(seed=2012) function normalize_data normalize_data(orig_pose_dictionary) function run_task run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True ) function lrs lrs(epoch, lr, freq=10) function convert_to_mars_format convert_to_mars_format(df, colnames, animal_setup) function convert_to_pandas_df convert_to_pandas_df(data, colnames=None) function compute_dl_probability_features compute_dl_probability_features(df: DataFrame, raw_col_names: list, **kwargs) class Trainer method __init__ __init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fbe61fb2050>, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fbe20a39200> ) method delete_model delete_model() method get_test_prediction_probabilities get_test_prediction_probabilities() method initialize_model initialize_model(**kwargs) method train train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"Features.dl features"},{"location":"api-docs/features.dl_features/#module-featuresdl_features","text":"","title":"module features.dl_features"},{"location":"api-docs/features.dl_features/#global-variables","text":"has_keras THIS_FILE_DIR default_config feature_spaces sweeps_baseline","title":"Global Variables"},{"location":"api-docs/features.dl_features/#function-seed_everything","text":"seed_everything(seed=2012)","title":"function seed_everything"},{"location":"api-docs/features.dl_features/#function-normalize_data","text":"normalize_data(orig_pose_dictionary)","title":"function normalize_data"},{"location":"api-docs/features.dl_features/#function-run_task","text":"run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True )","title":"function run_task"},{"location":"api-docs/features.dl_features/#function-lrs","text":"lrs(epoch, lr, freq=10)","title":"function lrs"},{"location":"api-docs/features.dl_features/#function-convert_to_mars_format","text":"convert_to_mars_format(df, colnames, animal_setup)","title":"function convert_to_mars_format"},{"location":"api-docs/features.dl_features/#function-convert_to_pandas_df","text":"convert_to_pandas_df(data, colnames=None)","title":"function convert_to_pandas_df"},{"location":"api-docs/features.dl_features/#function-compute_dl_probability_features","text":"compute_dl_probability_features(df: DataFrame, raw_col_names: list, **kwargs)","title":"function compute_dl_probability_features"},{"location":"api-docs/features.dl_features/#class-trainer","text":"","title":"class Trainer"},{"location":"api-docs/features.dl_features/#method-__init__","text":"__init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fbe61fb2050>, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fbe20a39200> )","title":"method __init__"},{"location":"api-docs/features.dl_features/#method-delete_model","text":"delete_model()","title":"method delete_model"},{"location":"api-docs/features.dl_features/#method-get_test_prediction_probabilities","text":"get_test_prediction_probabilities()","title":"method get_test_prediction_probabilities"},{"location":"api-docs/features.dl_features/#method-initialize_model","text":"initialize_model(**kwargs)","title":"method initialize_model"},{"location":"api-docs/features.dl_features/#method-train","text":"train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"method train"},{"location":"api-docs/features.features/","text":"module features.features Functions to take pose tracks and compute a set of features from them. Global Variables default_tracking_columns FEATURE_MAKERS function feature_class_maker feature_class_maker(name, compute_function, required_columns=[]) class CNN1DProb function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Centroid function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidInteranimal function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidInteranimalSpeed function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidVelocity function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Distances function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class MARSFeatures function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class MARSReduced function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Social function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Speeds function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Features method __init__ __init__() method transform transform(df) This file was automatically generated via lazydocs .","title":"Features.features"},{"location":"api-docs/features.features/#module-featuresfeatures","text":"Functions to take pose tracks and compute a set of features from them.","title":"module features.features"},{"location":"api-docs/features.features/#global-variables","text":"default_tracking_columns FEATURE_MAKERS","title":"Global Variables"},{"location":"api-docs/features.features/#function-feature_class_maker","text":"feature_class_maker(name, compute_function, required_columns=[])","title":"function feature_class_maker"},{"location":"api-docs/features.features/#class-cnn1dprob","text":"","title":"class CNN1DProb"},{"location":"api-docs/features.features/#function-__init__","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-centroid","text":"","title":"class Centroid"},{"location":"api-docs/features.features/#function-__init___1","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_1","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_1","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_1","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-centroidinteranimal","text":"","title":"class CentroidInteranimal"},{"location":"api-docs/features.features/#function-__init___2","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_2","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_2","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_2","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-centroidinteranimalspeed","text":"","title":"class CentroidInteranimalSpeed"},{"location":"api-docs/features.features/#function-__init___3","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_3","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_3","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_3","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-centroidvelocity","text":"","title":"class CentroidVelocity"},{"location":"api-docs/features.features/#function-__init___4","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_4","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_4","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_4","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-distances","text":"","title":"class Distances"},{"location":"api-docs/features.features/#function-__init___5","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_5","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_5","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_5","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-marsfeatures","text":"","title":"class MARSFeatures"},{"location":"api-docs/features.features/#function-__init___6","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_6","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_6","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_6","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-marsreduced","text":"","title":"class MARSReduced"},{"location":"api-docs/features.features/#function-__init___7","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_7","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_7","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_7","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-social","text":"","title":"class Social"},{"location":"api-docs/features.features/#function-__init___8","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_8","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_8","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_8","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-speeds","text":"","title":"class Speeds"},{"location":"api-docs/features.features/#function-__init___9","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs/features.features/#function-fit_9","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs/features.features/#function-fit_transform_9","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs/features.features/#function-transform_9","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs/features.features/#class-features","text":"","title":"class Features"},{"location":"api-docs/features.features/#method-__init__","text":"__init__()","title":"method __init__"},{"location":"api-docs/features.features/#method-transform","text":"transform(df) This file was automatically generated via lazydocs .","title":"method transform"},{"location":"api-docs/features.generic_features/","text":"module features.generic_features Functions to take pose tracks and compute a set of features from them function compute_centerofmass_interanimal_distances compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_centerofmass_interanimal_speed compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass compute_centerofmass( df: DataFrame, raw_col_names: list, bodyparts: list = [], **kwargs ) \u2192 DataFrame function compute_centerofmass_velocity compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, n_shifts=5, bodyparts: list = [], **kwargs ) \u2192 DataFrame function compute_speed_features compute_speed_features( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Features.generic features"},{"location":"api-docs/features.generic_features/#module-featuresgeneric_features","text":"Functions to take pose tracks and compute a set of features from them","title":"module features.generic_features"},{"location":"api-docs/features.generic_features/#function-compute_centerofmass_interanimal_distances","text":"compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_distances"},{"location":"api-docs/features.generic_features/#function-compute_centerofmass_interanimal_speed","text":"compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_speed"},{"location":"api-docs/features.generic_features/#function-compute_centerofmass","text":"compute_centerofmass( df: DataFrame, raw_col_names: list, bodyparts: list = [], **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass"},{"location":"api-docs/features.generic_features/#function-compute_centerofmass_velocity","text":"compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, n_shifts=5, bodyparts: list = [], **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_velocity"},{"location":"api-docs/features.generic_features/#function-compute_speed_features","text":"compute_speed_features( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_speed_features"},{"location":"api-docs/features.generic_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_distance_features"},{"location":"api-docs/features.mars_features/","text":"module features.mars_features Global Variables XY_IDS function augment_features augment_features(window_size=5, n_shifts=3, mode='shift') function boiler_plate boiler_plate(features_df) function make_features_distances make_features_distances(df) function make_features_mars make_features_mars(df, n_shifts=3, mode='shift') function make_features_mars_distr make_features_mars_distr(x, y) function make_features_mars_reduced make_features_mars_reduced(df, n_shifts=2, mode='diff') function make_features_velocities make_features_velocities(df, n_shifts=5) function make_features_social make_features_social(df, n_shifts=3, mode='shift') function compute_mars_features compute_mars_features(df: DataFrame, raw_col_names: list, **kwargs) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_mars_reduced_features compute_mars_reduced_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_social_features compute_social_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_velocity_features compute_velocity_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Features.mars features"},{"location":"api-docs/features.mars_features/#module-featuresmars_features","text":"","title":"module features.mars_features"},{"location":"api-docs/features.mars_features/#global-variables","text":"XY_IDS","title":"Global Variables"},{"location":"api-docs/features.mars_features/#function-augment_features","text":"augment_features(window_size=5, n_shifts=3, mode='shift')","title":"function augment_features"},{"location":"api-docs/features.mars_features/#function-boiler_plate","text":"boiler_plate(features_df)","title":"function boiler_plate"},{"location":"api-docs/features.mars_features/#function-make_features_distances","text":"make_features_distances(df)","title":"function make_features_distances"},{"location":"api-docs/features.mars_features/#function-make_features_mars","text":"make_features_mars(df, n_shifts=3, mode='shift')","title":"function make_features_mars"},{"location":"api-docs/features.mars_features/#function-make_features_mars_distr","text":"make_features_mars_distr(x, y)","title":"function make_features_mars_distr"},{"location":"api-docs/features.mars_features/#function-make_features_mars_reduced","text":"make_features_mars_reduced(df, n_shifts=2, mode='diff')","title":"function make_features_mars_reduced"},{"location":"api-docs/features.mars_features/#function-make_features_velocities","text":"make_features_velocities(df, n_shifts=5)","title":"function make_features_velocities"},{"location":"api-docs/features.mars_features/#function-make_features_social","text":"make_features_social(df, n_shifts=3, mode='shift')","title":"function make_features_social"},{"location":"api-docs/features.mars_features/#function-compute_mars_features","text":"compute_mars_features(df: DataFrame, raw_col_names: list, **kwargs) \u2192 DataFrame","title":"function compute_mars_features"},{"location":"api-docs/features.mars_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_distance_features"},{"location":"api-docs/features.mars_features/#function-compute_mars_reduced_features","text":"compute_mars_reduced_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_mars_reduced_features"},{"location":"api-docs/features.mars_features/#function-compute_social_features","text":"compute_social_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_social_features"},{"location":"api-docs/features.mars_features/#function-compute_velocity_features","text":"compute_velocity_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_velocity_features"},{"location":"api-docs/features/","text":"module features Global Variables FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Features"},{"location":"api-docs/features/#module-features","text":"","title":"module features"},{"location":"api-docs/features/#global-variables","text":"FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs/interpolation/","text":"module interpolation function interpolate_lowconf_points interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"Interpolation"},{"location":"api-docs/interpolation/#module-interpolation","text":"","title":"module interpolation"},{"location":"api-docs/interpolation/#function-interpolate_lowconf_points","text":"interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"function interpolate_lowconf_points"},{"location":"api-docs/io/","text":"module io Loading and saving tracking and behavior annotation files Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS function uniquifier uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order function save_sklearn_model save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to function load_sklearn_model load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model function read_DLC_tracks read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer function save_DLC_tracks_h5 save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe function load_data load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object. function get_sample_data_paths get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files function get_sample_data get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files function read_boris_annotation read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors. function create_behavior_labels create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors. class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"api-docs/io/#module-io","text":"Loading and saving tracking and behavior annotation files","title":"module io"},{"location":"api-docs/io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS","title":"Global Variables"},{"location":"api-docs/io/#function-uniquifier","text":"uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order","title":"function uniquifier"},{"location":"api-docs/io/#function-save_sklearn_model","text":"save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to","title":"function save_sklearn_model"},{"location":"api-docs/io/#function-load_sklearn_model","text":"load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model","title":"function load_sklearn_model"},{"location":"api-docs/io/#function-read_dlc_tracks","text":"read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer","title":"function read_DLC_tracks"},{"location":"api-docs/io/#function-save_dlc_tracks_h5","text":"save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe","title":"function save_DLC_tracks_h5"},{"location":"api-docs/io/#function-load_data","text":"load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object.","title":"function load_data"},{"location":"api-docs/io/#function-get_sample_data_paths","text":"get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files","title":"function get_sample_data_paths"},{"location":"api-docs/io/#function-get_sample_data","text":"get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files","title":"function get_sample_data"},{"location":"api-docs/io/#function-read_boris_annotation","text":"read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function read_boris_annotation"},{"location":"api-docs/io/#function-create_behavior_labels","text":"create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function create_behavior_labels"},{"location":"api-docs/io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"api-docs/io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"api-docs/io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"api-docs/io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"api-docs/io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"api-docs/models/","text":"module models Basic video tracking and behavior class that houses data class HMMSklearn method __init__ __init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize property params method fit fit(X, y) method predict predict(X) class F1Optimizer method __init__ __init__(N=1000, labels=[1]) method fit fit(X, y) method fit_transform fit_transform(X, y=None) method predict predict(X) method predict_proba predict_proba(X) method transform transform(X) class ModelTransformer method __init__ __init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit() method fit fit(X, y) method fit_transform fit_transform(X, y=None) method transform transform(X) This file was automatically generated via lazydocs .","title":"Models"},{"location":"api-docs/models/#module-models","text":"Basic video tracking and behavior class that houses data","title":"module models"},{"location":"api-docs/models/#class-hmmsklearn","text":"","title":"class HMMSklearn"},{"location":"api-docs/models/#method-__init__","text":"__init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize","title":"method __init__"},{"location":"api-docs/models/#property-params","text":"","title":"property params"},{"location":"api-docs/models/#method-fit","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-predict","text":"predict(X)","title":"method predict"},{"location":"api-docs/models/#class-f1optimizer","text":"","title":"class F1Optimizer"},{"location":"api-docs/models/#method-__init___1","text":"__init__(N=1000, labels=[1])","title":"method __init__"},{"location":"api-docs/models/#method-fit_1","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-fit_transform","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs/models/#method-predict_1","text":"predict(X)","title":"method predict"},{"location":"api-docs/models/#method-predict_proba","text":"predict_proba(X)","title":"method predict_proba"},{"location":"api-docs/models/#method-transform","text":"transform(X)","title":"method transform"},{"location":"api-docs/models/#class-modeltransformer","text":"","title":"class ModelTransformer"},{"location":"api-docs/models/#method-__init___2","text":"__init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit()","title":"method __init__"},{"location":"api-docs/models/#method-fit_2","text":"fit(X, y)","title":"method fit"},{"location":"api-docs/models/#method-fit_transform_1","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs/models/#method-transform_1","text":"transform(X) This file was automatically generated via lazydocs .","title":"method transform"},{"location":"api-docs/plot/","text":"module plot Global Variables global_config function plot_embedding plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_unsupervised_results plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_ethogram plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects function create_ethogram_video create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None function create_sample_videos create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None function create_mosaic_video create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None class MplColorHelper method __init__ __init__(cmap_name, start_val, stop_val) method get_rgb get_rgb(val) This file was automatically generated via lazydocs .","title":"Plot"},{"location":"api-docs/plot/#module-plot","text":"","title":"module plot"},{"location":"api-docs/plot/#global-variables","text":"global_config","title":"Global Variables"},{"location":"api-docs/plot/#function-plot_embedding","text":"plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_embedding"},{"location":"api-docs/plot/#function-plot_unsupervised_results","text":"plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_unsupervised_results"},{"location":"api-docs/plot/#function-plot_ethogram","text":"plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects","title":"function plot_ethogram"},{"location":"api-docs/plot/#function-create_ethogram_video","text":"create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None","title":"function create_ethogram_video"},{"location":"api-docs/plot/#function-create_sample_videos","text":"create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None","title":"function create_sample_videos"},{"location":"api-docs/plot/#function-create_mosaic_video","text":"create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None","title":"function create_mosaic_video"},{"location":"api-docs/plot/#class-mplcolorhelper","text":"","title":"class MplColorHelper"},{"location":"api-docs/plot/#method-__init__","text":"__init__(cmap_name, start_val, stop_val)","title":"method __init__"},{"location":"api-docs/plot/#method-get_rgb","text":"get_rgb(val) This file was automatically generated via lazydocs .","title":"method get_rgb"},{"location":"api-docs/unsupervised/","text":"module unsupervised function compute_tsne_embedding compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding function compute_morlet compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time. function compute_density compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts) function compute_watershed compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate. function cluster_behaviors cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"Unsupervised"},{"location":"api-docs/unsupervised/#module-unsupervised","text":"","title":"module unsupervised"},{"location":"api-docs/unsupervised/#function-compute_tsne_embedding","text":"compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding","title":"function compute_tsne_embedding"},{"location":"api-docs/unsupervised/#function-compute_morlet","text":"compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time.","title":"function compute_morlet"},{"location":"api-docs/unsupervised/#function-compute_density","text":"compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts)","title":"function compute_density"},{"location":"api-docs/unsupervised/#function-compute_watershed","text":"compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate.","title":"function compute_watershed"},{"location":"api-docs/unsupervised/#function-cluster_behaviors","text":"cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"function cluster_behaviors"},{"location":"api-docs/utils/","text":"module utils Small helper utilities function checkFFMPEG checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api-docs/utils/#module-utils","text":"Small helper utilities","title":"module utils"},{"location":"api-docs/utils/#function-checkffmpeg","text":"checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"function checkFFMPEG"},{"location":"api-docs/version/","text":"module version This file was automatically generated via lazydocs .","title":"Version"},{"location":"api-docs/version/#module-version","text":"This file was automatically generated via lazydocs .","title":"module version"},{"location":"api-docs/video/","text":"module video Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class. A note on unit conversions For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed. Global Variables global_config FEATURE_MAKERS UNIT_DICT function create_metadata create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided function create_dataset create_dataset( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods. function load_experiment load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file function get_sample_openfield_data get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files class EthologyMetadataAccessor method __init__ __init__(pandas_obj) property n_videos property reverse_label_key property videos class EthologyFeaturesAccessor method __init__ __init__(pandas_obj) method activate activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated). method add add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed method deactivate deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns. method deactivate_cols deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features. method regex regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names class EthologyPoseAccessor method __init__ __init__(pandas_obj) class EthologyMLAccessor method __init__ __init__(pandas_obj) property features property folds property group property labels property splitter class EthologyIOAccessor method __init__ __init__(pandas_obj) method load load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file. method save save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path. method save_movie save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out' method to_dlc_csv to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"Video"},{"location":"api-docs/video/#module-video","text":"Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class.","title":"module video"},{"location":"api-docs/video/#a-note-on-unit-conversions","text":"For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed.","title":"A note on unit conversions"},{"location":"api-docs/video/#global-variables","text":"global_config FEATURE_MAKERS UNIT_DICT","title":"Global Variables"},{"location":"api-docs/video/#function-create_metadata","text":"create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided","title":"function create_metadata"},{"location":"api-docs/video/#function-create_dataset","text":"create_dataset( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods.","title":"function create_dataset"},{"location":"api-docs/video/#function-load_experiment","text":"load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file","title":"function load_experiment"},{"location":"api-docs/video/#function-get_sample_openfield_data","text":"get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files","title":"function get_sample_openfield_data"},{"location":"api-docs/video/#class-ethologymetadataaccessor","text":"","title":"class EthologyMetadataAccessor"},{"location":"api-docs/video/#method-__init__","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-n_videos","text":"","title":"property n_videos"},{"location":"api-docs/video/#property-reverse_label_key","text":"","title":"property reverse_label_key"},{"location":"api-docs/video/#property-videos","text":"","title":"property videos"},{"location":"api-docs/video/#class-ethologyfeaturesaccessor","text":"","title":"class EthologyFeaturesAccessor"},{"location":"api-docs/video/#method-__init___1","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#method-activate","text":"activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated).","title":"method activate"},{"location":"api-docs/video/#method-add","text":"add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed","title":"method add"},{"location":"api-docs/video/#method-deactivate","text":"deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns.","title":"method deactivate"},{"location":"api-docs/video/#method-deactivate_cols","text":"deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features.","title":"method deactivate_cols"},{"location":"api-docs/video/#method-regex","text":"regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names","title":"method regex"},{"location":"api-docs/video/#class-ethologyposeaccessor","text":"","title":"class EthologyPoseAccessor"},{"location":"api-docs/video/#method-__init___2","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#class-ethologymlaccessor","text":"","title":"class EthologyMLAccessor"},{"location":"api-docs/video/#method-__init___3","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#property-features","text":"","title":"property features"},{"location":"api-docs/video/#property-folds","text":"","title":"property folds"},{"location":"api-docs/video/#property-group","text":"","title":"property group"},{"location":"api-docs/video/#property-labels","text":"","title":"property labels"},{"location":"api-docs/video/#property-splitter","text":"","title":"property splitter"},{"location":"api-docs/video/#class-ethologyioaccessor","text":"","title":"class EthologyIOAccessor"},{"location":"api-docs/video/#method-__init___4","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs/video/#method-load","text":"load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file.","title":"method load"},{"location":"api-docs/video/#method-save","text":"save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path.","title":"method save"},{"location":"api-docs/video/#method-save_movie","text":"save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out'","title":"method save_movie"},{"location":"api-docs/video/#method-to_dlc_csv","text":"to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"method to_dlc_csv"},{"location":"api-docs_backup/","text":"API Overview Modules config : Configuration options for ethome functions. features features.cnn1d features.dl_features features.features : Functions to take pose tracks and compute a set of features from them. features.generic_features : Functions to take pose tracks and compute a set of features from them features.mars_features interpolation io : Loading and saving tracking and behavior annotation files models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities version video : Basic video tracking and behavior class that houses data. Classes cnn1d.MABe_Generator dl_features.Trainer features.CNN1DProb features.Centroid features.CentroidInteranimal features.CentroidInteranimalSpeed features.CentroidVelocity features.Distances features.Features features.MARSFeatures features.MARSReduced features.Social features.Speeds io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor Functions cnn1d.build_baseline_model cnn1d.features_distances cnn1d.features_distances_normalized cnn1d.features_identity cnn1d.features_mars cnn1d.features_mars_distr cnn1d.features_via_sklearn cnn1d.make_df dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything features.CNN1DProb.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CNN1DProb.fit features.CNN1DProb.fit_transform features.CNN1DProb.transform : Make the features. This is called internally by the dataset object when running add_features . features.Centroid.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Centroid.fit features.Centroid.fit_transform features.Centroid.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimal.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimal.fit features.CentroidInteranimal.fit_transform features.CentroidInteranimal.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimalSpeed.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimalSpeed.fit features.CentroidInteranimalSpeed.fit_transform features.CentroidInteranimalSpeed.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidVelocity.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidVelocity.fit features.CentroidVelocity.fit_transform features.CentroidVelocity.transform : Make the features. This is called internally by the dataset object when running add_features . features.Distances.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Distances.fit features.Distances.fit_transform features.Distances.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSFeatures.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSFeatures.fit features.MARSFeatures.fit_transform features.MARSFeatures.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSReduced.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSReduced.fit features.MARSReduced.fit_transform features.MARSReduced.transform : Make the features. This is called internally by the dataset object when running add_features . features.Social.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Social.fit features.Social.fit_transform features.Social.transform : Make the features. This is called internally by the dataset object when running add_features . features.Speeds.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Speeds.fit features.Speeds.fit_transform features.Speeds.transform : Make the features. This is called internally by the dataset object when running add_features . features.feature_class_maker generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features mars_features.augment_features mars_features.boiler_plate mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features mars_features.make_features_distances mars_features.make_features_mars mars_features.make_features_mars_distr mars_features.make_features_mars_reduced mars_features.make_features_social mars_features.make_features_velocities interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api-docs_backup/#api-overview","text":"","title":"API Overview"},{"location":"api-docs_backup/#modules","text":"config : Configuration options for ethome functions. features features.cnn1d features.dl_features features.features : Functions to take pose tracks and compute a set of features from them. features.generic_features : Functions to take pose tracks and compute a set of features from them features.mars_features interpolation io : Loading and saving tracking and behavior annotation files models : Basic video tracking and behavior class that houses data plot unsupervised utils : Small helper utilities version video : Basic video tracking and behavior class that houses data.","title":"Modules"},{"location":"api-docs_backup/#classes","text":"cnn1d.MABe_Generator dl_features.Trainer features.CNN1DProb features.Centroid features.CentroidInteranimal features.CentroidInteranimalSpeed features.CentroidVelocity features.Distances features.Features features.MARSFeatures features.MARSReduced features.Social features.Speeds io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation models.F1Optimizer models.HMMSklearn models.ModelTransformer plot.MplColorHelper video.EthologyFeaturesAccessor video.EthologyIOAccessor video.EthologyMLAccessor video.EthologyMetadataAccessor video.EthologyPoseAccessor","title":"Classes"},{"location":"api-docs_backup/#functions","text":"cnn1d.build_baseline_model cnn1d.features_distances cnn1d.features_distances_normalized cnn1d.features_identity cnn1d.features_mars cnn1d.features_mars_distr cnn1d.features_via_sklearn cnn1d.make_df dl_features.compute_dl_probability_features dl_features.convert_to_mars_format dl_features.convert_to_pandas_df dl_features.lrs dl_features.normalize_data dl_features.run_task dl_features.seed_everything features.CNN1DProb.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CNN1DProb.fit features.CNN1DProb.fit_transform features.CNN1DProb.transform : Make the features. This is called internally by the dataset object when running add_features . features.Centroid.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Centroid.fit features.Centroid.fit_transform features.Centroid.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimal.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimal.fit features.CentroidInteranimal.fit_transform features.CentroidInteranimal.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidInteranimalSpeed.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidInteranimalSpeed.fit features.CentroidInteranimalSpeed.fit_transform features.CentroidInteranimalSpeed.transform : Make the features. This is called internally by the dataset object when running add_features . features.CentroidVelocity.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.CentroidVelocity.fit features.CentroidVelocity.fit_transform features.CentroidVelocity.transform : Make the features. This is called internally by the dataset object when running add_features . features.Distances.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Distances.fit features.Distances.fit_transform features.Distances.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSFeatures.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSFeatures.fit features.MARSFeatures.fit_transform features.MARSFeatures.transform : Make the features. This is called internally by the dataset object when running add_features . features.MARSReduced.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.MARSReduced.fit features.MARSReduced.fit_transform features.MARSReduced.transform : Make the features. This is called internally by the dataset object when running add_features . features.Social.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Social.fit features.Social.fit_transform features.Social.transform : Make the features. This is called internally by the dataset object when running add_features . features.Speeds.__init__ : Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. features.Speeds.fit features.Speeds.fit_transform features.Speeds.transform : Make the features. This is called internally by the dataset object when running add_features . features.feature_class_maker generic_features.compute_centerofmass generic_features.compute_centerofmass_interanimal_distances generic_features.compute_centerofmass_interanimal_speed generic_features.compute_centerofmass_velocity generic_features.compute_distance_features generic_features.compute_speed_features mars_features.augment_features mars_features.boiler_plate mars_features.compute_distance_features mars_features.compute_mars_features mars_features.compute_mars_reduced_features mars_features.compute_social_features mars_features.compute_velocity_features mars_features.make_features_distances mars_features.make_features_mars mars_features.make_features_mars_distr mars_features.make_features_mars_reduced mars_features.make_features_social mars_features.make_features_velocities interpolation.interpolate_lowconf_points : Interpolate raw tracking points if their probabilities are available. io.create_behavior_labels : Create behavior labels from BORIS exported csv files. io.get_sample_data : Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration io.get_sample_data_paths : Get path to sample data files provided with package. io.load_data : Load an object from a pickle file io.load_sklearn_model : Load sklearn model from file io.read_DLC_tracks : Read in tracks from DLC. io.read_boris_annotation : Read behavior annotation from BORIS exported csv file. io.save_DLC_tracks_h5 : Save DLC tracks in h5 format. io.save_sklearn_model : Save sklearn model to file io.uniquifier : Return a sequence (e.g. list) with unique elements only, but maintaining original list order plot.create_ethogram_video : Overlay ethogram on top of source video with ffmpeg plot.create_mosaic_video : Take a set of video clips and turn them into a mosaic using ffmpeg plot.create_sample_videos : Create a sample of videos displaying the labeled behaviors using ffmpeg. plot.plot_embedding : Scatterplot of a 2D TSNE or UMAP embedding from the dataset. plot.plot_ethogram : Simple ethogram of one video, up to a certain frame number. plot.plot_unsupervised_results : Set of plots for unsupervised behavior clustering results unsupervised.cluster_behaviors : Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. unsupervised.compute_density : Compute kernel density estimate of embedding. unsupervised.compute_morlet : Compute morlet wavelet transform of a time series. unsupervised.compute_tsne_embedding : Compute TSNE embedding. Only for a random subset of rows. unsupervised.compute_watershed : Compute watershed clustering of a density matrix. utils.checkFFMPEG : Check for ffmpeg dependencies video.create_metadata : Prepare a metadata dictionary for defining a ExperimentDataFrame. video.create_dataset : Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. video.get_sample_openfield_data : Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. video.load_experiment : Load DataFrame from file. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api-docs_backup/config/","text":"module config Configuration options for ethome functions. Global Variables global_config This file was automatically generated via lazydocs .","title":"Config"},{"location":"api-docs_backup/config/#module-config","text":"Configuration options for ethome functions.","title":"module config"},{"location":"api-docs_backup/config/#global-variables","text":"global_config This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs_backup/dl.dl_features/","text":"module dl.dl_features Global Variables sweeps_baseline feature_spaces has_keras THIS_FILE_DIR function seed_everything seed_everything(seed=2012) function normalize_data normalize_data(orig_pose_dictionary) function run_task run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True ) function lrs lrs(epoch, lr, freq=10) function convert_to_mars_format convert_to_mars_format(df, colnames, animal_setup) function convert_to_pandas_df convert_to_pandas_df(data, colnames=None) function compute_dl_probability_features compute_dl_probability_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) class Trainer method __init__ __init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fc19c9c5830>, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fc2d16bba70> ) method delete_model delete_model() method get_test_prediction_probabilities get_test_prediction_probabilities() method initialize_model initialize_model(**kwargs) method train train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"Dl.dl features"},{"location":"api-docs_backup/dl.dl_features/#module-dldl_features","text":"","title":"module dl.dl_features"},{"location":"api-docs_backup/dl.dl_features/#global-variables","text":"sweeps_baseline feature_spaces has_keras THIS_FILE_DIR","title":"Global Variables"},{"location":"api-docs_backup/dl.dl_features/#function-seed_everything","text":"seed_everything(seed=2012)","title":"function seed_everything"},{"location":"api-docs_backup/dl.dl_features/#function-normalize_data","text":"normalize_data(orig_pose_dictionary)","title":"function normalize_data"},{"location":"api-docs_backup/dl.dl_features/#function-run_task","text":"run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True )","title":"function run_task"},{"location":"api-docs_backup/dl.dl_features/#function-lrs","text":"lrs(epoch, lr, freq=10)","title":"function lrs"},{"location":"api-docs_backup/dl.dl_features/#function-convert_to_mars_format","text":"convert_to_mars_format(df, colnames, animal_setup)","title":"function convert_to_mars_format"},{"location":"api-docs_backup/dl.dl_features/#function-convert_to_pandas_df","text":"convert_to_pandas_df(data, colnames=None)","title":"function convert_to_pandas_df"},{"location":"api-docs_backup/dl.dl_features/#function-compute_dl_probability_features","text":"compute_dl_probability_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs )","title":"function compute_dl_probability_features"},{"location":"api-docs_backup/dl.dl_features/#class-trainer","text":"","title":"class Trainer"},{"location":"api-docs_backup/dl.dl_features/#method-__init__","text":"__init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7fc19c9c5830>, Generator=<class 'ethome.features.dl_generators.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7fc2d16bba70> )","title":"method __init__"},{"location":"api-docs_backup/dl.dl_features/#method-delete_model","text":"delete_model()","title":"method delete_model"},{"location":"api-docs_backup/dl.dl_features/#method-get_test_prediction_probabilities","text":"get_test_prediction_probabilities()","title":"method get_test_prediction_probabilities"},{"location":"api-docs_backup/dl.dl_features/#method-initialize_model","text":"initialize_model(**kwargs)","title":"method initialize_model"},{"location":"api-docs_backup/dl.dl_features/#method-train","text":"train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"method train"},{"location":"api-docs_backup/dl.dl_generators/","text":"module dl.dl_generators Global Variables has_keras function make_df make_df(pts, colnames=None) function features_identity features_identity(inputs) function features_via_sklearn features_via_sklearn(inputs, featurizer) function features_mars features_mars(x) function features_mars_distr features_mars_distr(x) function features_distances features_distances(inputs) function features_distances_normalized features_distances_normalized(inputs) class MABe_Generator method __init__ __init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fc2ac27c290> ) method augment_fn augment_fn(x) method on_epoch_end on_epoch_end() This file was automatically generated via lazydocs .","title":"Dl.dl generators"},{"location":"api-docs_backup/dl.dl_generators/#module-dldl_generators","text":"","title":"module dl.dl_generators"},{"location":"api-docs_backup/dl.dl_generators/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs_backup/dl.dl_generators/#function-make_df","text":"make_df(pts, colnames=None)","title":"function make_df"},{"location":"api-docs_backup/dl.dl_generators/#function-features_identity","text":"features_identity(inputs)","title":"function features_identity"},{"location":"api-docs_backup/dl.dl_generators/#function-features_via_sklearn","text":"features_via_sklearn(inputs, featurizer)","title":"function features_via_sklearn"},{"location":"api-docs_backup/dl.dl_generators/#function-features_mars","text":"features_mars(x)","title":"function features_mars"},{"location":"api-docs_backup/dl.dl_generators/#function-features_mars_distr","text":"features_mars_distr(x)","title":"function features_mars_distr"},{"location":"api-docs_backup/dl.dl_generators/#function-features_distances","text":"features_distances(inputs)","title":"function features_distances"},{"location":"api-docs_backup/dl.dl_generators/#function-features_distances_normalized","text":"features_distances_normalized(inputs)","title":"function features_distances_normalized"},{"location":"api-docs_backup/dl.dl_generators/#class-mabe_generator","text":"","title":"class MABe_Generator"},{"location":"api-docs_backup/dl.dl_generators/#method-__init__","text":"__init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7fc2ac27c290> )","title":"method __init__"},{"location":"api-docs_backup/dl.dl_generators/#method-augment_fn","text":"augment_fn(x)","title":"method augment_fn"},{"location":"api-docs_backup/dl.dl_generators/#method-on_epoch_end","text":"on_epoch_end() This file was automatically generated via lazydocs .","title":"method on_epoch_end"},{"location":"api-docs_backup/dl.dl_models/","text":"module dl.dl_models Global Variables has_keras function build_baseline_model build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) This file was automatically generated via lazydocs .","title":"Dl.dl models"},{"location":"api-docs_backup/dl.dl_models/#module-dldl_models","text":"","title":"module dl.dl_models"},{"location":"api-docs_backup/dl.dl_models/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs_backup/dl.dl_models/#function-build_baseline_model","text":"build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) This file was automatically generated via lazydocs .","title":"function build_baseline_model"},{"location":"api-docs_backup/dl.feature_engineering/","text":"module dl.feature_engineering Global Variables XY_IDS function augment_features augment_features(window_size=5, n_shifts=3, mode='shift') function boiler_plate boiler_plate(features_df) function make_features_distances make_features_distances(df, animal_setup) function make_features_mars make_features_mars(df, animal_setup, n_shifts=3, mode='shift') function make_features_mars_distr make_features_mars_distr(x, y) function make_features_mars_reduced make_features_mars_reduced(df, animal_setup, n_shifts=2, mode='diff') function make_features_velocities make_features_velocities(df, animal_setup, n_shifts=5) function make_features_social make_features_social(df, animal_setup, n_shifts=3, mode='shift') This file was automatically generated via lazydocs .","title":"Dl.feature engineering"},{"location":"api-docs_backup/dl.feature_engineering/#module-dlfeature_engineering","text":"","title":"module dl.feature_engineering"},{"location":"api-docs_backup/dl.feature_engineering/#global-variables","text":"XY_IDS","title":"Global Variables"},{"location":"api-docs_backup/dl.feature_engineering/#function-augment_features","text":"augment_features(window_size=5, n_shifts=3, mode='shift')","title":"function augment_features"},{"location":"api-docs_backup/dl.feature_engineering/#function-boiler_plate","text":"boiler_plate(features_df)","title":"function boiler_plate"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_distances","text":"make_features_distances(df, animal_setup)","title":"function make_features_distances"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_mars","text":"make_features_mars(df, animal_setup, n_shifts=3, mode='shift')","title":"function make_features_mars"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_mars_distr","text":"make_features_mars_distr(x, y)","title":"function make_features_mars_distr"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_mars_reduced","text":"make_features_mars_reduced(df, animal_setup, n_shifts=2, mode='diff')","title":"function make_features_mars_reduced"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_velocities","text":"make_features_velocities(df, animal_setup, n_shifts=5)","title":"function make_features_velocities"},{"location":"api-docs_backup/dl.feature_engineering/#function-make_features_social","text":"make_features_social(df, animal_setup, n_shifts=3, mode='shift') This file was automatically generated via lazydocs .","title":"function make_features_social"},{"location":"api-docs_backup/dl.grid_searches/","text":"module dl.grid_searches Global Variables has_keras feature_spaces sweeps_baseline This file was automatically generated via lazydocs .","title":"Dl.grid searches"},{"location":"api-docs_backup/dl.grid_searches/#module-dlgrid_searches","text":"","title":"module dl.grid_searches"},{"location":"api-docs_backup/dl.grid_searches/#global-variables","text":"has_keras feature_spaces sweeps_baseline This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs_backup/dl/","text":"module dl This file was automatically generated via lazydocs .","title":"Dl"},{"location":"api-docs_backup/dl/#module-dl","text":"This file was automatically generated via lazydocs .","title":"module dl"},{"location":"api-docs_backup/features.cnn1d/","text":"module features.cnn1d Global Variables has_keras function build_baseline_model build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None ) function make_df make_df(pts, colnames=None) function features_identity features_identity(inputs) function features_via_sklearn features_via_sklearn(inputs, featurizer) function features_mars features_mars(x) function features_mars_distr features_mars_distr(x) function features_distances features_distances(inputs) function features_distances_normalized features_distances_normalized(inputs) class MABe_Generator method __init__ __init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7f5077dcd320> ) method augment_fn augment_fn(x) method on_epoch_end on_epoch_end() This file was automatically generated via lazydocs .","title":"Features.cnn1d"},{"location":"api-docs_backup/features.cnn1d/#module-featurescnn1d","text":"","title":"module features.cnn1d"},{"location":"api-docs_backup/features.cnn1d/#global-variables","text":"has_keras","title":"Global Variables"},{"location":"api-docs_backup/features.cnn1d/#function-build_baseline_model","text":"build_baseline_model( input_dim, layer_channels=(512, 256), dropout_rate=0.0, learning_rate=0.001, conv_size=5, num_classes=4, class_weight=None )","title":"function build_baseline_model"},{"location":"api-docs_backup/features.cnn1d/#function-make_df","text":"make_df(pts, colnames=None)","title":"function make_df"},{"location":"api-docs_backup/features.cnn1d/#function-features_identity","text":"features_identity(inputs)","title":"function features_identity"},{"location":"api-docs_backup/features.cnn1d/#function-features_via_sklearn","text":"features_via_sklearn(inputs, featurizer)","title":"function features_via_sklearn"},{"location":"api-docs_backup/features.cnn1d/#function-features_mars","text":"features_mars(x)","title":"function features_mars"},{"location":"api-docs_backup/features.cnn1d/#function-features_mars_distr","text":"features_mars_distr(x)","title":"function features_mars_distr"},{"location":"api-docs_backup/features.cnn1d/#function-features_distances","text":"features_distances(inputs)","title":"function features_distances"},{"location":"api-docs_backup/features.cnn1d/#function-features_distances_normalized","text":"features_distances_normalized(inputs)","title":"function features_distances_normalized"},{"location":"api-docs_backup/features.cnn1d/#class-mabe_generator","text":"","title":"class MABe_Generator"},{"location":"api-docs_backup/features.cnn1d/#method-__init__","text":"__init__( pose_dict, batch_size, dim, use_conv, num_classes, augment=False, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, shuffle=False, mode='fit', featurize=<function features_identity at 0x7f5077dcd320> )","title":"method __init__"},{"location":"api-docs_backup/features.cnn1d/#method-augment_fn","text":"augment_fn(x)","title":"method augment_fn"},{"location":"api-docs_backup/features.cnn1d/#method-on_epoch_end","text":"on_epoch_end() This file was automatically generated via lazydocs .","title":"method on_epoch_end"},{"location":"api-docs_backup/features.dl_features/","text":"module features.dl_features Global Variables has_keras THIS_FILE_DIR default_config feature_spaces sweeps_baseline function seed_everything seed_everything(seed=2012) function normalize_data normalize_data(orig_pose_dictionary) function run_task run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True ) function lrs lrs(epoch, lr, freq=10) function convert_to_mars_format convert_to_mars_format(df, colnames, animal_setup) function convert_to_pandas_df convert_to_pandas_df(data, colnames=None) function compute_dl_probability_features compute_dl_probability_features(df: DataFrame, raw_col_names: list, **kwargs) class Trainer method __init__ __init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7f50bc37b050>, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7f5077dcd320> ) method delete_model delete_model() method get_test_prediction_probabilities get_test_prediction_probabilities() method initialize_model initialize_model(**kwargs) method train train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"Features.dl features"},{"location":"api-docs_backup/features.dl_features/#module-featuresdl_features","text":"","title":"module features.dl_features"},{"location":"api-docs_backup/features.dl_features/#global-variables","text":"has_keras THIS_FILE_DIR default_config feature_spaces sweeps_baseline","title":"Global Variables"},{"location":"api-docs_backup/features.dl_features/#function-seed_everything","text":"seed_everything(seed=2012)","title":"function seed_everything"},{"location":"api-docs_backup/features.dl_features/#function-normalize_data","text":"normalize_data(orig_pose_dictionary)","title":"function normalize_data"},{"location":"api-docs_backup/features.dl_features/#function-run_task","text":"run_task( vocabulary, test_data, config_name, build_model, skip_test_prediction=False, seed=2021, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, params=None, use_conv=True )","title":"function run_task"},{"location":"api-docs_backup/features.dl_features/#function-lrs","text":"lrs(epoch, lr, freq=10)","title":"function lrs"},{"location":"api-docs_backup/features.dl_features/#function-convert_to_mars_format","text":"convert_to_mars_format(df, colnames, animal_setup)","title":"function convert_to_mars_format"},{"location":"api-docs_backup/features.dl_features/#function-convert_to_pandas_df","text":"convert_to_pandas_df(data, colnames=None)","title":"function convert_to_pandas_df"},{"location":"api-docs_backup/features.dl_features/#function-compute_dl_probability_features","text":"compute_dl_probability_features(df: DataFrame, raw_col_names: list, **kwargs)","title":"function compute_dl_probability_features"},{"location":"api-docs_backup/features.dl_features/#class-trainer","text":"","title":"class Trainer"},{"location":"api-docs_backup/features.dl_features/#method-__init__","text":"__init__( feature_dim, num_classes, test_data=None, class_to_number=None, past_frames=0, future_frames=0, frame_gap=1, use_conv=False, build_model=<function build_baseline_model at 0x7f50bc37b050>, Generator=<class 'features.cnn1d.MABe_Generator'>, use_callbacks=False, learning_decay_freq=10, featurizer=<function features_identity at 0x7f5077dcd320> )","title":"method __init__"},{"location":"api-docs_backup/features.dl_features/#method-delete_model","text":"delete_model()","title":"method delete_model"},{"location":"api-docs_backup/features.dl_features/#method-get_test_prediction_probabilities","text":"get_test_prediction_probabilities()","title":"method get_test_prediction_probabilities"},{"location":"api-docs_backup/features.dl_features/#method-initialize_model","text":"initialize_model(**kwargs)","title":"method initialize_model"},{"location":"api-docs_backup/features.dl_features/#method-train","text":"train(model_params, class_weight=None, n_folds=5) This file was automatically generated via lazydocs .","title":"method train"},{"location":"api-docs_backup/features.features/","text":"module features.features Functions to take pose tracks and compute a set of features from them. Global Variables default_tracking_columns FEATURE_MAKERS function feature_class_maker feature_class_maker(name, compute_function, required_columns=[]) class CNN1DProb function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Centroid function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidInteranimal function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidInteranimalSpeed function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class CentroidVelocity function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Distances function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class MARSFeatures function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class MARSReduced function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Social function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Speeds function __init__ __init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features. function fit fit(edf, **kwargs) function fit_transform fit_transform(edf, **kwargs) function transform transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function. class Features method __init__ __init__() method transform transform(df) This file was automatically generated via lazydocs .","title":"Features.features"},{"location":"api-docs_backup/features.features/#module-featuresfeatures","text":"Functions to take pose tracks and compute a set of features from them.","title":"module features.features"},{"location":"api-docs_backup/features.features/#global-variables","text":"default_tracking_columns FEATURE_MAKERS","title":"Global Variables"},{"location":"api-docs_backup/features.features/#function-feature_class_maker","text":"feature_class_maker(name, compute_function, required_columns=[])","title":"function feature_class_maker"},{"location":"api-docs_backup/features.features/#class-cnn1dprob","text":"","title":"class CNN1DProb"},{"location":"api-docs_backup/features.features/#function-__init__","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-centroid","text":"","title":"class Centroid"},{"location":"api-docs_backup/features.features/#function-__init___1","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_1","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_1","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_1","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-centroidinteranimal","text":"","title":"class CentroidInteranimal"},{"location":"api-docs_backup/features.features/#function-__init___2","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_2","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_2","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_2","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-centroidinteranimalspeed","text":"","title":"class CentroidInteranimalSpeed"},{"location":"api-docs_backup/features.features/#function-__init___3","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_3","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_3","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_3","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-centroidvelocity","text":"","title":"class CentroidVelocity"},{"location":"api-docs_backup/features.features/#function-__init___4","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_4","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_4","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_4","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-distances","text":"","title":"class Distances"},{"location":"api-docs_backup/features.features/#function-__init___5","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_5","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_5","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_5","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-marsfeatures","text":"","title":"class MARSFeatures"},{"location":"api-docs_backup/features.features/#function-__init___6","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_6","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_6","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_6","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-marsreduced","text":"","title":"class MARSReduced"},{"location":"api-docs_backup/features.features/#function-__init___7","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_7","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_7","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_7","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-social","text":"","title":"class Social"},{"location":"api-docs_backup/features.features/#function-__init___8","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_8","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_8","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_8","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-speeds","text":"","title":"class Speeds"},{"location":"api-docs_backup/features.features/#function-__init___9","text":"__init__(required_columns=None, **kwargs) Feature creation object. This houses the feature creation function and the columns that are required to compute the features. Performs some checks on data to make sure has these columns. See docstring for the features model for more information. Args: required_columns : The columns that are required to compute the features.","title":"function __init__"},{"location":"api-docs_backup/features.features/#function-fit_9","text":"fit(edf, **kwargs)","title":"function fit"},{"location":"api-docs_backup/features.features/#function-fit_transform_9","text":"fit_transform(edf, **kwargs)","title":"function fit_transform"},{"location":"api-docs_backup/features.features/#function-transform_9","text":"transform(edf, **kwargs) Make the features. This is called internally by the dataset object when running add_features . Args: edf : The ExperimentDataFrame to compute the features on. **kwargs : Extra arguments passed onto the feature creation function.","title":"function transform"},{"location":"api-docs_backup/features.features/#class-features","text":"","title":"class Features"},{"location":"api-docs_backup/features.features/#method-__init__","text":"__init__()","title":"method __init__"},{"location":"api-docs_backup/features.features/#method-transform","text":"transform(df) This file was automatically generated via lazydocs .","title":"method transform"},{"location":"api-docs_backup/features.generic_features/","text":"module features.generic_features Functions to take pose tracks and compute a set of features from them function compute_centerofmass_interanimal_distances compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_centerofmass_interanimal_speed compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass compute_centerofmass( df: DataFrame, raw_col_names: list, bodyparts: list = [], **kwargs ) \u2192 DataFrame function compute_centerofmass_velocity compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, n_shifts=5, bodyparts: list = [], **kwargs ) \u2192 DataFrame function compute_speed_features compute_speed_features( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Features.generic features"},{"location":"api-docs_backup/features.generic_features/#module-featuresgeneric_features","text":"Functions to take pose tracks and compute a set of features from them","title":"module features.generic_features"},{"location":"api-docs_backup/features.generic_features/#function-compute_centerofmass_interanimal_distances","text":"compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_distances"},{"location":"api-docs_backup/features.generic_features/#function-compute_centerofmass_interanimal_speed","text":"compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_speed"},{"location":"api-docs_backup/features.generic_features/#function-compute_centerofmass","text":"compute_centerofmass( df: DataFrame, raw_col_names: list, bodyparts: list = [], **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass"},{"location":"api-docs_backup/features.generic_features/#function-compute_centerofmass_velocity","text":"compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, n_shifts=5, bodyparts: list = [], **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_velocity"},{"location":"api-docs_backup/features.generic_features/#function-compute_speed_features","text":"compute_speed_features( df: DataFrame, raw_col_names: list, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_speed_features"},{"location":"api-docs_backup/features.generic_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_distance_features"},{"location":"api-docs_backup/features.mars_features/","text":"module features.mars_features Global Variables XY_IDS function augment_features augment_features(window_size=5, n_shifts=3, mode='shift') function boiler_plate boiler_plate(features_df) function make_features_distances make_features_distances(df) function make_features_mars make_features_mars(df, n_shifts=3, mode='shift') function make_features_mars_distr make_features_mars_distr(x, y) function make_features_mars_reduced make_features_mars_reduced(df, n_shifts=2, mode='diff') function make_features_velocities make_features_velocities(df, n_shifts=5) function make_features_social make_features_social(df, n_shifts=3, mode='shift') function compute_mars_features compute_mars_features(df: DataFrame, raw_col_names: list, **kwargs) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_mars_reduced_features compute_mars_reduced_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_social_features compute_social_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame function compute_velocity_features compute_velocity_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Features.mars features"},{"location":"api-docs_backup/features.mars_features/#module-featuresmars_features","text":"","title":"module features.mars_features"},{"location":"api-docs_backup/features.mars_features/#global-variables","text":"XY_IDS","title":"Global Variables"},{"location":"api-docs_backup/features.mars_features/#function-augment_features","text":"augment_features(window_size=5, n_shifts=3, mode='shift')","title":"function augment_features"},{"location":"api-docs_backup/features.mars_features/#function-boiler_plate","text":"boiler_plate(features_df)","title":"function boiler_plate"},{"location":"api-docs_backup/features.mars_features/#function-make_features_distances","text":"make_features_distances(df)","title":"function make_features_distances"},{"location":"api-docs_backup/features.mars_features/#function-make_features_mars","text":"make_features_mars(df, n_shifts=3, mode='shift')","title":"function make_features_mars"},{"location":"api-docs_backup/features.mars_features/#function-make_features_mars_distr","text":"make_features_mars_distr(x, y)","title":"function make_features_mars_distr"},{"location":"api-docs_backup/features.mars_features/#function-make_features_mars_reduced","text":"make_features_mars_reduced(df, n_shifts=2, mode='diff')","title":"function make_features_mars_reduced"},{"location":"api-docs_backup/features.mars_features/#function-make_features_velocities","text":"make_features_velocities(df, n_shifts=5)","title":"function make_features_velocities"},{"location":"api-docs_backup/features.mars_features/#function-make_features_social","text":"make_features_social(df, n_shifts=3, mode='shift')","title":"function make_features_social"},{"location":"api-docs_backup/features.mars_features/#function-compute_mars_features","text":"compute_mars_features(df: DataFrame, raw_col_names: list, **kwargs) \u2192 DataFrame","title":"function compute_mars_features"},{"location":"api-docs_backup/features.mars_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_distance_features"},{"location":"api-docs_backup/features.mars_features/#function-compute_mars_reduced_features","text":"compute_mars_reduced_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_mars_reduced_features"},{"location":"api-docs_backup/features.mars_features/#function-compute_social_features","text":"compute_social_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame","title":"function compute_social_features"},{"location":"api-docs_backup/features.mars_features/#function-compute_velocity_features","text":"compute_velocity_features( df: DataFrame, raw_col_names: list, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_velocity_features"},{"location":"api-docs_backup/features/","text":"module features Global Variables FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Features"},{"location":"api-docs_backup/features/#module-features","text":"","title":"module features"},{"location":"api-docs_backup/features/#global-variables","text":"FEATURE_MAKERS This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api-docs_backup/generic_features/","text":"module generic_features Functions to take pose tracks and compute a set of features from them function compute_centerofmass_interanimal_distances compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_centerofmass_interanimal_speed compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass_velocity compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_centerofmass compute_centerofmass( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_speed_features compute_speed_features( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Generic features"},{"location":"api-docs_backup/generic_features/#module-generic_features","text":"Functions to take pose tracks and compute a set of features from them","title":"module generic_features"},{"location":"api-docs_backup/generic_features/#function-compute_centerofmass_interanimal_distances","text":"compute_centerofmass_interanimal_distances( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_distances"},{"location":"api-docs_backup/generic_features/#function-compute_centerofmass_interanimal_speed","text":"compute_centerofmass_interanimal_speed( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_interanimal_speed"},{"location":"api-docs_backup/generic_features/#function-compute_centerofmass_velocity","text":"compute_centerofmass_velocity( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass_velocity"},{"location":"api-docs_backup/generic_features/#function-compute_centerofmass","text":"compute_centerofmass( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_centerofmass"},{"location":"api-docs_backup/generic_features/#function-compute_speed_features","text":"compute_speed_features( df: DataFrame, raw_col_names: list, animal_setup: dict, n_shifts=5, **kwargs ) \u2192 DataFrame","title":"function compute_speed_features"},{"location":"api-docs_backup/generic_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_distance_features"},{"location":"api-docs_backup/interpolation/","text":"module interpolation function interpolate_lowconf_points interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"Interpolation"},{"location":"api-docs_backup/interpolation/#module-interpolation","text":"","title":"module interpolation"},{"location":"api-docs_backup/interpolation/#function-interpolate_lowconf_points","text":"interpolate_lowconf_points( edf: DataFrame, conf_threshold: float = 0.9, in_place: bool = True, rolling_window: bool = True, window_size: int = 3 ) \u2192 DataFrame Interpolate raw tracking points if their probabilities are available. Args: edf : pandas DataFrame containing the tracks to interpolate conf_threshold : default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead in_place : default True. Whether to replace data in place rolling_window : default True. Whether to use a rolling window to interpolate window_size : default 3. The size of the rolling window to use Returns: Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification This file was automatically generated via lazydocs .","title":"function interpolate_lowconf_points"},{"location":"api-docs_backup/io/","text":"module io Loading and saving tracking and behavior annotation files Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS function uniquifier uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order function save_sklearn_model save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to function load_sklearn_model load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model function read_DLC_tracks read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer function save_DLC_tracks_h5 save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe function load_data load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object. function get_sample_data_paths get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files function get_sample_data get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files function read_boris_annotation read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors. function create_behavior_labels create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors. class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"api-docs_backup/io/#module-io","text":"Loading and saving tracking and behavior annotation files","title":"module io"},{"location":"api-docs_backup/io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END XY_IDS XYLIKELIHOOD_IDS","title":"Global Variables"},{"location":"api-docs_backup/io/#function-uniquifier","text":"uniquifier(seq) Return a sequence (e.g. list) with unique elements only, but maintaining original list order","title":"function uniquifier"},{"location":"api-docs_backup/io/#function-save_sklearn_model","text":"save_sklearn_model(model, fn_out) Save sklearn model to file Args: model : sklearn model to save fn_out : filename to save to","title":"function save_sklearn_model"},{"location":"api-docs_backup/io/#function-load_sklearn_model","text":"load_sklearn_model(fn_in) Load sklearn model from file Args: fn_in : filename to load from Returns: the loaded sklearn model","title":"function load_sklearn_model"},{"location":"api-docs_backup/io/#function-read_dlc_tracks","text":"read_DLC_tracks( fn_in: str, part_renamer: dict = None, animal_renamer: dict = None, read_likelihoods: bool = True ) \u2192 tuple Read in tracks from DLC. Args: fn_in : csv file that has DLC tracks part_renamer : dictionary to rename body parts, if needed animal_renamer : dictionary to rename animals, if needed read_likelihoods : default True. Whether to attach DLC likelihoods to table Returns: Pandas DataFrame with (n_animals 2 n_body_parts) columns plus with filename and frame, List of body parts, List of animals, Columns names for DLC tracks (excluding likelihoods, if read in), Scorer","title":"function read_DLC_tracks"},{"location":"api-docs_backup/io/#function-save_dlc_tracks_h5","text":"save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None Save DLC tracks in h5 format. Args: df : Pandas dataframe to save fn_out : Where to save the dataframe","title":"function save_DLC_tracks_h5"},{"location":"api-docs_backup/io/#function-load_data","text":"load_data(fn: str) Load an object from a pickle file Args: fn : The filename Returns: The pickled object.","title":"function load_data"},{"location":"api-docs_backup/io/#function-get_sample_data_paths","text":"get_sample_data_paths() Get path to sample data files provided with package. Returns: (tuple) list of DLC tracking file, list of boris annotation files","title":"function get_sample_data_paths"},{"location":"api-docs_backup/io/#function-get_sample_data","text":"get_sample_data() Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration Returns: (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files","title":"function get_sample_data"},{"location":"api-docs_backup/io/#function-read_boris_annotation","text":"read_boris_annotation( fn_in: str, fps: int, duration: float, behav_labels: dict = None ) \u2192 tuple Read behavior annotation from BORIS exported csv file. This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. Args: fn_in : The filename with BORIS behavior annotations to load fps : The frames per second of the video duration : The duration of the video in seconds behav_labels : If provided, only import behaviors with these names. Default = None = import everything. Returns: A numpy array which indicates, for all frames, which behavior is occuring. 0 = no behavior, 1 and above are the labels of the behaviors. A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function read_boris_annotation"},{"location":"api-docs_backup/io/#function-create_behavior_labels","text":"create_behavior_labels(boris_files) Create behavior labels from BORIS exported csv files. Args: boris_files : List of BORIS exported csv files Returns: A dictionary with keys the numerical labels and values the names of the behaviors.","title":"function create_behavior_labels"},{"location":"api-docs_backup/io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"api-docs_backup/io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"api-docs_backup/io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"api-docs_backup/io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"api-docs_backup/io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"api-docs_backup/mars_features/","text":"module mars_features function compute_mars_features compute_mars_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_distance_features compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_mars_reduced_features compute_mars_reduced_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_social_features compute_social_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame function compute_velocity_features compute_velocity_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"Mars features"},{"location":"api-docs_backup/mars_features/#module-mars_features","text":"","title":"module mars_features"},{"location":"api-docs_backup/mars_features/#function-compute_mars_features","text":"compute_mars_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_mars_features"},{"location":"api-docs_backup/mars_features/#function-compute_distance_features","text":"compute_distance_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_distance_features"},{"location":"api-docs_backup/mars_features/#function-compute_mars_reduced_features","text":"compute_mars_reduced_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_mars_reduced_features"},{"location":"api-docs_backup/mars_features/#function-compute_social_features","text":"compute_social_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame","title":"function compute_social_features"},{"location":"api-docs_backup/mars_features/#function-compute_velocity_features","text":"compute_velocity_features( df: DataFrame, raw_col_names: list, animal_setup: dict, **kwargs ) \u2192 DataFrame This file was automatically generated via lazydocs .","title":"function compute_velocity_features"},{"location":"api-docs_backup/ml/","text":"module ml Machine learning functions This file was automatically generated via lazydocs .","title":"Ml"},{"location":"api-docs_backup/ml/#module-ml","text":"Machine learning functions This file was automatically generated via lazydocs .","title":"module ml"},{"location":"api-docs_backup/models/","text":"module models Basic video tracking and behavior class that houses data class HMMSklearn method __init__ __init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize property params method fit fit(X, y) method predict predict(X) class F1Optimizer method __init__ __init__(N=1000, labels=[1]) method fit fit(X, y) method fit_transform fit_transform(X, y=None) method predict predict(X) method predict_proba predict_proba(X) method transform transform(X) class ModelTransformer method __init__ __init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit() method fit fit(X, y) method fit_transform fit_transform(X, y=None) method transform transform(X) This file was automatically generated via lazydocs .","title":"Models"},{"location":"api-docs_backup/models/#module-models","text":"Basic video tracking and behavior class that houses data","title":"module models"},{"location":"api-docs_backup/models/#class-hmmsklearn","text":"","title":"class HMMSklearn"},{"location":"api-docs_backup/models/#method-__init__","text":"__init__(D, C=11) HMM model from Linderman state-space model package ssm, tweaked slightly to fit with sklearn syntax Args: D : number of behavioral categories C : number of bins to discretize","title":"method __init__"},{"location":"api-docs_backup/models/#property-params","text":"","title":"property params"},{"location":"api-docs_backup/models/#method-fit","text":"fit(X, y)","title":"method fit"},{"location":"api-docs_backup/models/#method-predict","text":"predict(X)","title":"method predict"},{"location":"api-docs_backup/models/#class-f1optimizer","text":"","title":"class F1Optimizer"},{"location":"api-docs_backup/models/#method-__init___1","text":"__init__(N=1000, labels=[1])","title":"method __init__"},{"location":"api-docs_backup/models/#method-fit_1","text":"fit(X, y)","title":"method fit"},{"location":"api-docs_backup/models/#method-fit_transform","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs_backup/models/#method-predict_1","text":"predict(X)","title":"method predict"},{"location":"api-docs_backup/models/#method-predict_proba","text":"predict_proba(X)","title":"method predict_proba"},{"location":"api-docs_backup/models/#method-transform","text":"transform(X)","title":"method transform"},{"location":"api-docs_backup/models/#class-modeltransformer","text":"","title":"class ModelTransformer"},{"location":"api-docs_backup/models/#method-__init___2","text":"__init__(Model, *args, **kwargs) Turns an sklearn model into a model that can be used in a pipeline. Useful for stacking models. Basically, implements transform and fit_transform as model.predict_prob, without or with fit Args: Model : sklearn model to be used for prediction args : args to be passed to Model.fit() kwargs : kwargs to be passed to Model.fit()","title":"method __init__"},{"location":"api-docs_backup/models/#method-fit_2","text":"fit(X, y)","title":"method fit"},{"location":"api-docs_backup/models/#method-fit_transform_1","text":"fit_transform(X, y=None)","title":"method fit_transform"},{"location":"api-docs_backup/models/#method-transform_1","text":"transform(X) This file was automatically generated via lazydocs .","title":"method transform"},{"location":"api-docs_backup/plot/","text":"module plot Global Variables global_config function plot_embedding plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_unsupervised_results plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects. function plot_ethogram plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects function create_ethogram_video create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None function create_sample_videos create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None function create_mosaic_video create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None class MplColorHelper method __init__ __init__(cmap_name, start_val, stop_val) method get_rgb get_rgb(val) This file was automatically generated via lazydocs .","title":"Plot"},{"location":"api-docs_backup/plot/#module-plot","text":"","title":"module plot"},{"location":"api-docs_backup/plot/#global-variables","text":"global_config","title":"Global Variables"},{"location":"api-docs_backup/plot/#function-plot_embedding","text":"plot_embedding( dataset: DataFrame, col_names: list = ['embedding_0', 'embedding_1'], color_col: str = None, figsize: tuple = (10, 10), **kwargs ) \u2192 tuple Scatterplot of a 2D TSNE or UMAP embedding from the dataset. Args: dataset : data col_names : list of column names to use for the x and y axes color_col : if provided, a column that will be used to color the points in the scatter plot figsize : tuple with the dimensions of the plot (in inches) kwargs : All other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_embedding"},{"location":"api-docs_backup/plot/#function-plot_unsupervised_results","text":"plot_unsupervised_results( dataset: DataFrame, cluster_results: tuple, col_names: list = ['embedding_0', 'embedding_1'], figsize: tuple = (15, 4), **kwargs ) Set of plots for unsupervised behavior clustering results Args: dataset : data cluster_results : tuple output by 'cluster_behaviors' col_names : list of column names to use for the x and y axes figsize : tuple with the plot dimensions, in inches kwargs : all other keyword pairs are sent to Matplotlib's scatter function Returns: tuple (fig, axes). The Figure and Axes objects.","title":"function plot_unsupervised_results"},{"location":"api-docs_backup/plot/#function-plot_ethogram","text":"plot_ethogram( dataset: DataFrame, vid_key: str, query_label: str = 'unsup_behavior_label', frame_limit: int = 4000, figsize: tuple = (16, 2) ) \u2192 tuple Simple ethogram of one video, up to a certain frame number. Args: dataset: - vid_key : key (in dataset.metadata) pointing to the video to make ethogram for - query_label : the column containing the behavior labels to plot - frame_limit : only make the ethogram for frames between [0, frame_limit] - figsize : tuple with figure size (in inches) Returns: tuple (fig, axes). The Figure and Axes objects","title":"function plot_ethogram"},{"location":"api-docs_backup/plot/#function-create_ethogram_video","text":"create_ethogram_video( dataset: DataFrame, vid_key: str, query_label: str, out_file: str, frame_limit: int = 4000, im_dim: float = 16, min_frames: int = 3 ) \u2192 None Overlay ethogram on top of source video with ffmpeg Args: dataset : source dataset vid_key : the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video_files' that points to the source video location query_label : the column containing the behavior labels to plot out_file : output path for created video frame_limit : only make the ethogram/video for frames [0, frame_limit] in_dim : x dimension (in inches) of ethogram min_frames : any behaviors occurring for less than this number of frames are not labeled Returns: None","title":"function create_ethogram_video"},{"location":"api-docs_backup/plot/#function-create_sample_videos","text":"create_sample_videos( dataset: DataFrame, video_dir: str, out_dir: str, query_col: str = 'unsup_behavior_label', N_sample_rows: int = 16, window_size: int = 2, fps: float = 30, N_supersample_rows: int = 1000 ) \u2192 None Create a sample of videos displaying the labeled behaviors using ffmpeg. For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. Args: dataset : source dataset video_dir : location of source video files out_dir : base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi query_label : the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior N_sample_rows : number of clips to extract per behavior window_size : amount of video to extract on either side of the sampled frame, in seconds fps : frames per second of videos N_supersample_rows : this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. Returns: None","title":"function create_sample_videos"},{"location":"api-docs_backup/plot/#function-create_mosaic_video","text":"create_mosaic_video( vid_dir: str, output_file: str, ndim: tuple = (1600, 1200) ) \u2192 None Take a set of video clips and turn them into a mosaic using ffmpeg 16 videos are tiled. Args: vid_dir : source directory with videos in it output_file : output video path ndim : tuple with the output video dimensions, in pixels Returns: None","title":"function create_mosaic_video"},{"location":"api-docs_backup/plot/#class-mplcolorhelper","text":"","title":"class MplColorHelper"},{"location":"api-docs_backup/plot/#method-__init__","text":"__init__(cmap_name, start_val, stop_val)","title":"method __init__"},{"location":"api-docs_backup/plot/#method-get_rgb","text":"get_rgb(val) This file was automatically generated via lazydocs .","title":"method get_rgb"},{"location":"api-docs_backup/unsupervised/","text":"module unsupervised function compute_tsne_embedding compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding function compute_morlet compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time. function compute_density compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts) function compute_watershed compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate. function cluster_behaviors cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"Unsupervised"},{"location":"api-docs_backup/unsupervised/#module-unsupervised","text":"","title":"module unsupervised"},{"location":"api-docs_backup/unsupervised/#function-compute_tsne_embedding","text":"compute_tsne_embedding( dataset: DataFrame, cols: list, N_rows: int = 20000, n_components=2, perplexity=30 ) \u2192 tuple Compute TSNE embedding. Only for a random subset of rows. Args: dataset : Input data cols : A list of column names to produce the embedding for N_rows : A number of rows to randomly sample for the embedding. Only these rows are embedded. n_components : The number of dimensions to embed the data into. perplexity : The perplexity of the TSNE embedding. Returns: The tuple: - A numpy array with the embedding data, only for a random subset of row - The rows that were used for the embedding","title":"function compute_tsne_embedding"},{"location":"api-docs_backup/unsupervised/#function-compute_morlet","text":"compute_morlet( data: ndarray, dt: float = 0.03333333333333333, n_freq: int = 5, w: float = 3 ) \u2192 ndarray Compute morlet wavelet transform of a time series. Args: data : A 2D array containing the time series data, with dimensions (n_pts x n_channels) dt : The time step of the time series n_freq : The number of frequencies to compute w : The width of the morlet wavelet Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time.","title":"function compute_morlet"},{"location":"api-docs_backup/unsupervised/#function-compute_density","text":"compute_density( dataset: DataFrame, embedding_extent: tuple, bandwidth: float = 0.5, n_pts: int = 300, N_sample_rows: int = 50000, rows: list = None ) \u2192 ndarray Compute kernel density estimate of embedding. Args: dataset : pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') embedding_extent : the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) bandwidth : the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out n_pts : number of points over which to evaluate the KDE N_sample_rows : number of rows to randomly sample to generate estimate rows : If provided, use these rows instead of a random sample Returns: Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts)","title":"function compute_density"},{"location":"api-docs_backup/unsupervised/#function-compute_watershed","text":"compute_watershed( dens_matrix: ndarray, positive_only: bool = False, cutoff: float = 0 ) \u2192 tuple Compute watershed clustering of a density matrix. Args: dens_matrix : A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. positive_only : Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. cutoff : The cutoff value to apply if positive_only = True Returns: A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate.","title":"function compute_watershed"},{"location":"api-docs_backup/unsupervised/#function-cluster_behaviors","text":"cluster_behaviors( dataset: DataFrame, feature_cols: list, N_rows: int = 200000, use_morlet: bool = False, use_umap: bool = True, n_pts: int = 300, bandwidth: float = 0.5, **kwargs ) \u2192 tuple Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. Note that this will modify the dataset dataframe in place. The following columns are added to dataset: 'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix 'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. Args: dataset : the pd.DataFrame with the features of interest feature_cols : list of column names to perform the clustering on N_rows : number of rows to perform the embedding on. If 'None', then all rows are used. use_morlet : Apply Morlet wavelet transform to the feature cols before computing the embedding use_umap : If True will use UMAP dimensionality reduction, if False will use TSNE n_pts : dimension of grid the kernel density estimate is evaluated on. bandwidth : Gaussian kernel bandwidth for kernel estimate **kwargs : All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) Returns: A tuple with components: - dens_matrix : the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding - labels : numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs - embedding_extent : the coordinates in embedding space that dens_matrix is approximating the density over This file was automatically generated via lazydocs .","title":"function cluster_behaviors"},{"location":"api-docs_backup/utils/","text":"module utils Small helper utilities function checkFFMPEG checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api-docs_backup/utils/#module-utils","text":"Small helper utilities","title":"module utils"},{"location":"api-docs_backup/utils/#function-checkffmpeg","text":"checkFFMPEG() \u2192 bool Check for ffmpeg dependencies Returns: True if can find ffmpeg in path, false otherwise This file was automatically generated via lazydocs .","title":"function checkFFMPEG"},{"location":"api-docs_backup/version/","text":"module version This file was automatically generated via lazydocs .","title":"Version"},{"location":"api-docs_backup/version/#module-version","text":"This file was automatically generated via lazydocs .","title":"module version"},{"location":"api-docs_backup/video/","text":"module video Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class. A note on unit conversions For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed. Global Variables global_config FEATURE_MAKERS UNIT_DICT function create_metadata create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided function create_dataset create_dataset( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods. function load_experiment load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file function get_sample_openfield_data get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files class EthologyMetadataAccessor method __init__ __init__(pandas_obj) property n_videos property reverse_label_key property videos class EthologyFeaturesAccessor method __init__ __init__(pandas_obj) method activate activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated). method add add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed method deactivate deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns. method deactivate_cols deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features. method regex regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names class EthologyPoseAccessor method __init__ __init__(pandas_obj) class EthologyMLAccessor method __init__ __init__(pandas_obj) property features property folds property group property labels property splitter class EthologyIOAccessor method __init__ __init__(pandas_obj) method load load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file. method save save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path. method save_movie save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out' method to_dlc_csv to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"Video"},{"location":"api-docs_backup/video/#module-video","text":"Basic video tracking and behavior class that houses data. Basic object is the ExperimentDataFrame class.","title":"module video"},{"location":"api-docs_backup/video/#a-note-on-unit-conversions","text":"For the unit rescaling, if the dlc/tracking file is already in desired units, either in physical distances, or pixels, then don't provide all of 'frame_width', 'resolution', and 'frame_width_units'. If you want to keep track of the units, you can add a 'units' key to the metadata. This could be 'pixels', or 'cm', as appropriate. If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide 'frame_width', 'frame_width_units' and 'resolution' for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates. Resolution is a tuple (H,W) in pixels of the videos. 'frame_width' is the width of the image, in units 'frame_width_units' When this is done, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video If any of the provided parameters are provided, but are not the right format, or some values are missing, a warning is given and the rescaling is not performed.","title":"A note on unit conversions"},{"location":"api-docs_backup/video/#global-variables","text":"global_config FEATURE_MAKERS UNIT_DICT","title":"Global Variables"},{"location":"api-docs_backup/video/#function-create_metadata","text":"create_metadata(tracking_files: list, **kwargs) \u2192 dict Prepare a metadata dictionary for defining a ExperimentDataFrame. Only required argument is list of DLC tracking file names. Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the DLC tracking files, or an iterable object of the same length of the list of DLC tracking files. Each element in the iterable will be tagged with the corresponding DLC file. Args: tracking_files : List of DLC tracking .csvs **kwargs : described as above Returns: Dictionary whose keys are DLC tracking file names, and contains a dictionary with key,values containing the metadata provided","title":"function create_metadata"},{"location":"api-docs_backup/video/#function-create_dataset","text":"create_dataset( metadata: dict, label_key: dict = None, part_renamer: dict = None, animal_renamer: dict = None ) \u2192 DataFrame Houses DLC tracking data and behavior annotations in pandas DataFrame for ML, along with relevant metadata, features and behavior annotation labels. Args: metadata : Dictionary whose keys are DLC tracking csvs, and value is a dictionary of associated metadata for that video. Most easiest to create with 'create_metadata'. Required keys are : ['fps'] label_key : Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided. part_renamer : Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) animal_renamer : Default None. Dictionary that can rename animals from tracking files if needed Returns: DataFrame object. This is a pandas DataFrame with additional metadata and methods.","title":"function create_dataset"},{"location":"api-docs_backup/video/#function-load_experiment","text":"load_experiment(fn_in: str) \u2192 DataFrame Load DataFrame from file. Args: fn_in : path to file to load Returns: DataFrame object from pickle file","title":"function load_experiment"},{"location":"api-docs_backup/video/#function-get_sample_openfield_data","text":"get_sample_openfield_data() Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. Returns: DataFrame with the corresponding tracking and behavior annotation files","title":"function get_sample_openfield_data"},{"location":"api-docs_backup/video/#class-ethologymetadataaccessor","text":"","title":"class EthologyMetadataAccessor"},{"location":"api-docs_backup/video/#method-__init__","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs_backup/video/#property-n_videos","text":"","title":"property n_videos"},{"location":"api-docs_backup/video/#property-reverse_label_key","text":"","title":"property reverse_label_key"},{"location":"api-docs_backup/video/#property-videos","text":"","title":"property videos"},{"location":"api-docs_backup/video/#class-ethologyfeaturesaccessor","text":"","title":"class EthologyFeaturesAccessor"},{"location":"api-docs_backup/video/#method-__init___1","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs_backup/video/#method-activate","text":"activate(name: str) \u2192 list Add already present columns in data frame to the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be added Returns: List of matched columns (may include columns that were already activated).","title":"method activate"},{"location":"api-docs_backup/video/#method-add","text":"add( feature_maker, featureset_name: str = None, add_to_features=True, required_columns=[], **kwargs ) \u2192 list Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore. Args: featuremaker : A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work featureset_name : Name to prepend to the added features add_to_features : Whether to add to list of active features (i.e. will be returned by the .features property) Returns: List of new columns that are computed","title":"method add"},{"location":"api-docs_backup/video/#method-deactivate","text":"deactivate(name: str) \u2192 list Remove columns from the feature set. Args: name : string for pattern matching -- any feature that starts with this string will be removed Returns: List of removed columns.","title":"method deactivate"},{"location":"api-docs_backup/video/#method-deactivate_cols","text":"deactivate_cols(col_names: list) \u2192 list Remove provided columns from set of feature columns. Args: col_names : list of column names Returns: The columns that were removed from those designated as features.","title":"method deactivate_cols"},{"location":"api-docs_backup/video/#method-regex","text":"regex(pattern: str) \u2192 list Return a list of column names that match the provided regex pattern. Args: pattern : a regex pattern to match column names to Returns: list of column names","title":"method regex"},{"location":"api-docs_backup/video/#class-ethologyposeaccessor","text":"","title":"class EthologyPoseAccessor"},{"location":"api-docs_backup/video/#method-__init___2","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs_backup/video/#class-ethologymlaccessor","text":"","title":"class EthologyMLAccessor"},{"location":"api-docs_backup/video/#method-__init___3","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs_backup/video/#property-features","text":"","title":"property features"},{"location":"api-docs_backup/video/#property-folds","text":"","title":"property folds"},{"location":"api-docs_backup/video/#property-group","text":"","title":"property group"},{"location":"api-docs_backup/video/#property-labels","text":"","title":"property labels"},{"location":"api-docs_backup/video/#property-splitter","text":"","title":"property splitter"},{"location":"api-docs_backup/video/#class-ethologyioaccessor","text":"","title":"class EthologyIOAccessor"},{"location":"api-docs_backup/video/#method-__init___4","text":"__init__(pandas_obj)","title":"method __init__"},{"location":"api-docs_backup/video/#method-load","text":"load(fn_in: str) \u2192 DataFrame Load ExperimentDataFrame object from pickle file. Args: fn_in : path to load pickle file from. Returns: None. Data in this object is populated with contents of file.","title":"method load"},{"location":"api-docs_backup/video/#method-save","text":"save(fn_out: str) \u2192 None Save ExperimentDataFrame object with pickle. Args: fn_out : location to write pickle file to Returns: None. File is saved to path.","title":"method save"},{"location":"api-docs_backup/video/#method-save_movie","text":"save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid. ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of DLC tracks is known. Args: label_columns : list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. path_out : the directory to output the videos too video_filenames : list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. Returns: None. Videos are saved to 'path_out'","title":"method save_movie"},{"location":"api-docs_backup/video/#method-to_dlc_csv","text":"to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None Save ExperimentDataFrame tracking files to DLC csv format. Only save tracking data, not other computed features. Args: base_dir : base_dir to write DLC csv files to save_h5_too : if True, also save the data as an h5 file Returns: None. Files are saved to path. This file was automatically generated via lazydocs .","title":"method to_dlc_csv"}]}