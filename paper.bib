@article {Nilsson2020-simba,
	author = {Nilsson, Simon RO and Goodwin, Nastacia L. and Choong, Jia Jie and Hwang, Sophia and Wright, Hayden R and Norville, Zane C and Tong, Xiaoyu and Lin, Dayu and Bentzley, Brandon S. and Eshel, Neir and McLaughlin, Ryan J and Golden, Sam A.},
	title = {Simple Behavioral Analysis (SimBA) {\textendash} an open source toolkit for computer classification of complex social behaviors in experimental animals},
	elocation-id = {2020.04.19.049452},
	year = {2020},
	doi = {10.1101/2020.04.19.049452},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Aberrant social behavior is a core feature of many neuropsychiatric disorders, yet the study of complex social behavior in freely moving rodents is relatively infrequently incorporated into preclinical models. This likely contributes to limited translational impact. A major bottleneck for the adoption of socially complex, ethology-rich, preclinical procedures are the technical limitations for consistently annotating detailed behavioral repertoires of rodent social behavior. Manual annotation is subjective, prone to observer drift, and extremely time-intensive. Commercial approaches are expensive and inferior to manual annotation. Open-source alternatives often require significant investments in specialized hardware and significant computational and programming knowledge. By combining recent computational advances in convolutional neural networks and pose-estimation with further machine learning analysis, complex rodent social behavior is primed for inclusion under the umbrella of computational neuroethology.Here we present an open-source package with graphical interface and workflow (Simple Behavioral Analysis, SimBA) that uses pose-estimation to create supervised machine learning predictive classifiers of rodent social behavior, with millisecond resolution and accuracies that can out-perform human observers. SimBA does not require specialized video acquisition hardware nor extensive computational background. Standard descriptive statistical analysis, along with graphical region of interest annotation, are provided in addition to predictive classifier generation. To increase ease-of-use for behavioural neuroscientists, we designed SimBA with accessible menus for pre-processing videos, annotating behavioural training datasets, selecting advanced machine learning options, robust classifier validation functions and flexible visualizations tools. This allows for predictive classifier transparency, explainability and tunability prior to, and during, experimental use. We demonstrate that this approach is flexible and robust in both mice and rats by classifying social behaviors that are commonly central to the study of brain function and social motivation. Finally, we provide a library of poseestimation weights and behavioral predictive classifiers for resident-intruder behaviors in mice and rats. All code and data, together with detailed tutorials and documentation, are available on the SimBA GitHub repository.Graphical abstract SimBA graphical interface (GUI) for creating supervised machine learning classifiers of rodent social behavior.(a) Pre-process videos. SimBA supports common video pre-processing functions (e.g., cropping, clipping, sampling, format conversion, etc.) that can be performed either on single videos, or as a batch.(b) Managing poseestimation data and creating classification projects. Pose-estimation tracking projects in DeepLabCut and DeepPoseKit can be either imported or created and managed within the SimBA graphical user interface, and the tracking results are imported into SimBA classification projects.SimBA also supports userdrawn region-of-interests (ROIs) for descriptive statistics of animal movements, or as features in machine learning classification projects.(c) Create classifiers, perform classifications, and analyze classification data. SimBA has graphical tools for correcting poseestimation tracking inaccuracies when multiple subjects are within a single frame, annotating behavioral events from videos, and optimizing machine learning hyperparameters and discrimination thresholds. A number of validation checkpoints and logs are included for increased classifier explainability and tunability prior to, and during, experimental use. Both detailed and summary data are provided at the end of classifier analysis. SimBA accepts behavioral annotations generated elsewhere (such as through JWatcher) that can be imported into SimBA classification projects.(d) Visualize classification results. SimBA has several options for visualizing machine learning classifications, animal movements and ROI data, and analyzing the durations and frequencies of classified behaviors.See the SimBA GitHub repository for a comprehensive documentation and user tutorials.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/04/21/2020.04.19.049452},
	eprint = {https://www.biorxiv.org/content/early/2020/04/21/2020.04.19.049452.full.pdf},
	journal = {bioRxiv}
}
@software{Mathis2020-DLC2Kinematics,
  author       = {Mathis, Mackenzie and
                  Lauer, Jessy and
                  Nath, Tanmay and
                  Beauzile, Michael and
                  Hausmann, Sébastien and
                  Schneider, Steffen and
                  Mathis, Alexander},
  title        = {{DLC2Kinematics: a post-deeplabcut module for 
                   kinematic analysis}},
  month        = feb,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.0.4},
  doi          = {10.5281/zenodo.6669074},
  url          = {https://doi.org/10.5281/zenodo.6669074}
}
@article{Sturman2020-dlcanalyzer,
author = {Sturman, Oliver and Ziegler, Lukas and Schläppi, Christa and Akyol, Furkan and Privitera, Mattia and Slominski, Daria and Grimm, Christina and Thieren, Laetitia and Zerbi, Valerio and Grewe, Benjamin and Bohacek, Johannes},
year = {2020},
month = {07},
pages = {},
title = {Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions},
volume = {45},
journal = {Neuropsychopharmacology},
doi = {10.1038/s41386-020-0776-y}
}
@article{Mathis2020DeepLT,
    title={Deep learning tools for the measurement of animal behavior in neuroscience},
    author={Mackenzie W. Mathis and Alexander Mathis},
    journal={Current Opinion in Neurobiology},
    year={2020},
    volume={60},
    pages={1-11}
}
@article{Lauer2022MultianimalPE,
    title={Multi-animal pose estimation, identification and tracking with DeepLabCut},
    author={Jessy Lauer and Mu Zhou and Shaokai Ye and William Menegas and Steffen Schneider and Tanmay Nath and Mohammed Mostafizur Rahman and     Valentina Di Santo and Daniel Soberanes and Guoping Feng and Venkatesh N. Murthy and George Lauder and Catherine Dulac and M. Mathis and Alexander Mathis},
    journal={Nature Methods},
    year={2022},
    volume={19},
    pages={496 - 504}
}
@article {Segalin2021-bento,
article_type = {journal},
title = {The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice},
author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J and Perona, Pietro and Anderson, David J and Kennedy, Ann},
editor = {Berman, Gordon J and Wassum, Kate M and Gal, Asaf},
volume = 10,
year = 2021,
month = {nov},
pub_date = {2021-11-30},
pages = {e63720},
citation = {eLife 2021;10:e63720},
doi = {10.7554/eLife.63720},
url = {https://doi.org/10.7554/eLife.63720},
abstract = {The study of naturalistic social behavior requires quantification of animals’ interactions. This is generally done through manual annotation—a highly time-consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS’s annotations to human annotations and find that MARS’s pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis in a package that is user-friendly and easily modifiable.},
keywords = {social behavior, pose estimation, machine learning, computer vision, microendoscopic imaging, software},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}
