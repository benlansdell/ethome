{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ethome","title":"Ethome","text":"<p>Tools for machine learning of animal behavior. </p> <p>This library interprets pose-tracking files and behavior annotations to create features, train behavior classifiers, interpolate pose tracking data and other common analysis tasks. </p> <p>At present pose tracking data from DLC, SLEAP and NWB formats are supported, and behavior annotations from BORIS and NWB formats are supported.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Read in animal pose data and corresponding behavior annotations to make supervised learning easy</li> <li>Scale data to desired physical units</li> <li>Interpolate pose data to improve low-confidence predictions </li> <li>Create generic features for analysis and downstream ML tasks</li> <li>Create features specifically for mouse resident-intruder setup</li> <li>Quickly generate plots and movies with behavior predictions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install ethome-ml\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>It's easiest to start with an NWB file, which has metadata already connected to the pose data. </p> <p>Import</p> <pre><code>from ethome import create_dataset\nfrom ethome.io import get_sample_nwb_paths\n</code></pre> <p>Gather a sample NWB file</p> <pre><code>fn_in = get_sample_nwb_paths()\n</code></pre> <p>Create the dataframe:</p> <pre><code>dataset = create_dataset(fn_in)\n</code></pre> <p><code>dataset</code> is an extended pandas DataFrame, so can be treated exactly as you would treat any other dataframe. <code>ethome</code> adds a bunch of metadata about the dataset, for instance you can list the body parts with:</p> <pre><code>dataset.pose.body_parts\n</code></pre> <p>A key functionality of <code>ethome</code> is the ability to easily create features for machine learning. You can use pre-built featuresets or make your own. For instance:</p> <pre><code>dataset.features.add('distances')\n</code></pre> <p>will compute all distances between all body parts (both between and within animals).</p> <p>There are featuresets specifically tailored for social mice studies (resident intruder). For instance, </p> <pre><code>dataset.features.add('cnn1d_prob')\n</code></pre> <p>Uses a pretrained CNN to output probabilities of 3 behaviors (attack, mount, social investigation). For this, you must have labeled your body parts in a certain way (refer to How To). Other, more generic, feature creation functions are provided that work for any animal configuration. </p> <p>Now you can access a features table, labels, and groups for learning with <code>dataset.ml.features, dataset.ml.labels, dataset.ml.group</code>. From here it's easy to use some ML libraries to train a behavior classifier. For example:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n\ncv = LeaveOneGroupOut()\nmodel = RandomForestClassifier()\ncross_val_score(model, \n                dataset.ml.features, \n                dataset.ml.labels, \n                groups = dataset.ml.group, \n                cv = cv)\n</code></pre> <p>Since the <code>dataset</code> object is just an extended Pandas dataframe we can manipulate it as such. E.g. we can add our model predictions to the dataframe:</p> <pre><code>from sklearn.model_selection import cross_val_predict\npredictions = cross_val_predict(model, \n                                dataset.ml.features, \n                                dataset.ml.labels, \n                                groups = dataset.ml.group, \n                                cv = cv)\ndataset['prediction'] = predictions\n</code></pre> <p>If the raw video file paths are provided in the metadata, under the <code>video</code> key, we can make a movie overlaying these predictions over the original video:</p> <pre><code>dataset.io.save_movie(['label', 'prediction'], '.')\n</code></pre> <p>where <code>label</code> and <code>prediction</code> reference column names to annotate the video with.</p> <p>A more detailed run through of features is provided in the How To guide.</p>"},{"location":"how-to/","title":"How To guide","text":"<p>This guide covers the all of the tasks you can perform with <code>ethome</code>, roughly in the order you'd want to do them. A very basic outline is also in the Quick Start section of the readme. </p> <p>This guide covers basic usage -- it doesn't comprehensively describe how to use every function or feature in <code>ethome</code>; you can consult the API docs for complete information on usage. After installation, cut and past the code samples below to follow along.</p>"},{"location":"how-to/#0-installation","title":"0 Installation","text":"<p>Just do</p> <pre><code>pip install ethome-ml\n</code></pre> <p>You may want to also install <code>tensorflow</code> if you want to use the CNN features for a resident-intruder setup.</p>"},{"location":"how-to/#1-loading-your-data","title":"1 Loading your data","text":"<p><code>ethome</code> makes it easy to perform common machine learning analyses on pose-tracking data, perhaps in combination with behavioral annotations. The basic object of the package is an extended pandas <code>DataFrame</code>, which provides associated support functions that are suited for behavior analysis. The <code>DataFrame</code> object will house data from one or more video's worth of pose data, along with associated metadata for each video. The key thing you need to get started, then, is pose tracking data. At present, data from DeepLabCut, SLEAP or pose data stored in NWB files is supported (via the <code>ndx-pose</code> extension). </p>"},{"location":"how-to/#1a-loading-nwb-files","title":"1a Loading NWB files","text":"<p>The NeurodataWithoutBorders format can store both pose tracking data and behavioral annotations, along with associated metadata. If all of your data is stored in this format, then it's easy to import it into <code>ethome</code>:</p> <pre><code>from ethome import create_dataset\nfrom ethome.io import get_sample_nwb_paths\nfn_in = get_sample_nwb_paths()\nrecordings = create_dataset(fn_in)\n</code></pre> <p>You can provide multiple recordings, just provide a list of paths instead. Each separate file is assumed to represent a similar setup but different session/experiment/time period. I.e., they're not meant to represent the same session from different cameras, or the same session for different animals.  </p>"},{"location":"how-to/#1b-loading-tracking-files","title":"1b Loading tracking files","text":"<p>If your data is stored in DeepLabCut <code>csv</code>s or <code>h5</code> files, or SLEAP exported analysis <code>h5</code> files, perhaps with accompanying behavioral annotations from BORIS, then you'll have to associate these with each other, and provide relevant metadata yourself. Sections 1b -&gt; 1e outline how to do this. Data stored in NWB files have already addressed each of these steps and you can skip these sections. </p> <p>To import just your tracking data, you can simple do: </p> <pre><code>tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv']\nrecordings = create_dataset(tracking_csvs)\n</code></pre> <p>But generally, you may want to provide metadata with each tracking file. This can be done just by providing keyword arguments:</p> <pre><code>tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv']\nfps = 30\nresolution = (1200, 1600)\nrecordings = create_dataset(tracking_csvs, fps = fps, resolution = resolution)\n</code></pre> <p>This loads the dataframe, with <code>fps</code> and <code>resolution</code> associated to each input csv. </p> <p>NOTE: Any keyword that is a list of the same length as the tracking files is zipped with the tracking files accordingly. That is, if the resolution of the videos is different:</p> <pre><code>tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv']\nfps = 30\nresolutions = [(1200, 1600), (800, 1200)]\nrecordings = create_dataset(tracking_csvs, fps = fps, resolution = resolutions)\n</code></pre> <p>Rather than assigning the same value to all videos, the entry <code>resolutions[i]</code> would then be associated with <code>tracking_csvs[i]</code>. These lists, therefore, must be sorted appropriately.</p> <p>NOTE: It is strongly recommended the <code>fps</code> field is provided for all videos, so that frame numbers can be converted into times, which is needed for loading data from other sources (e.g. BORIS).</p>"},{"location":"how-to/#1c-loading-behavioral-annotation-data","title":"1c Loading behavioral annotation data","text":"<p>The (optional) <code>labels</code> keyword is used to include corresponding behavioral annotation files:</p> <pre><code>tracking_csvs = ['./dlc_tracking_file_1.csv', './dlc_tracking_file_2.csv']\nlabels = ['./boris_tracking_file_1.csv', './boris_tracking_file_2.csv']\nrecordings = create_dataset(tracking_csvs, labels = labels)\n</code></pre> <p>Behavior annotations should be exported from BORIS, in tabular form (as a csv), for each video to be imported.</p>"},{"location":"how-to/#1d-loading-video-data","title":"1d Loading video data","text":"<p>The (optional) <code>video</code> keyword is used to provide the path(s) to the corresponding video(s) that were tracked. If available, this will be used by some of the visualization functions.</p>"},{"location":"how-to/#1e-scaling-pose-data","title":"1e Scaling pose data","text":"<p>There is some support for scaling the data to get it into desired units, consistent across all recordings. </p> <p>If the tracking is in pixels and you do want to rescale it to some physical distance, you should provide keywords <code>frame_width</code>, <code>frame_width_units</code> and <code>resolution</code> for all videos. This ensures the entire dataset is using the same units. The package will use these values for each video to rescale the (presumed) pixel coordinates to physical coordinates.</p> <p><code>resolution</code> is a tuple (H,W) in pixels of the videos and <code>frame_width</code> is the width of the image, in units <code>frame_width_units</code>.</p> <p>By default, all coordinates are converted to 'mm'. The pair 'units':'mm' is added to the metadata dictionary for each video. You can specify the units by providing the <code>units</code> key yourself. Supported units include: 'mm', 'cm', 'm', 'in', 'ft'.</p> <p>If the DLC/tracking files are already in desired units, either in physical distances, or pixels, then do not provide all of the fields <code>frame_width</code>, <code>resolution</code>, and <code>frame_width_units</code>. If you want to keep track of the units, you can add a <code>units</code> key to the metadata. This could be <code>pixels</code>, <code>cm</code>, etc, as appropriate.</p>"},{"location":"how-to/#1f-renaming-things","title":"1f Renaming things","text":"<p>If your tracking project named the animals some way, but you want them named another way in this dataframe, you can provide an <code>animal_renamer</code> dictionary as an argument to the constructor:</p> <pre><code>recordings = create_dataset(tracking_csvs, animal_renamer={'adult': 'resident', 'juvenile':'intruder'})\n</code></pre> <p>Similarly with the body parts, you can provide a <code>part_renamer</code> dictionary.</p>"},{"location":"how-to/#1g-metadata","title":"1g Metadata","text":"<p>When <code>recordings</code> is created, additional metadata is computed and accessible via: * <code>recordings.metadata</code> houses the following attributes:     * <code>details</code>: the metadata given to create_dataset     * <code>videos</code>: list of videos given in <code>metadata</code>     * <code>n_videos</code>: number of videos in DataFrame     * <code>label_key</code>: associates numbers with text labels for each behavior     * <code>reverse_label_key</code>: associates text labels for each behavior number * <code>recordings.pose</code>, houses pose information:     * <code>body_parts</code>: list of body parts loaded from the DLC/SLEAP file(s)     * <code>animals</code>: list of animals loaded from the DLC/SLEAP files(s)     * <code>animal_setup</code>: dictionary detailing animal parts and names     * <code>raw_track_columns</code>: all original columns names loaded from DLC/SLEAP</p>"},{"location":"how-to/#2-interpolate-low-confidence-pose-tracking","title":"2 Interpolate low-confidence pose tracking","text":"<p>Some simple support for interpolating low-confidence tracks in DLC/SLEAP is provided. Often predicted locations below a given confidence level are noisy and unreliable, and better tracks may be obtained by removing these predictions and interpolating from more confident predictions on either side of the uncertain prediction. </p> <p>You can achieve this with</p> <pre><code>from ethome import interpolate_lowconf_points\ninterpolate_lowconf_points(recordings)\n</code></pre>"},{"location":"how-to/#3-generate-features","title":"3 Generate features","text":"<p>To do machine learning you'll want to create features from the pose tracking data. <code>ethome</code> can help you do this in a few different ways. You can either use one of the feature-making functions provided or create a custom feature-making function, or custom class. </p> <p>To use the inbuilt functions you can reference them by identifying string, or provide the function itself to the <code>features.add</code> function. For instance, to compute the distances between all body parts (within and between animals), you could do:</p> <pre><code>recordings.features.add('distances')\n</code></pre> <p>This will compute and add the distances between all body parts of all animals.</p>"},{"location":"how-to/#3a-in-built-support-for-resident-intruder-setup","title":"3a In-built support for resident-intruder setup","text":"<p>First, if your setup is a social mouse study, involving two mice, similar enough to the standard resident-intruder setup, then you can use some pre-designed feature sets. The body parts that are tracked must be those from the MARS dataset (See figure). You will have to have labeled and tracked your mice in DLC/SLEAP in the same way. (with the same animal and body part names -- <code>ethome</code>'s <code>create_dataset</code> function can rename them appropriately)</p> <p></p> <p>The animals must be named <code>resident</code> and <code>intruder</code>, and the body parts must be: <code>nose</code>, <code>leftear</code>, <code>rightear</code>, <code>neck</code>, <code>lefthip</code>, <code>righthip</code>, and <code>tail</code>.</p> <p>The <code>cnn1d_prob</code>, <code>mars</code>, <code>mars_reduced</code> and <code>social</code> functions can be used to make features for this setup. </p> <ul> <li><code>cnn1d_prob</code> runs a 1D CNN and outputs prediction probabilities of three behaviors (attack, mount, and investigation). Even if you're not interested in these exact behaviors, they may still be useful for predicting the occurance of other behaviors, as part of an ensemble model. </li> <li><code>mars</code> computes a long list of features as used in the MARS paper. You can refer to that paper for more details. </li> <li><code>mars_reduced</code> is a reduced version of the MARS features</li> <li><code>social</code> is a set of features that only involve measures of one animal in relation to the other.</li> </ul>"},{"location":"how-to/#3b-generic-features","title":"3b Generic features","text":"<p>You can generate more generic features using the following functions: * <code>centroid</code> the centroid of each animal's body parts * <code>centroid_velocity</code> the velocity of the centroids * <code>centroid_interanimal</code> the distances between the centroids of all the animals * <code>centroid_interanimal_speed</code> the rate of change of <code>centroid_interanimal</code> * <code>intrabodypartspeeds</code> the speeds of all body parts * <code>intrabodypartdistances</code> the distances between all animals body parts (inter- and intra-animal) * <code>distances</code> is an alias for <code>intrabodypartdistances</code></p> <p>These classes work for any animal setup, not just resident-intruder with specific body parts, as assumed for the <code>mars</code> features.</p>"},{"location":"how-to/#3c-add-your-own-features","title":"3c Add your own features","text":"<p>There are two ways to add your own feature sets to your DataFrame. </p> <p>The first is to create a function that takes a pandas DataFrame, and returns a new DataFrame with the features you want to add. For example:</p> <pre><code>def diff_cols(df, required_columns = []):\n    return df[required_columns].diff()\n\nrecordings.features.add(diff_cols, required_columns = ['resident_neck_x', 'resident_neck_y'])\n</code></pre> <p>The second is to create a class that has, at the least, the method <code>transform</code>. </p> <pre><code>class BodyPartDiff:\n    def __init__(self, required_columns):\n        self.required_columns = required_columns\n\n    def transform(self, df):\n        return df[self.required_columns].diff()\n\nhead_diff = BodyPartDiff(['resident_neck_x', 'resident_neck_y'])\nrecordings.features.add(head_diff)\n</code></pre> <p>This is more verbose than the above, but has the advantage that the it can be re-used. E.g. you may want to fit the instance to training data and apply it to test data, similar to an sklearn model.</p>"},{"location":"how-to/#3d-features-manipulation","title":"3d Features manipulation","text":"<p>By default, when new features are added to the dataframe, they are considered 'active'. Active features can be accessed through</p> <pre><code>recordings.ml.features\n</code></pre> <p>You can pass this to any ML method for further processing. This <code>.ml.features</code> is just a convenience for managing the long list of features you will have created in the steps above. You can always just treat <code>recordings</code> like a Pandas DataFrame and do ML how you would normally. </p> <p>To activate features you can use <code>recordings.features.activate</code>, and to deactivate features you can use <code>recordings.features.deactivate</code>. Deactivating keeps them in the DataFrame, but just no longer includes those features in the <code>recordings.ml.features</code> view.</p>"},{"location":"how-to/#4-fit-a-model-for-behavior-classification","title":"4 Fit a model for behavior classification","text":"<p>Ok! The hard work is done, so now you can easily train a behavior classifier based on the features you've computed and the labels provided. </p> <p>E.g.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n\ncv = LeaveOneGroupOut()\nmodel = RandomForestClassifier()\ncross_val_score(model, recordings.ml.features, recordings.ml.labels, recordings.ml.group, cv = cv)\n</code></pre> <p>A convenience function that essentially runs the above lines is provided,  <code>add_randomforest_predictions</code>:</p> <pre><code>from ethome import add_randomforest_predictions\nadd_randomforest_predictions(recordings)\n</code></pre> <p>which can be used as a starting point for developing behavior classifiers. </p>"},{"location":"how-to/#5-make-output-movies","title":"5 Make output movies","text":"<p>Now we have our model we can make a video of its predictions. Provide the column names whose state we're going to overlay on the video, along with the directory to output the videos:</p> <pre><code>recordings.io.save_movie(['label', 'prediction'], '.')\n</code></pre> <p>The video field in the <code>metadata</code>, specifying the path to the underlying video, has to be present for each recording for this to work. </p>"},{"location":"how-to/#6-save-your-data","title":"6 Save your data","text":"<p>You can save your data as a pickled DataFrame with</p> <pre><code>recordings.io.save('outfile.pkl')\n</code></pre> <p>(and can be loaded again with:)</p> <pre><code>recordings = pd.DataFrame.io.load('outfile.pkl')\n</code></pre> <p>NOTE: By importing <code>ethome</code> you extend the functionality of the pandas DataFrame, hence can access things like <code>.io.load</code></p>"},{"location":"how-to/#7-summary-and-reference-list-of-added-functionality-by-ethome","title":"7 Summary and reference list of added functionality by <code>ethome</code>","text":"<p>For reference, the metadata and added functions added to the dataframe are: * <code>recordings.metadata</code>, which houses     * <code>details</code>: the metadata dictionary given to create_dataset     * <code>videos</code>: list of videos given in <code>metadata</code>     * <code>n_videos</code>: number of videos in DataFrame     * <code>label_key</code>: associates numbers with text labels for each behavior     * <code>reverse_label_key</code>: associates text labels for each behavior number * <code>recordings.pose</code>, houses pose information:     * <code>body_parts</code>: list of body parts loaded from the DLC/SLEAP file(s)     * <code>animals</code>: list of animals loaded from the DLC/SLEAP files(s)     * <code>animal_setup</code>: dictionary detailing animal parts and names     * <code>raw_track_columns</code>: all original columns names loaded from DLC/SLEAP * <code>recordings.features</code>, feature creation and manipulation     * <code>activate</code>: activate columns by name     * <code>deactivate</code>: deactivate columns by name     * <code>regex</code>: select column names based on regex     * <code>add</code>: create new features * <code>recordings.ml</code>, machine learning conveniences     * <code>features</code>: a 'active' set of features     * <code>labels</code>: if loaded from BORIS, behavior labels. from text to label with <code>recordings.metadata.label_key</code>     * <code>group</code>: the corresponding video filename for all rows in the table -- can be used for GroupKFold CV, or similar * <code>recordings.io</code>, I/O functions     * <code>save</code>: save DataFrame as pickle file     * <code>to_dlc_csv</code>: save original tracking data back into csv -- if you interpolated or otherwise manipulated the data     * <code>load</code>: load DataFrame from pickle file     * <code>save_movie</code>: create a movie with some feature column you indicate overlaid</p> <p>See the API docs for usage details.</p>"},{"location":"how-to/#8-some-caveats","title":"8 Some caveats","text":"<p>The workflow assumes you import all your data into one DataFrame. Combining two DataFrames (append or concat) is not officially supported. It may behave well, or it may not.</p>"},{"location":"api-docs/","title":"Overview","text":""},{"location":"api-docs/#api-overview","title":"API Overview","text":""},{"location":"api-docs/#modules","title":"Modules","text":"<ul> <li><code>interpolation</code></li> <li><code>io</code>: Loading and saving tracking and behavior annotation files </li> <li><code>utils</code>: Small helper utilities</li> <li><code>video</code>: Basic video tracking and behavior class that houses data</li> </ul>"},{"location":"api-docs/#classes","title":"Classes","text":"<ul> <li><code>io.BufferedIOBase</code>: Base class for buffered IO objects.</li> <li><code>io.IOBase</code>: The abstract base class for all I/O classes, acting on streams of</li> <li><code>io.RawIOBase</code>: Base class for raw binary I/O.</li> <li><code>io.TextIOBase</code>: Base class for text I/O.</li> <li><code>io.UnsupportedOperation</code></li> <li><code>video.EthologyFeaturesAccessor</code></li> <li><code>video.EthologyIOAccessor</code></li> <li><code>video.EthologyMLAccessor</code></li> <li><code>video.EthologyMetadataAccessor</code></li> <li><code>video.EthologyPoseAccessor</code></li> </ul>"},{"location":"api-docs/#functions","title":"Functions","text":"<ul> <li><code>interpolation.interpolate_lowconf_points</code>: Interpolate raw tracking points if their probabilities are available.</li> <li><code>io.create_behavior_labels</code>: Create behavior labels from BORIS exported csv files.</li> <li><code>io.get_sample_data</code>: Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration</li> <li><code>io.get_sample_data_paths</code>: Get path to sample data files provided with package. </li> <li><code>io.get_sample_nwb_paths</code>: Get path to a sample NWB file with tracking data for testing and dev purposes.</li> <li><code>io.load_data</code>: Load an object from a pickle file</li> <li><code>io.load_sklearn_model</code>: Load sklearn model from file</li> <li><code>io.read_DLC_tracks</code>: Read in tracks from DLC.</li> <li><code>io.read_NWB_tracks</code>: Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package).</li> <li><code>io.read_boris_annotation</code>: Read behavior annotation from BORIS exported csv file. </li> <li><code>io.read_sleap_tracks</code>: Read in tracks from SLEAP.</li> <li><code>io.save_DLC_tracks_h5</code>: Save DLC tracks in h5 format.</li> <li><code>io.save_sklearn_model</code>: Save sklearn model to file</li> <li><code>io.uniquifier</code>: Return a sequence (e.g. list) with unique elements only, but maintaining original list order</li> <li><code>utils.checkFFMPEG</code>: Check for ffmpeg dependencies</li> <li><code>utils.check_keras</code></li> <li><code>video.add_randomforest_predictions</code>: Perform cross validation of a RandomForestClassifier to predict behavior based on </li> <li><code>video.create_dataset</code>: Creates DataFrame that houses pose-tracking data and behavior annotations, along with relevant metadata, features and behavior annotation labels.</li> <li><code>video.create_metadata</code>: Prepare a metadata dictionary for defining a ExperimentDataFrame. </li> <li><code>video.get_sample_openfield_data</code>: Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC.</li> <li><code>video.load_experiment</code>: Load DataFrame from file.</li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/features/","title":"Features","text":""},{"location":"api-docs/features/#module-features","title":"module <code>features</code>","text":""},{"location":"api-docs/features/#global-variables","title":"Global Variables","text":"<ul> <li>FEATURE_MAKERS</li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/interpolation/","title":"Interpolation","text":""},{"location":"api-docs/interpolation/#module-interpolation","title":"module <code>interpolation</code>","text":""},{"location":"api-docs/interpolation/#function-interpolate_lowconf_points","title":"function <code>interpolate_lowconf_points</code>","text":"<pre><code>interpolate_lowconf_points(\n    edf: DataFrame,\n    conf_threshold: float = 0.9,\n    in_place: bool = True,\n    rolling_window: bool = True,\n    window_size: int = 3\n) \u2192 DataFrame\n</code></pre> <p>Interpolate raw tracking points if their probabilities are available. </p> <p>Args:</p> <ul> <li><code>edf</code>:  pandas DataFrame containing the tracks to interpolate </li> <li><code>conf_threshold</code>:  default 0.9. Confidence below which to count as uncertain, and to interpolate its value instead </li> <li><code>in_place</code>:  default True. Whether to replace data in place </li> <li><code>rolling_window</code>:  default True. Whether to use a rolling window to interpolate </li> <li><code>window_size</code>:  default 3. The size of the rolling window to use </li> </ul> <p>Returns:  Pandas dataframe with the filtered raw columns. Returns None if opted for in_place modification </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/io/","title":"Io","text":""},{"location":"api-docs/io/#module-io","title":"module <code>io</code>","text":"<p>Loading and saving tracking and behavior annotation files  </p>"},{"location":"api-docs/io/#global-variables","title":"Global Variables","text":"<ul> <li>DEFAULT_BUFFER_SIZE</li> <li>SEEK_SET</li> <li>SEEK_CUR</li> <li>SEEK_END</li> <li>XY_IDS</li> <li>XYLIKELIHOOD_IDS</li> </ul>"},{"location":"api-docs/io/#function-uniquifier","title":"function <code>uniquifier</code>","text":"<pre><code>uniquifier(seq)\n</code></pre> <p>Return a sequence (e.g. list) with unique elements only, but maintaining original list order </p> <p></p>"},{"location":"api-docs/io/#function-save_sklearn_model","title":"function <code>save_sklearn_model</code>","text":"<pre><code>save_sklearn_model(model, fn_out)\n</code></pre> <p>Save sklearn model to file </p> <p>Args:</p> <ul> <li><code>model</code>:  sklearn model to save </li> <li><code>fn_out</code>:  filename to save to </li> </ul> <p></p>"},{"location":"api-docs/io/#function-load_sklearn_model","title":"function <code>load_sklearn_model</code>","text":"<pre><code>load_sklearn_model(fn_in)\n</code></pre> <p>Load sklearn model from file </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  filename to load from </li> </ul> <p>Returns:  the loaded sklearn model </p> <p></p>"},{"location":"api-docs/io/#function-read_dlc_tracks","title":"function <code>read_DLC_tracks</code>","text":"<pre><code>read_DLC_tracks(\n    fn_in: str,\n    part_renamer: dict = None,\n    animal_renamer: dict = None,\n    read_likelihoods: bool = True,\n    labels: DataFrame = None\n) \u2192 tuple\n</code></pre> <p>Read in tracks from DLC. </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  csv or h5 file that has DLC tracks </li> <li><code>part_renamer</code>:  dictionary to rename body parts, if needed  </li> <li><code>animal_renamer</code>:  dictionary to rename animals, if needed </li> <li><code>read_likelihoods</code>:  default True. Whether to attach DLC likelihoods to table </li> </ul> <p>Returns:  Pandas DataFrame with (n_animals2n_body_parts) columns plus with filename and frame,   List of body parts,  List of animals,  Columns names for DLC tracks (excluding likelihoods, if read in),  Scorer </p> <p></p>"},{"location":"api-docs/io/#function-read_sleap_tracks","title":"function <code>read_sleap_tracks</code>","text":"<pre><code>read_sleap_tracks(\n    fn_in: str,\n    part_renamer: dict = None,\n    animal_renamer: dict = None,\n    read_likelihoods: bool = True,\n    labels: DataFrame = None\n) \u2192 tuple\n</code></pre> <p>Read in tracks from SLEAP. </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  csv or h5 file that has sleap tracks </li> <li><code>part_renamer</code>:  dictionary to rename body parts, if needed  </li> <li><code>animal_renamer</code>:  dictionary to rename animals, if needed </li> <li><code>read_likelihoods</code>:  default True. Whether to attach DLC likelihoods to table </li> </ul> <p>Returns:  Pandas DataFrame with (n_animals2n_body_parts) columns plus with filename and frame,   List of body parts,  List of animals,  Columns names for DLC tracks (excluding likelihoods, if read in),  Scorer </p> <p></p>"},{"location":"api-docs/io/#function-read_nwb_tracks","title":"function <code>read_NWB_tracks</code>","text":"<pre><code>read_NWB_tracks(\n    fn_in: str,\n    part_renamer: dict = None,\n    animal_renamer: dict = None,\n    read_likelihoods: bool = True\n) \u2192 tuple\n</code></pre> <p>Read in tracks from NWB PoseEstimiationSeries format (something saved using the DLC2NWB package). </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  nwb file that has the tracking information </li> <li><code>part_renamer</code>:  dictionary to rename body parts, if needed  </li> <li><code>animal_renamer</code>:  dictionary to rename animals, if needed </li> <li><code>read_likelihoods</code>:  default True. Whether to attach DLC likelihoods to table </li> </ul> <p>Returns:  Pandas DataFrame with (n_animals2n_body_parts) columns plus with filename and frame,   List of body parts,  List of animals,  Columns names for pose tracks (excluding likelihoods, if read in),  Scorer </p> <p></p>"},{"location":"api-docs/io/#function-save_dlc_tracks_h5","title":"function <code>save_DLC_tracks_h5</code>","text":"<pre><code>save_DLC_tracks_h5(df: DataFrame, fn_out: str) \u2192 None\n</code></pre> <p>Save DLC tracks in h5 format. </p> <p>Args:</p> <ul> <li><code>df</code>:  Pandas dataframe to save </li> <li><code>fn_out</code>:  Where to save the dataframe </li> </ul> <p></p>"},{"location":"api-docs/io/#function-load_data","title":"function <code>load_data</code>","text":"<pre><code>load_data(fn: str)\n</code></pre> <p>Load an object from a pickle file </p> <p>Args:</p> <ul> <li><code>fn</code>:  The filename </li> </ul> <p>Returns:  The pickled object. </p> <p></p>"},{"location":"api-docs/io/#function-get_sample_nwb_paths","title":"function <code>get_sample_nwb_paths</code>","text":"<pre><code>get_sample_nwb_paths()\n</code></pre> <p>Get path to a sample NWB file with tracking data for testing and dev purposes. </p> <p>Returns:   Path to a sample NWB file. </p> <p></p>"},{"location":"api-docs/io/#function-get_sample_data_paths","title":"function <code>get_sample_data_paths</code>","text":"<pre><code>get_sample_data_paths()\n</code></pre> <p>Get path to sample data files provided with package.  </p> <p>Returns:   (tuple) list of DLC tracking file, list of boris annotation files </p> <p></p>"},{"location":"api-docs/io/#function-get_sample_data","title":"function <code>get_sample_data</code>","text":"<pre><code>get_sample_data()\n</code></pre> <p>Load a sample dataset of 5 mice social interaction videos. Each video is approx. 5 minutes in duration </p> <p>Returns:   (ExperimentDataFrame) Data frame with the corresponding tracking and behavior annotation files </p> <p></p>"},{"location":"api-docs/io/#function-read_boris_annotation","title":"function <code>read_boris_annotation</code>","text":"<pre><code>read_boris_annotation(\n    fn_in: str,\n    fps: int,\n    duration: float,\n    behav_labels: list = None\n) \u2192 tuple\n</code></pre> <p>Read behavior annotation from BORIS exported csv file.  </p> <p>This will import behavior types specified (or all types, if behavior_list is None) and assign a numerical label to each. Overlapping annotations (those occurring simulataneously) are not supported. Any time the video is annotated as being in multiple states, the last state will be the one labeled. </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  The filename with BORIS behavior annotations to load </li> <li><code>fps</code>:  The frames per second of the video </li> <li><code>duration</code>:  The duration of the video in seconds </li> <li><code>behav_labels</code>:  If provided, only import behaviors with these names. Default = None = import everything.  </li> </ul> <p>Returns:  A dictionary of numpy arrays which gives, for all frames, which behavior is occuring. 0 = no behavior, 1 = behavior. </p> <p></p>"},{"location":"api-docs/io/#function-create_behavior_labels","title":"function <code>create_behavior_labels</code>","text":"<pre><code>create_behavior_labels(boris_files)\n</code></pre> <p>Create behavior labels from BORIS exported csv files. </p> <p>Args:</p> <ul> <li><code>boris_files</code>:  List of BORIS exported csv files </li> </ul> <p>Returns:  A dictionary with keys the numerical labels and values the names of the behaviors. </p> <p></p>"},{"location":"api-docs/io/#class-bufferediobase","title":"class <code>BufferedIOBase</code>","text":"<p>Base class for buffered IO objects. </p> <p>The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). </p> <p>In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. </p> <p>A typical implementation should not inherit from a RawIOBase implementation, but wrap one. </p> <p></p>"},{"location":"api-docs/io/#class-iobase","title":"class <code>IOBase</code>","text":"<p>The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. </p> <p>This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. </p> <p>Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. </p> <p>The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. </p> <p>Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. </p> <p>IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. </p> <p>IOBase also supports the :keyword:<code>with</code> statement. In this example, fp is closed after the suite of the with statement is complete: </p> <p>with open('spam.txt', 'r') as fp:  fp.write('Spam and eggs!') </p> <p></p>"},{"location":"api-docs/io/#class-rawiobase","title":"class <code>RawIOBase</code>","text":"<p>Base class for raw binary I/O. </p> <p></p>"},{"location":"api-docs/io/#class-textiobase","title":"class <code>TextIOBase</code>","text":"<p>Base class for text I/O. </p> <p>This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. </p> <p></p>"},{"location":"api-docs/io/#class-unsupportedoperation","title":"class <code>UnsupportedOperation</code>","text":"<p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/plot/","title":"Plot","text":""},{"location":"api-docs/plot/#module-plot","title":"module <code>plot</code>","text":""},{"location":"api-docs/plot/#global-variables","title":"Global Variables","text":"<ul> <li>global_config</li> </ul>"},{"location":"api-docs/plot/#function-plot_embedding","title":"function <code>plot_embedding</code>","text":"<pre><code>plot_embedding(\n    dataset: DataFrame,\n    col_names: list = ['embedding_0', 'embedding_1'],\n    color_col: str = None,\n    figsize: tuple = (10, 10),\n    **kwargs\n) \u2192 tuple\n</code></pre> <p>Scatterplot of a 2D TSNE or UMAP embedding from the dataset. </p> <p>Args:</p> <ul> <li><code>dataset</code>:  data </li> <li><code>col_names</code>:  list of column names to use for the x and y axes </li> <li><code>color_col</code>:  if provided, a column that will be used to color the points in the scatter plot </li> <li><code>figsize</code>:  tuple with the dimensions of the plot (in inches) </li> <li><code>kwargs</code>:  All other keyword pairs are sent to Matplotlib's scatter function </li> </ul> <p>Returns:  tuple (fig, axes). The Figure and Axes objects.  </p> <p></p>"},{"location":"api-docs/plot/#function-plot_unsupervised_results","title":"function <code>plot_unsupervised_results</code>","text":"<pre><code>plot_unsupervised_results(\n    dataset: DataFrame,\n    cluster_results: tuple,\n    col_names: list = ['embedding_0', 'embedding_1'],\n    figsize: tuple = (15, 4),\n    **kwargs\n)\n</code></pre> <p>Set of plots for unsupervised behavior clustering results </p> <p>Args:</p> <ul> <li><code>dataset</code>:  data </li> <li><code>cluster_results</code>:  tuple output by 'cluster_behaviors' </li> <li><code>col_names</code>:  list of column names to use for the x and y axes </li> <li><code>figsize</code>:  tuple with the plot dimensions, in inches </li> <li><code>kwargs</code>:  all other keyword pairs are sent to Matplotlib's scatter function </li> </ul> <p>Returns:  tuple (fig, axes). The Figure and Axes objects.  </p> <p></p>"},{"location":"api-docs/plot/#function-plot_ethogram","title":"function <code>plot_ethogram</code>","text":"<pre><code>plot_ethogram(\n    dataset: DataFrame,\n    vid_key: str,\n    query_label: str = 'unsup_behavior_label',\n    frame_limit: int = 4000,\n    figsize: tuple = (16, 2)\n) \u2192 tuple\n</code></pre> <p>Simple ethogram of one video, up to a certain frame number. </p> <p>Args:   dataset:  - <code>vid_key</code>:  key (in dataset.metadata) pointing to the video to make ethogram for   - <code>query_label</code>:  the column containing the behavior labels to plot   - <code>frame_limit</code>:  only make the ethogram for frames between [0, frame_limit]   - <code>figsize</code>:  tuple with figure size (in inches) </p> <p>Returns:  tuple (fig, axes). The Figure and Axes objects </p> <p></p>"},{"location":"api-docs/plot/#function-create_ethogram_video","title":"function <code>create_ethogram_video</code>","text":"<pre><code>create_ethogram_video(\n    dataset: DataFrame,\n    vid_key: str,\n    query_label: str,\n    out_file: str,\n    frame_limit: int = 4000,\n    im_dim: float = 16,\n    min_frames: int = 3\n) \u2192 None\n</code></pre> <p>Overlay ethogram on top of source video with ffmpeg </p> <p>Args:</p> <ul> <li><code>dataset</code>:  source dataset </li> <li><code>vid_key</code>:  the key (in dataset.metadata) pointing to the video to make ethogram for. metadata must have field 'video' that points to the source video location </li> <li><code>query_label</code>:  the column containing the behavior labels to plot </li> <li><code>out_file</code>:  output path for created video </li> <li><code>frame_limit</code>:  only make the ethogram/video for frames [0, frame_limit] </li> <li><code>in_dim</code>:  x dimension (in inches) of ethogram </li> <li><code>min_frames</code>:  any behaviors occurring for less than this number of frames are not labeled </li> </ul> <p>Returns:  None </p> <p></p>"},{"location":"api-docs/plot/#function-create_sample_videos","title":"function <code>create_sample_videos</code>","text":"<pre><code>create_sample_videos(\n    dataset: DataFrame,\n    video_dir: str,\n    out_dir: str,\n    query_col: str = 'unsup_behavior_label',\n    N_sample_rows: int = 16,\n    window_size: int = 2,\n    fps: float = 30,\n    N_supersample_rows: int = 1000\n) \u2192 None\n</code></pre> <p>Create a sample of videos displaying the labeled behaviors using ffmpeg.  </p> <p>For each behavior label, randomly choose frames from the entire dataset and extract short clips from source videos based around those points. Tries to select frames where the labeled behavior is exhibited in many frames of the clip. </p> <p>Args:</p> <ul> <li><code>dataset</code>:  source dataset </li> <li><code>video_dir</code>:  location of source video files </li> <li><code>out_dir</code>:  base output directory to save videos. Videos are saved in the form: [out_dir]/[behavior_label]/[video_name]_[time in seconds].avi </li> <li><code>query_label</code>:  the column containing the behavior labels to extract clips for. Each unique value in this column is treated as a separate behavior </li> <li><code>N_sample_rows</code>:  number of clips to extract per behavior </li> <li><code>window_size</code>:  amount of video to extract on either side of the sampled frame, in seconds </li> <li><code>fps</code>:  frames per second of videos </li> <li><code>N_supersample_rows</code>:  this many rows are randomly sampled for each behavior label, and the top N_sample_rows are returned (in terms of number of adjacent frames also exhibiting that behavior). Shouldn't need to play with this. </li> </ul> <p>Returns:  None </p> <p></p>"},{"location":"api-docs/plot/#function-create_mosaic_video","title":"function <code>create_mosaic_video</code>","text":"<pre><code>create_mosaic_video(\n    vid_dir: str,\n    output_file: str,\n    ndim: tuple = (1600, 1200)\n) \u2192 None\n</code></pre> <p>Take a set of video clips and turn them into a mosaic using ffmpeg  </p> <p>16 videos are tiled. </p> <p>Args:</p> <ul> <li><code>vid_dir</code>:  source directory with videos in it </li> <li><code>output_file</code>:  output video path </li> <li><code>ndim</code>:  tuple with the output video dimensions, in pixels </li> </ul> <p>Returns:  None     </p> <p></p>"},{"location":"api-docs/plot/#class-mplcolorhelper","title":"class <code>MplColorHelper</code>","text":""},{"location":"api-docs/plot/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(cmap_name, start_val, stop_val)\n</code></pre>"},{"location":"api-docs/plot/#method-get_rgb","title":"method <code>get_rgb</code>","text":"<pre><code>get_rgb(val)\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/unsupervised/","title":"Unsupervised","text":""},{"location":"api-docs/unsupervised/#module-unsupervised","title":"module <code>unsupervised</code>","text":""},{"location":"api-docs/unsupervised/#function-compute_tsne_embedding","title":"function <code>compute_tsne_embedding</code>","text":"<pre><code>compute_tsne_embedding(\n    dataset: DataFrame,\n    cols: list,\n    N_rows: int = 20000,\n    n_components=2,\n    perplexity=30\n) \u2192 tuple\n</code></pre> <p>Compute TSNE embedding. Only for a random subset of rows. </p> <p>Args:</p> <ul> <li><code>dataset</code>:  Input data </li> <li><code>cols</code>:  A list of column names to produce the embedding for </li> <li><code>N_rows</code>:  A number of rows to randomly sample for the embedding. Only these rows are embedded. </li> <li><code>n_components</code>:  The number of dimensions to embed the data into. </li> <li><code>perplexity</code>:  The perplexity of the TSNE embedding. </li> </ul> <p>Returns:  The tuple:         - A numpy array with the embedding data, only for a random subset of row          - The rows that were used for the embedding </p> <p></p>"},{"location":"api-docs/unsupervised/#function-compute_morlet","title":"function <code>compute_morlet</code>","text":"<pre><code>compute_morlet(\n    data: ndarray,\n    dt: float = 0.03333333333333333,\n    n_freq: int = 5,\n    w: float = 3\n) \u2192 ndarray\n</code></pre> <p>Compute morlet wavelet transform of a time series. </p> <p>Args:</p> <ul> <li><code>data</code>:  A 2D array containing the time series data, with dimensions (n_pts x n_channels) </li> <li><code>dt</code>:  The time step of the time series </li> <li><code>n_freq</code>:  The number of frequencies to compute </li> <li><code>w</code>:  The width of the morlet wavelet </li> </ul> <p>Returns A 2D numpy array with the morlet wavelet transform. The first dimension is the frequency, the second is the time. </p> <p></p>"},{"location":"api-docs/unsupervised/#function-compute_density","title":"function <code>compute_density</code>","text":"<pre><code>compute_density(\n    dataset: DataFrame,\n    embedding_extent: tuple,\n    bandwidth: float = 0.5,\n    n_pts: int = 300,\n    N_sample_rows: int = 50000,\n    rows: list = None\n) \u2192 ndarray\n</code></pre> <p>Compute kernel density estimate of embedding. </p> <p>Args:</p> <ul> <li><code>dataset</code>:  pd.DataFrame with embedding data loaded in it. (Must have already populated columns named 'embedding_0', 'embedding_1') </li> <li><code>embedding_extent</code>:  the bounds in which to apply the density estimate. Has the form (xmin, xmax, ymin, ymax) </li> <li><code>bandwidth</code>:  the Gaussian kernel bandwidth. Will depend on the scale of the embedding. Can be changed to affect the number of clusters pulled out </li> <li><code>n_pts</code>:  number of points over which to evaluate the KDE </li> <li><code>N_sample_rows</code>:  number of rows to randomly sample to generate estimate </li> <li><code>rows</code>:  If provided, use these rows instead of a random sample </li> </ul> <p>Returns:  Numpy array with KDE over the specified square region in the embedding space, with dimensions (n_pts x n_pts)     </p> <p></p>"},{"location":"api-docs/unsupervised/#function-compute_watershed","title":"function <code>compute_watershed</code>","text":"<pre><code>compute_watershed(\n    dens_matrix: ndarray,\n    positive_only: bool = False,\n    cutoff: float = 0\n) \u2192 tuple\n</code></pre> <p>Compute watershed clustering of a density matrix.  </p> <p>Args:</p> <ul> <li><code>dens_matrix</code>:  A square 2D numpy array, output from compute_density, containing the kernel density estimate of the embedding. </li> <li><code>positive_only</code>:  Whether to apply a threshold, 'cutoff'. If applied, 'cutoff' is subtracted from dens_matrix, and any value below zero is set to zero. Useful for only focusing on high density clusters. </li> <li><code>cutoff</code>:  The cutoff value to apply if positive_only = True </li> </ul> <p>Returns:  A numpy array with the same dimensions as dens_matrix. Each value in the array is the cluster ID for that coordinate. </p> <p></p>"},{"location":"api-docs/unsupervised/#function-cluster_behaviors","title":"function <code>cluster_behaviors</code>","text":"<pre><code>cluster_behaviors(\n    dataset: DataFrame,\n    feature_cols: list,\n    N_rows: int = 200000,\n    use_morlet: bool = False,\n    use_umap: bool = True,\n    n_pts: int = 300,\n    bandwidth: float = 0.5,\n    **kwargs\n) \u2192 tuple\n</code></pre> <p>Cluster behaviors based on dimensionality reduction, kernel density estimation, and watershed clustering. </p> <p>Note that this will modify the dataset dataframe in place. </p> <p>The following columns are added to dataset:   'embedding_index_[0/1]': the coordinates of each embedding coordinate in the returned density matrix  'unsup_behavior_label': the Watershed transform label for that row, based on its embedding coordinates. Rows whose embedding coordinate has no watershed cluster, or which fall outside the domain have value -1. </p> <p>Args:</p> <ul> <li><code>dataset</code>:  the pd.DataFrame with the features of interest </li> <li><code>feature_cols</code>:  list of column names to perform the clustering on </li> <li><code>N_rows</code>:  number of rows to perform the embedding on. If 'None', then all rows are used. </li> <li><code>use_morlet</code>:  Apply Morlet wavelet transform to the feature cols before computing the embedding </li> <li><code>use_umap</code>:  If True will use UMAP dimensionality reduction, if False will use TSNE </li> <li><code>n_pts</code>:  dimension of grid the kernel density estimate is evaluated on.  </li> <li><code>bandwidth</code>:  Gaussian kernel bandwidth for kernel estimate </li> <li><code>**kwargs</code>:  All other keyword parameters are sent to dimensionality reduction call (either TSNE or UMAP) </li> </ul> <p>Returns:  A tuple with components:   - <code>dens_matrix</code>:  the (n_pts x n_pts) numpy array with the density estimate of the 2D embedding   - <code>labels</code>:  numpy array with same dimensions are dens_matrix, but with values the watershed cluster IDs   - <code>embedding_extent</code>:  the coordinates in embedding space that dens_matrix is approximating the density over </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/utils/","title":"Utils","text":""},{"location":"api-docs/utils/#module-utils","title":"module <code>utils</code>","text":"<p>Small helper utilities </p> <p></p>"},{"location":"api-docs/utils/#function-checkffmpeg","title":"function <code>checkFFMPEG</code>","text":"<pre><code>checkFFMPEG() \u2192 bool\n</code></pre> <p>Check for ffmpeg dependencies </p> <p>Returns:   True if can find <code>ffmpeg</code> in path, false otherwise </p> <p></p>"},{"location":"api-docs/utils/#function-check_keras","title":"function <code>check_keras</code>","text":"<pre><code>check_keras()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api-docs/video/","title":"Video","text":""},{"location":"api-docs/video/#module-video","title":"module <code>video</code>","text":"<p>Basic video tracking and behavior class that houses data </p>"},{"location":"api-docs/video/#global-variables","title":"Global Variables","text":"<ul> <li>global_config</li> <li>FEATURE_MAKERS</li> <li>UNIT_DICT</li> </ul>"},{"location":"api-docs/video/#function-create_metadata","title":"function <code>create_metadata</code>","text":"<pre><code>create_metadata(tracking_files: list, **kwargs) \u2192 dict\n</code></pre> <p>Prepare a metadata dictionary for defining a ExperimentDataFrame.  </p> <p>Only required argument is list of pose tracking file names.  </p> <p>Any other keyword argument must be either a non-iterable object (e.g. a scalar parameter, like FPS) that will be copied and tagged to each of the pose tracking files, or an iterable object of the same length of the list of pose tracking files. Each element in the iterable will be tagged with the corresponding pose-tracking file. </p> <p>Args:</p> <ul> <li><code>tracking_files</code>:  List of pose tracking files </li> <li><code>**kwargs</code>:  described as above </li> </ul> <p>Returns:  Dictionary whose keys are pose-tracking file names, and contains a dictionary with key,values containing the metadata provided </p> <p></p>"},{"location":"api-docs/video/#function-create_dataset","title":"function <code>create_dataset</code>","text":"<pre><code>create_dataset(\n    input: dict = None,\n    label_key: dict = None,\n    part_renamer: dict = None,\n    animal_renamer: dict = None,\n    video: list = None,\n    labels: list = None,\n    **kwargs\n) \u2192 DataFrame\n</code></pre> <p>Creates DataFrame that houses pose-tracking data and behavior annotations, along with relevant metadata, features and behavior annotation labels. </p> <p>Args:</p> <ul> <li><code>input</code>:  String OR list of strings with path(s) to tracking file(s).   OR Dictionary whose keys are pose tracking files, and value is a dictionary of associated metadata  for that video (see <code>create_metadata</code> if using this construction option) </li> <li><code>label_key</code>:  Default None. Dictionary whose keys are positive integers and values are behavior labels. If none, then this is inferred from the behavior annotation files provided.   </li> <li><code>part_renamer</code>:  Default None. Dictionary that can rename body parts from tracking files if needed (for feature creation, e.g.) </li> <li><code>animal_renamer</code>:  Default None. Dictionary that can rename animals from tracking files if needed </li> <li><code>**kwargs</code>:  Any other data to associate with each of the tracking files. This includes label files, and other metadata.   Any list-like arguments of appropriate length are zipped (associated) with each tracking file. See How To guide for more information. </li> </ul> <p>Returns:  DataFrame object. This is a pandas DataFrame with additional metadata and methods. </p> <p></p>"},{"location":"api-docs/video/#function-load_experiment","title":"function <code>load_experiment</code>","text":"<pre><code>load_experiment(fn_in: str) \u2192 DataFrame\n</code></pre> <p>Load DataFrame from file. </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  path to file to load </li> </ul> <p>Returns:  DataFrame object from pickle file </p> <p></p>"},{"location":"api-docs/video/#function-get_sample_openfield_data","title":"function <code>get_sample_openfield_data</code>","text":"<pre><code>get_sample_openfield_data()\n</code></pre> <p>Load a sample dataset of 1 mouse in openfield setup. The video is the sample that comes with DLC. </p> <p>Returns:   DataFrame with the corresponding tracking and behavior annotation files </p> <p></p>"},{"location":"api-docs/video/#function-add_randomforest_predictions","title":"function <code>add_randomforest_predictions</code>","text":"<pre><code>add_randomforest_predictions(df: DataFrame)\n</code></pre> <p>Perform cross validation of a RandomForestClassifier to predict behavior based on  activated features. Can be useful to assess model performance, and if you have enough data. </p> <p>Args:</p> <ul> <li><code>df</code>:  Dataframe housing features and labels to perform classification.  Will perform leave-one-video-out cross validation hence dataframe needs at least two videos to run. </li> </ul> <p>Returns:  None. Modifies df in place, adding column 'prediction' with model predictions in it. </p> <p></p>"},{"location":"api-docs/video/#class-ethologymetadataaccessor","title":"class <code>EthologyMetadataAccessor</code>","text":""},{"location":"api-docs/video/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(pandas_obj)\n</code></pre>"},{"location":"api-docs/video/#property-details","title":"property details","text":""},{"location":"api-docs/video/#property-label_key","title":"property label_key","text":""},{"location":"api-docs/video/#property-n_videos","title":"property n_videos","text":""},{"location":"api-docs/video/#property-reverse_label_key","title":"property reverse_label_key","text":""},{"location":"api-docs/video/#property-videos","title":"property videos","text":""},{"location":"api-docs/video/#class-ethologyfeaturesaccessor","title":"class <code>EthologyFeaturesAccessor</code>","text":""},{"location":"api-docs/video/#method-__init___1","title":"method <code>__init__</code>","text":"<pre><code>__init__(pandas_obj)\n</code></pre>"},{"location":"api-docs/video/#property-active","title":"property active","text":""},{"location":"api-docs/video/#method-activate","title":"method <code>activate</code>","text":"<pre><code>activate(name: str) \u2192 list\n</code></pre> <p>Add already present columns in data frame to the feature set.  </p> <p>Args:</p> <ul> <li><code>name</code>:  string for pattern matching -- any feature that starts with this string will be added </li> </ul> <p>Returns:  List of matched columns (may include columns that were already activated). </p> <p></p>"},{"location":"api-docs/video/#method-add","title":"method <code>add</code>","text":"<pre><code>add(\n    feature_maker,\n    featureset_name: str = None,\n    add_to_features=True,\n    required_columns=[],\n    **kwargs\n) \u2192 list\n</code></pre> <p>Compute features to dataframe using Feature object. 'featureset_name' will be prepended to new columns, followed by a double underscore.  </p> <p>Args:</p> <ul> <li><code>featuremaker</code>:  A Feature object that houses the feature-making function to be executed and a list of required columns that must in the dataframe for this to work </li> <li><code>featureset_name</code>:  Name to prepend to the added features  </li> <li><code>add_to_features</code>:  Whether to add to list of active features (i.e. will be returned by the .features property) </li> </ul> <p>Returns:  List of new columns that are computed </p> <p></p>"},{"location":"api-docs/video/#method-deactivate","title":"method <code>deactivate</code>","text":"<pre><code>deactivate(name: str) \u2192 list\n</code></pre> <p>Remove columns from the feature set.  </p> <p>Args:</p> <ul> <li><code>name</code>:  string for pattern matching -- any feature that starts with this string will be removed </li> </ul> <p>Returns:  List of removed columns. </p> <p></p>"},{"location":"api-docs/video/#method-deactivate_cols","title":"method <code>deactivate_cols</code>","text":"<pre><code>deactivate_cols(col_names: list) \u2192 list\n</code></pre> <p>Remove provided columns from set of feature columns. </p> <p>Args:</p> <ul> <li><code>col_names</code>:  list of column names </li> </ul> <p>Returns:  The columns that were removed from those designated as features. </p> <p></p>"},{"location":"api-docs/video/#method-regex","title":"method <code>regex</code>","text":"<pre><code>regex(pattern: str) \u2192 list\n</code></pre> <p>Return a list of column names that match the provided regex pattern. </p> <p>Args:</p> <ul> <li><code>pattern</code>:  a regex pattern to match column names to </li> </ul> <p>Returns:  list of column names </p> <p></p>"},{"location":"api-docs/video/#class-ethologyposeaccessor","title":"class <code>EthologyPoseAccessor</code>","text":""},{"location":"api-docs/video/#method-__init___2","title":"method <code>__init__</code>","text":"<pre><code>__init__(pandas_obj)\n</code></pre>"},{"location":"api-docs/video/#property-animal_setup","title":"property animal_setup","text":""},{"location":"api-docs/video/#property-animals","title":"property animals","text":""},{"location":"api-docs/video/#property-body_parts","title":"property body_parts","text":""},{"location":"api-docs/video/#property-raw_track_columns","title":"property raw_track_columns","text":""},{"location":"api-docs/video/#class-ethologymlaccessor","title":"class <code>EthologyMLAccessor</code>","text":""},{"location":"api-docs/video/#method-__init___3","title":"method <code>__init__</code>","text":"<pre><code>__init__(pandas_obj)\n</code></pre>"},{"location":"api-docs/video/#property-features","title":"property features","text":""},{"location":"api-docs/video/#property-fold_cols","title":"property fold_cols","text":""},{"location":"api-docs/video/#property-folds","title":"property folds","text":""},{"location":"api-docs/video/#property-group","title":"property group","text":""},{"location":"api-docs/video/#property-label_cols","title":"property label_cols","text":""},{"location":"api-docs/video/#property-labels","title":"property labels","text":""},{"location":"api-docs/video/#property-splitter","title":"property splitter","text":""},{"location":"api-docs/video/#class-ethologyioaccessor","title":"class <code>EthologyIOAccessor</code>","text":""},{"location":"api-docs/video/#method-__init___4","title":"method <code>__init__</code>","text":"<pre><code>__init__(pandas_obj)\n</code></pre>"},{"location":"api-docs/video/#method-load","title":"method <code>load</code>","text":"<pre><code>load(fn_in: str) \u2192 DataFrame\n</code></pre> <p>Load ExperimentDataFrame object from pickle file. </p> <p>Args:</p> <ul> <li><code>fn_in</code>:  path to load pickle file from.  </li> </ul> <p>Returns:  None. Data in this object is populated with contents of file. </p> <p></p>"},{"location":"api-docs/video/#method-save","title":"method <code>save</code>","text":"<pre><code>save(fn_out: str) \u2192 None\n</code></pre> <p>Save ExperimentDataFrame object with pickle. </p> <p>Args:</p> <ul> <li><code>fn_out</code>:  location to write pickle file to </li> </ul> <p>Returns:  None. File is saved to path. </p> <p></p>"},{"location":"api-docs/video/#method-save_movie","title":"method <code>save_movie</code>","text":"<pre><code>save_movie(label_columns, path_out: str, video_filenames=None) \u2192 None\n</code></pre> <p>Given columns indicating behavior predictions or whatever else, make a video with these predictions overlaid.  </p> <p>ExperimentDataFrame metadata must have the keys 'video_file', so that the video associated with each set of pose tracks is known. </p> <p>Args:</p> <ul> <li><code>label_columns</code>:  list or dict of columns whose values to overlay on top of video. If dict, keys are the columns and values are the print-friendly version. </li> <li><code>path_out</code>:  the directory to output the videos too </li> <li><code>video_filenames</code>:  list or string. The set of videos to use. If not provided, then use all videos as given in the metadata. </li> </ul> <p>Returns:  None. Videos are saved to 'path_out' </p> <p></p>"},{"location":"api-docs/video/#method-to_dlc_csv","title":"method <code>to_dlc_csv</code>","text":"<pre><code>to_dlc_csv(base_dir: str, save_h5_too=False) \u2192 None\n</code></pre> <p>Save ExperimentDataFrame tracking files to DLC csv format. </p> <p>Only save tracking data, not other computed features. </p> <p>Args:</p> <ul> <li><code>base_dir</code>:  base_dir to write DLC csv files to </li> <li><code>save_h5_too</code>:  if True, also save the data as an h5 file </li> </ul> <p>Returns:  None. Files are saved to path. </p> <p>This file was automatically generated via lazydocs.</p>"}]}